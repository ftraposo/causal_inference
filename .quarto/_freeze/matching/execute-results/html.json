{
  "hash": "c4ad8b09b20624501711751222f35283",
  "result": {
    "markdown": "# Matching\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls())\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n#1. Data: Job Training Data\\\nToday we are using data from [\\textcolor{red}{Dedejia and Wahba}](https://users.nber.org/~rdehejia/papers/matching.pdf), which represents a subset of the original Lalonde 1986 data (including only male). We use it to familiarize ourselves with matching techniques.\n\nWe load the data, in .csv format:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlalonde=read.csv(\"http://www.spyroskosmidis.net/wp-content/uploads/2018/01/lalonde.csv\")\n```\n:::\n\n\nWhat variables does the data include?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(lalonde) #Checking first 6 rows of the data \n```\n:::\n\n\n-   treat = dummy for treatment (job-training) or control\n-   age = participant age\n-   educ = participant education\n-   black, hispanic = dummies for participant race\n-   married = participant's marital status\n-   nodegree = participant has no school degree\n-   re74 = real earnings in 1974\n-   re75 = real earnings in 1975\n-   unem74 = unemployed in 1974\n-   unem75 = unemployed in 1975\n-   re78 = real earnings in 1978 (outcome variable)\n\nThe data comes from a labour market experiment that randomized participants into treatment (on-job training) and control with the aim to estimate its effect on participants' earnings. The experimental benchmark is \\$1794 -- i.e. this is the effect of the treatment on earnings. Let's start with an exercise. The model below shows the treatment effect with no covariates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(re78~treat,data=lalonde))\n\nsummary(lm(re78~treat + age + educ + black + hispan + nodegree + \n                married + re74 + re75 + unem74 + unem75 ,data=lalonde))\n```\n:::\n\n\nAs we can see, from the full comparison group the estimate (difference in means) is -\\$8,498. What does this suggest about the earnings of those treated?\n\n#2. Exercise1\n\nTry to retrieve the experimental benchmark by adding covariates into the above regression.\n\n#3. Observational Data\n\nWe are trying to obtain the experimental benchmark effect in observational data and we are going to try to do that with matching. Before we proceed, though, let's run some randomization checks. Note here that in the context of Matching we call this process \"Balance Check\". Since the treatment is binary we are going to run a logistic regression (or probit or less conservatively OLS).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandom\t<- glm(treat\t~ age + educ + black + hispan + nodegree + \n                married + re74 + re75+unem74+unem75,\tdata\t=lalonde,\tfamily\t= binomial(\"logit\"))\nsummary(random)\n```\n:::\n\n\nWhat do we notice?\n\nNow, we are going to create a data frame by predicting the outcome from the regression we just ran. The first column will be the propensity score and the second will be a binary indicator for those treated (==1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps.df <- data.frame(pr_score = predict(random, type = \"response\"),\n                     treat = random$model$treat)\nhead(ps.df, n=100)\n```\n:::\n\n\nBefore we proceed, let's install and load one more package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"sm\")#This is a package that allows you to quickly plot two densities. \n#One can do this in many different ways.\nrequire(sm)\n```\n:::\n\n\nNow, let's run the density plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsm.density.compare(ps.df$pr_score, ps.df$treat, xlab=\"Propensity Score\")\ntitle(main=\"Propensity score by treatment status\")\ntext(0.6,10, \"Control\", col=\"red\")\ntext(0.6,8, \"Treated\", col=\"green\")\n```\n:::\n\n\n#3. Matching\n\nGenerally, you should follow these steps:\n\n1.  Run probit (or logit) with the treatment as outcome and (relevant) covariates as predictors\n2.  Predict probabilities -- this will create the propensity score. Note here that some (Sekhon e.g.) suggest that the logit score is superior to the propensity score.\n3.  Match each participant to one or more non-participants using the propensity score. You can do this in a variety of ways: Nearest Neighbour Matching (NN), Caliper Matching, Mahalanobis Distance, Exact Matching etc. The default of the `Matching` package we use today is NN.\n4.  Check balance after matching\n5.  Estimate ATT\n\nLet's start by installing and loading some useful packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"Matching\")\n#install.packages(\"cobalt\")\nlibrary(Matching) #provides functions for multivariate and propensity score matching and for finding optimal balance based on a genetic search algorithm.\nrequire(cobalt) # Generate balance tables and plots for covariates of groups preprocessed through matching, weighting or subclassification\n```\n:::\n\n\nThink back to our `random` model -- was the treatment randomly assigned? No. This is why we're doing matching...\n\nWe are going to use `Matching`, a package with three important functions:\n\n-   MatchBalance gives you balance before and after matching\n-   Match performs propensity score and distance (i.e. mahalanobis) Matching\n-   GenMatch performs genetic matching\n\nLet's start with (1). Let's check balance before matching with this package.\n\n$$smd = \\frac{\\bar{X}_t- \\bar{X}_c}{\\sqrt{\\frac{SD^2_{1}}{n_1} + \\frac{SD^2_{1}}{n_2}}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbal.bm=MatchBalance(treat~+age + educ + black + hispan + nodegree + \n                  married + re74 + re75+unem74+unem75,data=lalonde)\n```\n:::\n\n\nWhat does this tell us?\n\nNow, let's match! We use propensity score matching (PSM). By default, we estimate NN -- what does this mean? NN sets $C(\\bar{P}_i)=min|\\bar{P_i}-\\bar{P}_j|$. Basically, the non-participant(s) $\\bar{P}_j$ closest to our participant $\\bar{P}_i$ will be selected as a match. The estimator has the option to perform matching with or without replacement. Today, we will only use matching without replacement. It's good to know though that this method has the disadvantage that the final estimate will usually depend on the initial ordering of the treated observations for which the matches were selected. How many neighbours should you use? It's usually better to oversample -- use more neighbours. This, however, involves a trade-off between bias and variance: it trades lower variance with increased bias. Generally, in choosing between different matching estimators, the one with the best balance is the most satisfactory one.\n\nTo retrieve the propensity score, we estimate a logit model predicting assignment to treatment -- just what we did above in `random`. We'll call this `ps` though it's exactly the same model as `random`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps\t<- glm(treat\t~ age + educ + black + hispan + nodegree + \n            married + re74 + re75+unem74+unem75,\t\n          data\t=lalonde,\tfamily\t= binomial())\n```\n:::\n\n\nFormula to calculate the conditional probability of receiving the treatment (predicted probabilities): $$P(W_I|X_i = x_i) = E(W_i) = \\frac{e^{x_i\\beta_i}}{1 + e^{x_i\\beta_i}} = \\frac{1}{1 + e^{e^{-\\beta_i}}}$$\n\nNext, we are going to match each treatment observation with a control, without replacement (that means, we are only going to use unique pairs of treatment-control, by contrast to matching with replacement, where the same individual can be control for several treated units).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1111) \nmatch.ps<- Match(Y=lalonde$re78,\tTr=lalonde$treat,\tX=ps$fitted,\n                 replace=FALSE)\nsummary(match.ps)\n```\n:::\n\n\nWe got \\$1642. Pretty good, right?\n\nNow that we've done the matching, let's check balance after matching.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbal.ps=MatchBalance(treat~+age + educ + black + hispan + nodegree + \n                      married + re74 + re75+unem74+unem75,\n                    match.out=match.ps,data=lalonde)\n```\n:::\n\n\nIt's getting better, but how about age? To see better what is happening, let's plot first the standardized mean differences with a threshold of 0.1. Note here that the package gives you the standardized mean differences for the unmatched and matched data. Have a look at the code below to see how the standardized mean differences are calculated for the unmatched age variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsddiffage=(mean(lalonde$age[lalonde$treat==1])-mean(lalonde$age[lalonde$treat==0]))/\n  sd(lalonde$age[lalonde$treat==1])\nabs(sddiffage*100)\n```\n:::\n\n\nThe package gives you something very similar. The following plot by cobalt does not multiply it by 100.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlove.plot(bal.tab(match.ps, treat~age + educ + black + hispan +\n                    nodegree + married + re74 + re75+unem74+unem75,\n                  data = lalonde),stat = \"mean.diffs\", \n          threshold = .1, var.order = \"unadjusted\" ,abs=T)\n```\n:::\n\n\nIdeally, all covariates would be on the 0 line. The further away the absolute mean differences are, the worse the balance. Now we have seen what happens with the means of the covariates. Let's see next what happens with their distributions. Remember, it can well be that the means are the same but the distributions are statistically different. We'll test that like we did last week with a Kolomogorov-Smirnov Statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlove.plot(bal.tab(match.ps, treat~age + educ + black + hispan + \n                    nodegree + married + re74 + re75+unem74+\n                    unem75, data = lalonde),\n          stat = \"ks.statistics\", var.order = \"unadjusted\")\n```\n:::\n\n\nAgain, ideally everything would be on 0 (that would mean the covariate distributions are identical between the treatment and the control). Note also that kolomogorov-smirnov statistics are only available for continuous covariates.\n\n## 3. Propensity Score, With Replacement\n\nNow, let's use propensity score WITH replacement. Remember, this means that a single control unit can be matched to several treated units. Matching without replacement can yield very bad matches if the number of comparison observations comparable to the treated observations is small. It keeps variance low at the cost of potential bias. Matching with replacement keeps bias low at the cost of a larger variance since you are using the same subjects again and again.\n\nHow many neighbors should you use? It's usually better to oversample -- use more neighbors. This, however, involves a trade-off between bias and variance: it trades lower variance with less potential bias.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7777)\nmatch.ps2<- Match(Y=lalonde$re78,\tTr=lalonde$treat,\tX=ps$fitted,\n                  replace=T)\nsummary(match.ps2)\n```\n:::\n\n\n\\$1314, not great, not terrible. And how's the balance?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalancepost=MatchBalance(treat~., match.out=match.ps2,data=lalonde[,-c(1,11)])\n```\n:::\n\n\nLet's plot the standardized mean differences:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlove.plot(bal.tab(match.ps2, treat~.,data=lalonde[,-c(1,11)]),stat = \"mean.diffs\", \n          threshold = .1, var.order = \"unadjusted\" ,abs=T)\n```\n:::\n\n\n#4. Exercise 2\n\nSo far the closest we've got to the experimental benchmark was with 1642. Let's try Caliper Matching Now. Caliper Matching is a variant of NN method that attempts to avoid 'bad' matches ($\\bar{P}_j$ that are far from our $\\bar{P}_i$. In order to do this, Caliper Matching imposes a tolerance on the maximum distance between the two. If, for some participants, no matches are found within the specified caliper (distance) then they will be excluded from the analysis. Theoretically, Caliper should be better than NN, since it corrects for 'bad' matches. Compared to NN, Caliper (also Kernel, LLR) use weighted averages. Caliper for example uses the weighted average over multiple persons within the caliper.\n\nLet's estimate Caliper with a distance of 0.1 standard deviations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1111)\nmatch.ps.cal0.1=Match(Y=lalonde$re78,\tTr=lalonde$treat,\t\n                          X=ps$fitted, caliper=0.1,\treplace=F)\nsummary(match.ps.cal0.1)\n```\n:::\n\n\nWe got \\$1374 That's worse than before. How's the balance though?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbal.ps.cal0.1=MatchBalance(treat~+age + educ + black + hispan + nodegree + \n             married + re74+re75+unem74+unem75,\n             match.out=match.ps.cal0.1,data=lalonde)\n```\n:::\n\n\nand plot that\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlove.plot(bal.tab(match.ps.cal0.1, treat~age + educ + black + hispan +\n                    nodegree + married + re74 + re75+unem74+unem75,\n                  data = lalonde),stat = \"mean.diffs\", \n          threshold = .1, var.order = \"unadjusted\" ,abs=T)\n```\n:::\n\n\nDoes this look good? Remember we wanted to get as close as possible to \\$1794. Try again by changing the caliper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's set seed equal to 5555\n# let's try a caliper equal to 0.25  \n# let's output of our Match function  \"match.ps.cal25\"\n\nset.seed(5555)\nmatch.ps.cal25 <- Match(Y = lalonde$re78, Tr = lalonde$treat, X = ps$fitted,\n                        caliper = 0.25, replace = F)\n\nsummary(match.ps.cal25)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's check for balance before and after matching using a 0.25 caliper. \n\nbal.ps.cal25 = MatchBalance(treat~+age + educ + black + hispan + nodegree +\n                              married + re74 + re75 + unem74 + unem75,\n                            match.out = match.ps.cal25, data = lalonde)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's generate the balance plot\n```\n:::\n\n\n#5. Mahalanobis Distance\n\nLike propensity score matching, Mahalanobis distance matching is built on the notion of distance between observations of pretreatment covariates. It uses the complete variance covariance matrix, which means that the relationship between variables is included in the analysis. The contribution to the distance calculation of two highly correlated covariates will then be lower than that of less correlated ones. In essence Mahalanobis distance matching scales the distance by the inverse of the covariance matrix.\n\n$$MD(X_i, X_j) = \\sqrt{(X_i - X_j)'\\Sigma^{-1}(X_i - X_j)}$$\n\nFor exact match, $$MD = (X_i, X_j) = 0$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlalonde_cov <- lalonde %>%\n  dplyr::select(-X, -treat, -re78)\n\nset.seed(2222)\nmatch.maha  <-  Match(Y=lalonde$re78,Tr=lalonde$treat,X=lalonde_cov,\n                     BiasAdjust=F,estimand=\"ATT\",M=1, Weight = 2)\n\n\nsummary(match.maha)\n```\n:::\n\n\nWith Mahalanobis, we get \\$1715, so it's slightly better than the caliper =0.1 and both NNs matching\n\nAnd checking balance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbal.maha  <- MatchBalance(lalonde$treat~.,match.out = match.maha, data=lalonde_cov,ks=FALSE)\n```\n:::\n\n\nVisualizing that in a plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlove.plot(bal.tab(match.maha, treat~.,data=lalonde[,-c(1,11)]),stat = \"mean.diffs\", \n          threshold = 0.1, var.order = \"unadjusted\" ,abs=T)\n```\n:::\n\n\n| Matching technique | Estimation | Diff to benchmark |\n|--------------------|------------|:-----------------:|\n| Benchmark          | 1794       |         0         |\n| Regression         | -8506      |       10300       |\n| Regression + cov   | 1068       |        726        |\n| NN without rep     | 1642       |        152        |\n| NN with rep        | 1714       |        80         |\n| Caliper (0.1)      | 1374       |        420        |\n| Capiler (0.25)     | 1839       |        45         |\n| Mahalanobis        | 1715       |        79         |\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}