{
  "hash": "31c261d3982366b6a783538e8c241666",
  "result": {
    "markdown": "# Experiments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(\n\techo = TRUE,\n\tmessage = TRUE,\n\twarning = TRUE\n)\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(tidymodels)\n\nrm(list = ls())\n```\n:::\n\n\n#1. Data: Party Brands Today we are using data from Noam Lupu's work on [\\textcolor{red}{ Party Brands and Partisanship}](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2012.00615.x) in Argentina. We use it to familiarize ourselves with survey experiments by replicating some of his results.\n\nWhat we need to know about the paper:\n\n-   Two main theories on the dynamics of partisanship:\n    -   Partisanship as a social identity (stable)\n    -   Partisanship as a rational maximisation expected utility (unstable)\n\nWhat is Lupu trying to empirically test on this paper?\n\n-   That partisanship as a social identify does not imply necessarily partisan stability\n-   Test his theory that incorporates comparative fit, which means that people think their group differs from other groups\n-   Testable implication: When parties converge, their brands are diluted, thus, it become more difficult for voters to distinguish parties from one another and subsequently party attachments are weaken.\n\n# Details of the experiment\n\n-   A survey experiment\n\n-   Manipulation of information about political parties that suggest either *diverge* or *converge* among the parties\n\n-   Between-subject design with three-treatment arms, plus a control\n\n-   Information was gleaned from party manifestos and official statements\n\n-   Control: Party symbols with name of the party leaders\n\n-   Outcomes of interest: -*Partisanship* as the proportion of respondents said they identified with a political party -*Partisan attachment* on a 0 - 10 scale We load the data, in .csv format, as follows:\n\nYou can use the following shortcuts to insert a r chunk: Ctrl + Alt + I or Cmd + Option + I\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrands=read.csv(\"http://www.spyroskosmidis.net/wp-content/uploads/2018/01/lupubrands.csv\")\n\n# tidy version:\nbrands_tidy <- as_tibble(brands)\n```\n:::\n\n\nWhat variables does the data include?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(brands) #Checking first 6 rows of the data \n\n# tidy version:\nglimpse(brands_tidy)\n\nbrands_tidy %>%\n  head() %>%\n  knitr::kable()\n```\n:::\n\n\nAmong these variables, let's focus on the treatment variables and the outcome. There are four variables associated with the treatments and they are captured by the categorical variable`formulario`\n\n-   1 = Control group\n-   2 = Platform Information\n-   3 = Alliance Switching\n-   4 = Full Text\n\nThis variable is recoded into the dummies `frm2=1` which takes the value 1 if the respondent received the platform information treatment, `frm3=1` for alliance switching and `frm4=1` for the full text. Note the omitted dummy represents the control group.\n\nFor now, however, let's use the original, categorical variable. As it is conventional, we aim to have the control group assigned to value 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrands$treat=brands$formulario-1  #Assigning value 0 to the control group\n\n# tidy version: \nbrands_tidy <- brands_tidy %>%\n  mutate(treat = formulario - 1) \n```\n:::\n\n\nDid we define the treatment variable correctly? Let's check again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cbind(brands$formulario, brands$treat, brands$frm2,brands$frm3,brands$frm4))\n\nassignment <- brands_tidy %>%\n  select(formulario, treat, frm2, frm3, frm4)\n\nhead(assignment)\n```\n:::\n\n\nEverything looks good! Taking the first row as example, when `formulario==3` (column1), the treatment variable we generated takes the value 2 (formulario-1) and the dummy for value 3 (frm3) take the value 1, whereas the others are 0.\n\nThe Model: OLS Estimations\n\nLupu is interested in the effect of the three treatment conditions on partisanship. Among the two outcomes, `pid` (whether the individual identifies with a party) and `pidstr` (a 10 point scale representing the strength of the respondent's party identification), we will focus below mainly on the latter.\n\n#3.Let's beging with a simple OLS model with no covariates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(pidstr~treat, data=brands))\n\n# tidy version: \nlin_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(pidstr ~ treat, data = brands_tidy)\n\ntidy(lin_fit)\n```\n:::\n\n\nDoes this regression make sense? Hint: `class(brands$treat)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Let's assign labels to each group. This will help with the output!\n\nbrands$treat[brands$treat==0]=\"Control\"\nbrands$treat[brands$treat==1]=\"Platform\"\nbrands$treat[brands$treat==2]=\"Switch\"\nbrands$treat[brands$treat==3]=\"Full\"\n\n\n# tidy version:\nbrands_tidy <- brands_tidy %>%\n  mutate(treat = recode(\n    treat,\n    `0` = \"Control\",\n    `1` = \"Platform\",\n    `2` = \"Switch\",\n    `3` = \"Full\"\n  ))\n```\n:::\n\n\nRe-running the regression from above and telling R to model treat as a factor variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(pidstr~factor(treat), data=brands))\n\n# tidy version: \nlin_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(pidstr ~ factor(treat), data = brands_tidy)\n\ntidy(lin_fit)\n```\n:::\n\n\nThe OLS regression results look like mere mean differences. Let's see if that is true:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npid.control=mean(brands$pidstr[brands$treat==\"Control\"], na.rm=T)\npid.platform=mean(brands$pidstr[brands$treat==\"Platform\"],na.rm=T)\npid.switch=mean(brands$pidstr[brands$treat==\"Switch\"],na.rm=T)\npid.full=mean(brands$pidstr[brands$treat==\"Full\"],na.rm=T)\n\nate.platform=pid.platform-pid.control\nate.switch= pid.switch-pid.control\nate.full=pid.full-pid.control\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tidy version:\n\npid.control <- brands_tidy %>% # E_Y_X_control\n  filter(treat == \"Control\") %>%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.platform <- brands_tidy %>% # E_Y_X_platform\n  filter(treat == \"Platform\") %>%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.switch <- brands_tidy %>% # E_Y_X_switch\n  filter(treat == \"Switch\") %>%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.switch <- brands_tidy %>% # E_Y_X_1_full\n  filter(treat == \"Full\") %>%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE)) \n\nate.platform=pid.platform-pid.control\nate.switch= pid.switch-pid.control\nate.full=pid.full-pid.control\n\nate.platform\nate.switch\nate.full\n```\n:::\n\n\nThe ATEs we calculated just now are identical to the OLS slopes we calculated before. So, why do OLS?\n\nFirstly, we prefer models that can calculate measures of dispersion for the estimates. In other words, through substracting the means by treatment group we would not be able to know whether the ATEs are statistically significant. Secondly, the advantage of using OLS is that we can include -pre-treatment- covariates.\n\nStill, can you think of other ways to estimate statistical uncertainty without using OLS?\n\nLet's see what happens when we add covariates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(pidstr~factor(treat)+age+income+educgrp+info, data=brands))\n```\n:::\n\n\nAre the results the same?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Re-estimate the main model\natemod=lm(pidstr~factor(treat), data=brands)\nsummary(atemod)\nnobs(atemod)\n\n#Add covariates\natemodcont=lm(pidstr~factor(treat)+age+income+educgrp+info, data=brands)\nsummary(atemodcont)\nnobs(atemodcont)\n#Why do we lose so many observations? Is this is a fair comparison of the ATE?\n\n#Let's constrain the estimation to the N of the model with the added covariates\nesample=rownames(as.matrix(resid(atemodcont)))\n\natemodsample=lm(pidstr~factor(treat), data=brands[esample,])\nsummary(atemodsample)\nnobs(atemodsample)\n\n#install.packages(\"stargazer\")\nlibrary(stargazer)\n\nstargazer(atemodcont, atemodsample,  type = \"text\")\n\n# sort of a tidy version:\n\nconstrain <- brands_tidy %>% \n  filter(row.names(brands_tidy) %in% esample) \n\natemodsample=lm(pidstr~factor(treat), data=constrain)\nsummary(atemodsample)\nnobs(atemodsample)\n```\n:::\n\n\n*Randomization Checks*\n\nLet's do some randomization checks. Is the mean value of age similar across treatment groups?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(brands$age~brands$frm2, data=brands, subset=c(brands$frm3!=1,brands$frm4!=1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#and, similarly, for the other treatments\nt.test(brands$age~brands$frm3, data=brands, subset=c(brands$frm2!=1,brands$frm4!=1))\nt.test(brands$age~brands$frm4, data=brands, subset=c(brands$frm3!=1,brands$frm2!=1))\n```\n:::\n\n\nHow about income?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(brands$income~brands$frm2, data=brands, subset=c(brands$frm3!=1,brands$frm4!=1))\nt.test(brands$income~brands$frm3, data=brands, subset=c(brands$frm2!=1,brands$frm4!=1))\nt.test(brands$income~brands$frm4, data=brands, subset=c(brands$frm3!=1,brands$frm2!=1))\n```\n:::\n\n\nOr gender?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(brands$frm2,brands$female[brands$treat!=3 & brands$treat!=2])\nprop.test(table(brands$frm2,brands$female[brands$treat!=3 & brands$treat!=2]))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(table(brands$frm3,brands$female[brands$treat!=3 & brands$treat!=1]))\nprop.test(table(brands$frm4,brands$female[brands$treat!=3 & brands$treat!=2]))\n\n# tidy version - need to double check: \nlibrary(broom)\n\nbrands_tidy %>%\n  filter(treat ==  c(\"Switch\", \"Control\")) %>% \n  group_by(frm4, female) %>%\n  summarise(cases = n()) %>%\n  mutate(pop = sum(cases)) %>%\n  rowwise() %>%\n  mutate(tst = list(broom::tidy(prop.test(\n    cases, pop, conf.level = 0.95\n  )))) %>%\n  tidyr::unnest(tst)\n```\n:::\n\n\nAn equivalent to a ttest is the kolomogorov smirnov test that compares distributions (only works for continuous variables)\n\n$$\nD = \\max_{1 \\leq i \\leq N} (F(Y_i) - \\frac{i - 1}{N}, \\frac{i}{N} - F(Y_i))\n$$\n\n![Cummulative distribution functions](KS_Example.png)\n\n\n# Non compliance in Experiments \n\n\n\n\n\n\n\n## What Experiment?\n\nYou already know various types of experiments, such as survey experiments, lab experiments, field experiments.\n\nHow do natural experiments fit in?\n\n## What Experiment?\n\nYou already know various types of experiments, such as survey experiments, lab experiments, field experiments. \n\nHow do natural experiments fit in?\n\n- IR: Rotating Presidency of the Council of the EU (Carnegie/Marinov 2017)\n- CP: Population size & electoral system (Eggers 2015)\n- many others...\n- IR/CP: Election Observation (Hyde 2007)\n\n## 1. Data: Election Observation \n\nToday we are using data from Susan Hyde's work on [__the observer effect in international politics__](https://heinonline.org/HOL/LandingPage?handle=hein.journals/wpot60&div=6&id=&page) in Armenia. \n\n- Research Question: _Do international monitoring missions have an effect on electoral outcomes?_\n\n- Hyde's reasoning: Cross-sectional comparisons are not suitable as there will be endogeneity\n\n- Hyde's solution: Moving to the microlevel! (What effect do observers have on election-day fraud?)\n  - 2003 Presidential elections in Armenia\n  \n- The Experiment: Analysing observed and unobserved polling stations, relying on OSCE missions in terms of incumbent's vote share\n\n- Outcome: Hyde concludes there was a __5.9%__ difference in first round and a __2%__ difference in the second round (simple differences in means).\n\n\n## Let's check this\n\n- Do we buy that?\n\nWhat might be the problems associated with this natural experiment?\n\n## Let's check this\n\nFirst, as always, we load the file (this time from Stata)...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readstata13)\nhyde2007 <- read.dta13(file = '/cloud/project/hyde2007_uploadR.dta')\n```\n:::\n\n\n...and look at the data, that is at some variables of interest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cbind(hyde2007$pollingstation, hyde2007$urban, hyde2007$voterspersqkm, hyde2007$kocharian, hyde2007$mon_voting, hyde2007$mon_voting_R2, hyde2007$KocharianR2, hyde2007$osce_assignment))\n```\n:::\n\n\n__Exercise__: For the first round results, (i) calculate the difference in means of the incumbent's vote share (```kocharian```) between observed and unobserved polling stations and (ii) find out whether randomization worked regarding the rural-urban divide (```urban```).\n\n## ATE & Randomization: Round 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(density(hyde2007$kocharian[hyde2007$mon_voting==1], na.rm=T, from=min(0, na.rm=T), to=max(1, na.rm=T)))\nlines(density(hyde2007$kocharian[hyde2007$mon_voting==0], na.rm=T, from=min(0, na.rm=T), to=max(1, na.rm=T)))\n```\n:::\n\n\n## ATE & Randomization: Round 1\n\nTo calculate the difference in means, we could use \n\n- a t.test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(hyde2007$kocharian ~ hyde2007$mon_voting)\n```\n:::\n\n\n- an OLS regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols1=lm(kocharian ~ mon_voting, data=hyde2007)\nsummary(ols1)\n```\n:::\n\n\nWe can do the same (using a prop.test instead of a t.test) in order to check whether randomization worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(table(hyde2007$urban, hyde2007$mon_voting))\n\nols_urban=lm(urban ~ mon_voting, data=hyde2007)\nsummary(ols_urban)\n```\n:::\n\n\nWhat do we conclude? Do we have reason to think this might affect the validity of the results?\n\n\n## Round 2\n\nLet's quickly do the same for Round 2:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols2=lm(KocharianR2 ~ mon_voting_R2, data=hyde2007)\nsummary(ols2)\n```\n:::\n\nThis looks like a somewhat weaker effect of the observers...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols2_urban=lm(urban ~ mon_voting_R2, data=hyde2007)\nsummary(ols2_urban)\n```\n:::\n\nRound 2 seems to be even less balanced in terms of the representation of urban and rural polling stations. \n\n- What else could be relevant in Round 2? Why should we account for this?\n\n\n## Round 2\n\n- Hyde concludes that her _\"results suggest that if first-round monitoring took place then second-round monitoring had only a marginal additional deterrent effect.\"_ (p.56). She claims that first-round observation has a persistant effect.\n - Your thoughts on this?\n\n## Round 2\n\n- Hyde concludes that her _\"results suggest that if first-round monitoring took place then second-round monitoring had only a marginal additional deterrent effect.\"_ (p.56). She claims that first-round observation has a persistant effect.\n - Your thoughts on this?\n \nLet's see whether monitoring in the two rounds was independent. First, we check the number of polling stations observed per round.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(with(hyde2007, mon_voting==0 & mon_voting_R2==0))\nsum(with(hyde2007, mon_voting==1 & mon_voting_R2==0))\nsum(with(hyde2007, mon_voting==0 & mon_voting_R2==1))\nsum(with(hyde2007, mon_voting==1 & mon_voting_R2==1))\n```\n:::\n\nWell, in Round 2 more stations were observed that had been monitored in Round 1 than those that had not been monitored. Let's confirm significance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(table(hyde2007$mon_voting_R2, hyde2007$mon_voting))\n\n\nols2_full=lm(mon_voting_R2 ~ urban + mon_voting, data=hyde2007)\nsummary(ols2_full)\n```\n:::\n\n\nWhat do we conclude?\n\n- Round 2 monitoring was not randomized either.\n- If we apply experimental standards (which is what we should do), we conclude that Round 2 monitoring was not independent from Round 1 monitoring.\n\n\n## 2. Non-Compliance\n\nOne of the reasons for the imbalance between rural and urban polling station could be that observers did not adhere to their assigned polling stations. Let's now assume, the OSCE did a good job and assigned polling stations randomly (```osce_assignment```).\n\nIn that case, one-sided __non-compliance__ would be a problem. Polling stations that were supposed to be monitored were, in fact, not monitored on election day. Let's check Hyde's results once we consider non-compliance to be a problem\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(hyde2007) # it helps to avoid using the dollar sign\n```\n:::\n\n\nLet's start with a (some naive) simple differences in means:\n\n::: {.cell}\n\n```{.r .cell-code}\n#As-treated Analysis\n(mean(KocharianR2[mon_voting_R2==1], na.rm = TRUE) - mean(KocharianR2[mon_voting_R2==0], na.rm = TRUE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhyde2007 <- hyde2007 %>%  # remove any NAs\n  drop_na(KocharianR2)\n```\n:::\n\n\nLet's look at a simple crosstab, showing assigned treatment and actual treatment.\n\n::: {.cell}\n\n```{.r .cell-code}\n#Compliance Status - Binary\ntab.1 <- table(osce_assignment, mon_voting_R2)\nprint(tab.1)\n```\n:::\n\n\nNow, we calculate the ```ITT.y```. We can do this using differences in means or simply running an OLS.\n\n::: {.cell}\n\n```{.r .cell-code}\n#ITT_Y\nitt.y <- (mean(KocharianR2[osce_assignment==1], na.rm = TRUE) - mean(KocharianR2[osce_assignment==0], na.rm = TRUE))\nitt.y \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ITT_Y using OLS\nlibrary(sandwich)\nlibrary(lmtest)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nitt_fit <- lm(KocharianR2 ~ osce_assignment)\nsummary(itt_fit)\n#coeftest(itt_fit, vcovHC(itt_fit))  # Heteroscedasticity-Consistent Covariance Matrix Estimation-Consistent Covariance \n```\n:::\n\n\nAccordingly, we calculate the ITT.d\n\n::: {.cell}\n\n```{.r .cell-code}\n#ITT_D\nitt.d <- (mean(mon_voting_R2[osce_assignment==1], na.rm = TRUE) - mean(mon_voting_R2[osce_assignment==0], na.rm = TRUE))\nitt.d\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## ITT_D using OLS \nitt_d_fit <- lm(mon_voting_R2 ~ osce_assignment)\nsummary(itt_d_fit)\n# coeftest(itt_d_fit, vcovHC(itt_d_fit)) # Heteroscedasticity-Consistent Covariance Matrix Estimation\n```\n:::\n\n\nLet's quickly have a look at the compliance status and the broad picture. \n\n::: {.cell}\n\n```{.r .cell-code}\n#Let's remember the compliance status [1] 883 [2]258 [3] 0 [4]623\ntab.1 <- table(osce_assignment, mon_voting_R2)\nprint(tab.1)\n\n#Always-Takers  \n# Under one-sided non-compliance there are no Always takers, so this should be zero\n# 0 / (883 + 0) = 0 \nat <- tab.1[3]/(tab.1[1] + tab.1[3])  \nat\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Never Takers\n# 258/(258 + 623) = 0.29\n# This means that from those polling stations that were assigned to treatment, 29% will not be monitored\nnt <- tab.1[2]/(tab.1[2] + tab.1[4])\nnt\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Compliers (No Defiers By Assumption)\n1 - at - nt\n```\n:::\n\n\nFinally, using the ```ITT.y``` and the ```ITT.d``` , we can calculate the CACE:\n\n::: {.cell}\n\n```{.r .cell-code}\n#CACE: ITT_Y divided by ITT_D # COMPLIER AVERAGE CAUSAL EFFECT\n(mean(KocharianR2[osce_assignment==1], na.rm = TRUE) - mean(KocharianR2[osce_assignment==0], na.rm = TRUE))/(mean(mon_voting_R2[osce_assignment==1], na.rm = TRUE) - mean(mon_voting_R2[osce_assignment==0], na.rm = TRUE))\n\ncace <- itt.y/itt.d # This should be the same! \nprint(cace)\n```\n:::\n\n\nWe can also calculate the significance of the CACE. To do so, we calculate the standard errors of the CACE (this is in fact, an approximation):\n\n```SE(CACE) ≈ SE(ITT.Y) / ITT.D```\n\n::: {.cell}\n\n```{.r .cell-code}\nse_cace <- summary(itt_fit)$coefficients[4]/itt_d_fit$coefficients \nprint(se_cace)\n```\n:::\n\nWe then calculate the p-value as we did last week:\n\n::: {.cell}\n\n```{.r .cell-code}\nt_cace <- cace / se_cace\nprint(t_cace)\n\np <- (1 - pnorm(abs(t_cace), 0, 1)) * 2\np\n```\n:::\n\n\n\nAs we will be seeing in a few weeks time, we can also use 2SLS regressions to calculate the CACE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Via TSLS\nsummary(ivreg(KocharianR2 ~ mon_voting_R2|osce_assignment, data=hyde2007))\n```\n:::\n\n\n\n## So?\n\n- What do we make of Susan Hyde's paper?\n - Is this a \"bad\" experiment? Why?\n \n- Can there be \"perfect\" natural experiments?\n \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}