{"title":"Regression Discontinuity Design","markdown":{"headingText":"Regression Discontinuity Design","containsRefs":false,"markdown":"\n\n```{r, eval=F, echo=T}\nknitr::opts_chunk$set(\n\techo = TRUE,\n\tmessage = FALSE,\n\twarning = FALSE,\n\tcomment = NA\n)\n\nrm(list = ls())\n\nlibrary(readr)\neduc <- read_csv(\"islamic_women.csv\")\n```\n\n### Lecture Slides & Data\n\\\n\\\n\n\n```{r, eval=F, echo=T}\n#install.packages(\"downloadthis\")\nlibrary(downloadthis)\n\ndownload_link(\n  link = \"https://bayreuth-politics.github.io/CI22/data/islamic_women.csv\",\n  output_name = \"educ\",\n  output_extension = \".rdata\",\n  button_label = \"Lab 8 Data\",\n  button_type = \"success\",\n  has_icon = TRUE,\n  self_contained = TRUE\n)\n\n#download_link(\n#  link = \"https://github.com/dpir-ci/CI22/raw/gh-pages/docs/lectures/lecture7.pdf\",\n#  output_name = \"week7\",\n#  output_extension = \".pdf\",\n#  button_label = \"Lecture Slides\",\n#  button_type = \"default\",\n#  has_icon = FALSE,\n#  self_contained = FALSE\n#)\n```\n\n\\\n\\\n\n---\n\n# Recap \n\n### Sharp Regression Discontinuity\n\nIn RDDs we exploit that treatment assignment is determined by a known **assignment rule** that determines whether units are assigned to the treatment. In RDD, all units in the study receive a score, usually called *running variable*, *forcing variable* or *index*, and treatment is assigned to those units whose score is above a known cut-off (Cattaneo et al 2019). \n\nIn RDD we have three components: \n\n1. The score\n2. The cutoff \n3. The treatment. \n\nIn the Sharp Regression Discontinuity Design, the score determines -  deterministically - whether the unit is being assigned to treatment or to the control condition. However, we again face the fundamental problem of causal inference because we can only observe the untreated outcome for those units below the cutoff (control) and the treated outcome for those above the cut-off (treated). However, imposing the assumption of comparability between units with very similar values of the score but on opposite sides of cut-off enables us to calculate the treatment effect of the intervention. \n \nGiven the continuity assumption, we would expect that observations in a small neighbourhood around the cut-off will have very similar potential outcomes. Thus, this would justify using observations just below the cut-off as a reasonable proxy of what would the average outcome for those units just above the cut-off would have had if they had received the control condition instead of the treatment (i.e. counterfactual).  \n\nThe main goal in RD is to adequately perform an extrapolation of the average outcome of treated and untreated units at the cutoff. \n\n### Estimation \n\nThere are different ways that we could estimate the causal effect from RDD:\n\n- Linear\n- Linear with different slopes\n- High-order polynomial (or non-linear)\n- Non-parametric\n\n\n### Choice of Kernel function and Polynomial Order\n\nWhenever we conduct RDD we choose the following: \n\n1. Polynomial order $p$ function \n2. Kernel function $K(\\dot)$, which determines how observations within a bandwidth would be weighted, and there are different options: \"Uniform\", \"Triangular\", and \"Epanechnikov\". \n3. Bandwidth size: There is a trade-off between bias and efficiency. The closer you get to the cut-off, the less bias in the estimator, but more variance as there are fewer observations. \n4. We conduct weighted least squares and obtain the intercept of the chosen polynomial order above and below the cut-off. \n5. Calculate the sharp point estimate by subtracting the intercepts of these polynomials: $\\hat{\\tau_{SRD}} = \\alpha_{above} - \\alpha_{below}$\n\n\\\n\n\n### Falsification tests: \n\nWhenever we conduct RD we choose the following: \n\nWe learned that we can conduct multiple sensitivity and falsification tests: \n\n1. Sensitivity: To check that the results of our estimation are robust to different specifications\n2. Continuity: To check whether covariates do not jump at the threshold. \n3. Sorting: To check that units do not sort around the threshold. \n4. Placebo cut-offs: To check that outcomes do not change abruptly at an  arbitrary threshold (to test the continuity assumption)\n\n---\n\n**Before starting this seminar**\n\n1. Create a folder called \"lab8\"\n\n2. Download the data (you can use the button or the one at the top, or read csv files directly from  github): \n\n3. Open an R script (or Markdown file) and save  it in our “lab8” folder.\n\n4. Set your working directory using the setwd() function or by clicking on “More“. For example *setwd(\"~/Desktop/Causal Inference/2022/Lab8\")*\n\n5. Let's install an load packages that we will be using in this lab:\n\n```{r, eval=F, echo=T}\n\nlibrary(stargazer) # generate formated regression tables \nlibrary(texreg) # generate formatted regression tables\nlibrary(tidyverse) # to conduct some tidy operations\nlibrary(ggplot2)\n#install.packages(c(rdrobust, rddensity))\nlibrary(tidyverse)\n#install.packages(\"rdd\")\nlibrary(rdd) # to conduct McCrary Sorting Test \nlibrary(rdrobust) # to conduct non-parametric rd estimates \nlibrary(rddensity) # to conduct a density test and density plots\n\n```\n\n--- \n\n# Seminar Overview\n\nIn this **seminar**, we will cover the following topics:\n\\\n1. Conduct regression discontinuity using global polynomial estimation using `lm()` function\n\\\n2. Calculate LATE using non-parametric estimations using  `rdrobust()`. \n\\\n3. Conduct various robustness/falsification tests such as balance test, placebo outcome, density test, and falsification test. \n\n\n\n---\n\n## Islamic Rule and the Empowerment of the Poor and Pious - Meyerson (2014)\n\nIn this [paper](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9878), Meyerson is interested in the effects of Islamic parties' control of local governments on women's rights. He focuses on the educational attainment of young women. Meyerson conducts a Sharp RD design, based on close elections in Turkey. The challenge here is to compare municipalities where the support for Islamic parties is high and win the election, versus those that elected a secular mayor. \n\nYou would expect that municipalities controlled by Islamic parties would systematically differ from those that are controlled by a secular mayor. Particularly, if religious conservatism affects the educational outcomes of women. However, we can use RDD to isolate the treatment effect of interest from all systematic differences between treated and untreated units. \n\nWe can compare municipalities the Islamic party barely won the election versus municipalities where the Islamic party barely lost. This reveals the causal (local) effect of Islamic party control on women's educational attainment a few years later. One crucial condition to meet in this setup is that parties cannot systematically manipulate the vote share they obtain. \n\nThe data used in this study is from the 2014 mayoral election in Turkey. The unit of analysis is the municipality, and the running variable is the margin of victory. The outcome of interest is the educational attainment of women who attended high school during 1994-2000, calculated as a percentage of the cohort of women aged 15 to 20 in 2000 who had completed high school by 2000. \n\nWe will be using the following variables: \n\n| Variable             \t| Description                                                                              \t|\n|------------------\t|------------------------------------------------------------------------------------------\t|\n| ```margin``` \t| This variable represents the margin of victory of  Islamic parties in the 1994 election. A positive margin means that an Islamic party won.     \t|\n|```school_men```| secondary school completion rate for men aged between 15 and 20 \t|\n| ```school_women```   | the secondary school completion rate for women aged 15-20\t|\n| ```log_pop``` |  log of the municipality population in 1994         \t|\n| ```sex_ratio```         \t| gender ratio of the municipality in 1994\t                              \t|\n| ```log_area```         \t|  log of the municipality area in 1994\t|\n\\\n\nNow let's load the data. There are two ways to do this: \n\nYou can load the dataset from your laptop using the `read.csv()` function.  Here the dataset is called `educ` - but feel free to give it a different name if you prefer.\n\n```{r, eval=F, echo=T}\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab8\")\n# \nlibrary(readr)\n#educ <- read.csv(\"~/islamic_women.csv\")\n\nhead(educ)\n```\n\nLet's start by visualising the data. We will use ```plot()``` function to do this. This is the simplest scatter plot that you can come up with.\n\n**Exercise 1: Generate a plot using the `plot(X,Y)` function. Replace X with `educ$margin` and Y with `educ$school_women`.**\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nplot(educ$margin, educ$school_women)\n\n```\nThis is a very simple plot that shows the raw relationship between the margin of victory and the outcome variable. However, it conveys some important information. For example, the margin of victory is clustered around -0.5 and roughly 0.3. Also, the outcome variable, school attainment, usually goes from 0 to 40%. Now let's generate a slightly fancier plot.\n\n</details> \n\\\n\n**Exercise 2: Generate a scatter plot using `ggplot()` function. Use the functions below to add some additional features into this plot.**\n\n\n```{r, eval=F, echo=T}\n\nggplot(aes(x = running variable, y = outcome, colour =outcome), data = data) +\n  # Make points small and semi-transparent since there are lots of them\n  geom_point(size = 0.5, alpha = 0.5, position = position_jitter(width = 0, height = 0.25, seed = 1234)) + \n  # Add vertical line\n  geom_vline(xintercept = 0) + \n  # Add labels\n  labs(x = \"Label X\", y = \"Label Y\") + \n  # Turn off the color legend, since it's redundant\n  guides(color = FALSE)\n\n```\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\n# Let's check if this is a sharp RD. \nggplot(educ, aes(x = margin, y = school_women, colour =school_women)) +\n  # Make points small and semi-transparent since there are lots of them\n  geom_point(size = 0.5, alpha = 0.5, \n             position = position_jitter(width = 0, height = 0.25, seed = 1234)) + \n  # Add vertical line\n  geom_vline(xintercept = 0) + \n  # Add labels\n  labs(x = \"Vote share\", y = \"Womens' Educational Attainment (Proportion)\") + \n  # Turn off the color legend, since it's redundant\n  guides(color = FALSE)\n```\n\nWe can now see more clearly that most municipalities elected a secular mayor. Recall that we are using the margin of victory for the Islamic parties - a positive margin of victory is positive means that the Islamic parties won the election in that municipality.\n\n</details> \n\\\n\n\n**Exercise 3: Let's generate a dummy variable that is equal to \"Treated\" if the margin of victory *is equal or greater than zero* and \"Untreated\" otherwise. You can use the `mutate()` function and the `ifelse()` functions to do this. Remember to use the pipeline operator to store the variable into your existing data frame. See below the syntax for more information.**\n\n| Function/argument          | Description          |\n|----------------------------|----------------------|\n| ```data <- data %>% ```               | Pipeline operator to assign the new operation into a new data or existing data frame   |\n| ```mutate(new variable = ifelse(variable >= \"condition\", \"Treated\", \"Untreated\") ```     | If the condition is met, the new variable takes value equal to \"Treated\" and \"Untreated\" otherwise|\n\n\n```{r, , eval=F, echo=T}\n\ndata <- data %>% \n  mutate(newvariable = ifelse(variable >= 0 , \"Treated\", \"Untreated\"))\n\n```\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n  \n```{r, eval=F, echo=T}\neduc <- educ %>% \n  mutate(treat = ifelse(margin >= 0 , \"Treated\", \"Untreated\"))\n```\n\n\nNow that we have created our treatment condition variable, let's generate an additional plot that conveys information on the distribution of the running variable for both treated and untreated municipalities. \n\n</details> \n\\\n\n**Exercise 4: Generate a plot looking at the distribution margin of victory variable. Using the `ggplot()` function, set the `x` argument equal to the margin of victory variable. Set `fill` equal the new treatment variable `treat`. There is no need to add the `y` argument given that we expect to generate a histogram that will give us the number of observations for each value in the margin of victory variable. Add the `geom_histogram()` function. Set the argument `binwidth` in this function equal to 0.01 and set `colour` argument equal to \"dark\". Let's add the `geom_vline()` function and add a vertical line by setting the `xintercept` argument equal to zero. Add the `labs()` function and set `x` label equal to \"Margin of victory\" and the `y` label equal  to \"count\", and the `fill` argument equal to \"Treatment Status\".**\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\nggplot(educ, aes(x = margin, fill = treat)) +\n  geom_histogram(binwidth = 0.01, color = \"white\") + \n  geom_vline(xintercept = 0) + \n  labs(x = \"Margin of Victory\", y = \"Count\", fill = \"Treatment Status\")\n\n```\n\\\n\nIn this plot, we can see the distribution of the running variable (margin of victory). Again, we can see that in the majority of the municipalities a secular mayor was elected. \n\n</details> \n\\\n\n\n## Global Parametric Estimation \n\nAs we discussed in the lecture, there are different ways to estimate the causal effect using RD. These different approaches differ in the range of observations they include as well as how they estimate the average outcome for those units just above the cut-off and below the cut-off. One way to estimate the effect of the intervention in RDD is using OLS, but only using the running variable as the main predictor. In this case, we use the running variable measured as the distance from the cut-off. Stated formally:  $\\tilde{X_l} = X - c$. \n\nIn this case, the regression in the left-hand side would be equal to: \n\n$$Y= \\alpha_l + \\tilde{X} + \\epsilon$$\nWhereas the regression above the cut-off is equal to:  \n\n$$Y= \\alpha_r + \\tilde{X} + \\epsilon$$\nIt's important to point out that for all estimations the treatment effect is equal to the differences of the intercepts of the regressions above and below the cut-off. \n\n$$\\tau = \\alpha_r - \\alpha_l$$\n\nLet's do this manually. To do so, subset the data for those observations above and below the threshold. Then, regress the outcome on the **running variable**. Finally, subtract the intercepts from each regression. Let's do that in the following exercise. \n\n**Exercise 5: Run a regression using only `margin` variable as the predictor. Set your outcome variable equal to `school_women` variable. Set the `data` argument equal to `educ` (unless you called your data frame differently). Add the `subset` function inside of the `lm()` function and set it equal to `margin >= 0` for the regression above the cut-off and `margin < 0` for the regression below the cut-off. Use the `summary()` function to report your results, but also store the output of this function into an object. You can retrieve the intercept from the object that you stored the output from the `summary()` function this way:**\n\n\n```{r, eval=F, echo=T}\n\nobject <- summary(lm(outcome ~ variable, data = data, subset = variable >= condition))\n\n# intercept\nobject$coefficient[1]\n\n```\n\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\n# above\nabove <- summary(lm(school_women~margin, data = educ, subset = margin >= 0))\n\nabove$coefficients[1]\n\n# above\nbelow <- summary(lm(school_women~margin, data = educ, subset = margin < 0))\n\nbelow$coefficients[1]\n\n# 0.156453 - 0.162002\ntau_rd = above$coefficients[1] - below$coefficients[1]\ntau_rd\n```\n\nBased on this approach the intercepts of the two regressions yield the estimated value of the average outcome at the cut-off point for the treated and untreated units. This difference of intercepts is the estimated effect of  an Islamic party being in power (at the municipal level) - it suggests there is a 0.5 percentage decrease in women's educational attainment. \n\n</details> \n\\\n\n\nA more direct way of estimating the treatment effect is to run a pooled regression on both sides of the cut-off point, using the following specification: $Y = \\alpha + \\tau D + \\beta \\tilde{X} + \\epsilon$ \n\nWhere $\\tau$ is the coefficient of interest. Here again LATE is the difference between the two intercepts: $\\tau = \\alpha_r -\\alpha_l$.  When $D$ switches off and we are also controlling the different values of the forcing variable, $\\tilde{X}$, we get the slope of the regression below the threshold. Conversely, for units above the cut-off, $D$ switches on, and we control for different values of the forcing variable, we get the slope of the regression above the cut-off. The estimated effect of the treatment at $\\tilde{X}$ then provides the treatment effect ($\\tau$). \n\nNote that you are constraining the slope of the regression lines to the same on both sides of the cut-off. ($\\beta_l = \\beta_r$) This might not be consistent if the data structure varies and the single slope fails to appropriately approximating each side to the cutoff. \n\n\n**Exercise 6: Calculate the effect of the intervention using a regression model including the `margin` and `treat` variables. Use the`lm()` function to conduct this analysis. Store this regression into an object and call it `global2`. Use the `summary()` to inspect your results. Interpret the coefficient of interest.**\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\n# a more direct way is to run a pooled regression on both sides of the cut-off (constraining the slopes)\n\nglobal2 <- lm(school_women~treat+margin, data = educ)\nsummary(global2)\n```\nUsing this estimation approach, we obtain that, on average, the effect of a municipality in control of an Islamic party leads to a 1.7%-point increase in women's educational attainment.  \n\n</details> \n\\\n\nWe can also allow the regression function to differ on both sides of the cut-off by including interaction terms between $D$ and $\\tilde{X}$. This would be as follows:\n\n$$Y = \\alpha_l + \\tau D + \\beta_0\\tilde{X} + \\beta_1 (D \\times\\tilde{X})+ \\epsilon$$\n\n\nLet's do that in the following exercise.\n\n**Exercise 7: Calculate the effect of the intervention using a regression model including the `margin` and `treat` variables and the interaction between these two variables. Use the`lm()` function to conduct this analysis. Store this regression into an object and call it `global3`. Use the `summary()` to inspect your results. Interpret the coefficient of interest.**\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\n# a more direct way is to run a pooled regression on both sides of the cut-off (constraining the slopes)\n\nglobal3 <- lm(school_women~treat+margin + treat*margin, data = educ)\nsummary(global3)\n\n```\nHere, we find that the coefficient of interest is positive (`0.005`) yet insiginifcant. This means that, based on this model, the effect of an Islamic party in control of the municipal government does not lead to a change in women's educational attainment. \n\n</details> \n\\\n\nGlobal parametric models have a severe shortcoming - they rely upon observations that are far away from the cut-off. Indeed, the evidence against using a global polynomial approach is quite substantial. According to Cattaneo et al (2019), this estimation technique does not provide accurate point estimators and inference procedures with good statistical properties.\n\n**Exercise 7 (no coding required): Think about how the global polynomial approach weights each observation when it calculates the coefficient of interest?** \n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n\\\n\nOLS will estimate $\\tau$ based on all observations across the score. This means that the observations' very far from the cut-off weight is equal to that of very close ones'. In the worst-case scenario, if the observations are clustered far from the cut-off, the estimation of $\\tau$ would be heavily influenced by those values rather than those close.\n\n</details> \n\\\n\nWe can also use high-order polynomial to retrieve LATE. However, the evidence against using high-order polynomial seems to be quite robust [see here for a discussion on high-order polynomials)](https://www.tandfonline.com/doi/abs/10.1080/07350015.2017.1366909). In short, the issues with using high-order polynomials is that they leads to noisy estimates, they are sensitivity to the degree of the polynomial, and they have poor coverage of confidence intervals. \n\n\n**Exercise 8: Well - let's give it a try  nonetheless. Conduct a third-order polynomial regression function. Include in this model the `treat` and the `margin` variables. Also, add the  `margin` variable raised at the power of 2 and then at the power of 3. We also need to include the `I()` function or insulate function for the `margin` variable that is raised at the power of 2 and 3. The `I()` function insulates whatever is inside this function. It creates a new predictor that is the product of the margin variable by itself two and three times. Store this output in an object and call that object `global4`. Then, use the `summary()` to check the results of this specification. Interpret the results. **\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\nglobal4 <- lm(school_women~ treat + margin+I(margin^2)+ I(margin^3) , data = educ)\n\nsummary(global4)\n```\n\nUsing a high-order polynomial function, we find that womens' educational attainment decreases, on average, by roughly `2.1` per cent when an Islamic party is in power - yet the result is insignificant.\n\n</details> \n\\\n\nYou might have reason to prefer parametric approaches - or deem them more appropriate in some cases. Then, you should not resort to a global model. It's more appropriate to  only use observations that are close to the cut-off (above and below). Let's run the unconstrained model from **Exercise 7**, but this time we only use observations that are within 0.5 percentage points above and below the threshold. \n\n**Exercise 9: Run the same model used in `Exercise 7`, but subset your data taking only observations that are above and below 0.5 points from the threshold. Store the results from this regression into an object and call it `local`. Use the `summary()` to check your results. If you don't remember how to subset data in the `lm()` function. See the syntax below**\n\n```{r, eval=F, echo=T}\nlm(outcome ~ variable1 + variable2 + variable1 * variable2, data=data,\n           subset=(running_variable>=-0.5 & running_variable<=0.5))\n\n\n```\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\nlocal <- lm(school_women ~ margin + treat + treat * margin, data=educ,\n           subset=(margin>=-0.5 & margin<=0.5))\nsummary(local)\n```\n\n\nWe can see that using a local polynomial function, we find that the effect of Islamic rule is inconclusive in this case. \n\n</details> \n\\\n\n\nIt is important to stress that modern empirical work using  RDDs empirical work employs local polynomial methods. In this case, we are estimating the average outcomes for treated and untreated units using observations that are near the cut-off. This approach tends to be more robust and less sensitive to boundary and overfitting problems. In local polynomial point estimation, we are still using linear regression, but within a specific bandwidth near the threshold. In the following section, we will look at how to use the approach using a non-parametric estimation strategy. \n\n\n---\n\n\n## Non-Parametric Estimation\n\n\nLet's now estimate the LATE using a non-parametric estimator. Conveniently, we can easily do so by using the `rdrobust` package. As the name indicates, the package allows us to estimate robust measurements of uncertainty such as standard errors and confidence intervals. It is based on theoretical and technical work by Calonico, Cattaneo and Titiunik. `rdrobust` estimates robust bias-corrected confidence intervals that address the problem of undersmoothing conventional confidence intervals face in RDDs. In other words, a small bias would be required for them to be valid, which might not be the case. Moreover, they also address the poor performance of (non-robust) bias-corrected confidence intervals. \n\nAs suggested by the authors and somewhat counter-intuitively, we therefore use the point estimate provided by the conventional estimator, but robust standard errors, confidence intervals and p-values to make statistical inferences.\n\nThe `rdrobust` command has the following minimal syntax. You can use a uniform bandwidth or specify two different ones. We will work with further arguments later. Note that you do not need to specify if the running variable is centred on the cut-off as you can manually specify the cut-off using the `c`-argument.\n\n\n```{r, eval=F, echo=T}\n\nrobust_model = rdrobust(data$running_var, data$dependent_var, \n                       c=[cutoff], h=[bandwidth])\n\n\n```\n\n\n**Exercise 10: Estimate the LATE using `rdrobust` with a bandwidth of 5% on either side of the cutoff. Interpret your result.**\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrobust_5=rdrobust(educ$school_women, educ$margin, c=0, h=0.05)\nsummary(robust_5)\n```\n\nOur point estimate is `0.023`. That is, a victory of an Islamic party would be associated with an increase in the rate of women who complete school by `2.3%` - however, the p-value and confidence intervals indicate that the estimate is not statistically significant. Therefore, based on this model, we would conclude that winning an election does _not_ have a an effect on women schooling.\n\n</details> \n\n\\\n\nA bandwidth of 5% seems about reasonable. But we should better check different ones, too. Let's see what happens if we halve the bandwidth.\n\n**Exercise 11: Estimate the same model as before with a bandwidth of 2.5%. Report and interpret your results.**\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrobust_25=rdrobust(educ$school_women, educ$margin, c=0, h=0.025)\nsummary(robust_25)\n```\n\nWell, we can see that the number of observations used to estimate the effect has been reduced - which is reasonable and we should be aware of. The point estimate did not change much and, as before, we find that there is in fact no significant effect at the cutoff.\n\n</details> \n\n\\\n\nBandwidths of 5% or 2.5% around the cutoff seem somewhat reasonable in this case - but so would several others. How can we know what bandwidth we should use to estimate our effect?\n\nRecall the trade-off we are facing when choosing bandwidths that was discussed in the lecture: On the one hand we know that more narrow bandwidths are associated with less biased estimates - we rely on units that are indeed comparable: their distance in the running variable being as-if random the closer we get to the cut-off. On the other hand: the wider the bandwidths, the smaller the variance. As in several cases before, the structure of our data, such as the number of observations, plays an important role. Even if small bandwidths are desirable, it can be hard [impossible] to estimate a robust and significant effect if the number of observations around the cut-off is very small - even if there is a _true_ effect.\n\nLuckily, the `rdrobust` package provides a remedy for this. The packages allows us to specify that we want to use bandwidths that are optimal given the data input. The `rdrobust` command then picks the bandwidth that optimises the _mean square error_ - in other words, _MSE-optimal_ bandwidths. Note that this is the default bandwidth if you don't specify any. Let's try to find out what this would be in our case.\n\n**Exercise 12: Estimate the LATE using `rdrobust` and MSE-optimal bandwidths. To specify the model, replace the `h` argument with `bwselect=\"mserd\"`.**\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrobust_mserd=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"mserd\")\nsummary(robust_mserd)\n```\n\nThis now looks very different. We estimate a LATE of about `3%`, which  is significant at the 90% lvel. Note that the optimal bandwidths has been estimated to be `17.2%-points` on either side of the cut-off, with a separate optimal bandwidth for bias correction. Note that the _MSE-optimal_ bandwidth is optimal in statistical terms - we should always make sure to asses the bandwidths against our theory. Here, we'd compare parties' winning margins up to `17%-points`. \n\n</details> \n\n\\\n\nNote that, so far, we have used a single bandwidth for data below and above the cut-off. We can also specify different ones - both manually and in terms of optimal bandwidths. As the structure of our data might differ, different bandwidths might be optimal. We can specify two different _MSE-optimal_ bandwidth selectors by specifying `bwselect=\"msetwo\"`.\n\n**Exercise 13: Estimate the LATE using `rdrobust` and two MSE-optimal bandwidths. Interpret your results and compare it the model with a single optimal bandwidth.**\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrobust_msetwo=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"msetwo\")\nsummary(robust_msetwo)\n```\n\nThis now looks pretty similar to the single optimal bandwidth, which is a good sign. In fact, the optimal bandwidth for data points above the cut-off remains virtually unchanged. For the ones below the cut-off, the bandwidths is slightly extended. The point estimate and measures of uncertainty also remain virtually unchanged. If specifying two optimal bandwidths alters the results significantly, this is an indicator that the data should be inspected closely for the cause of the diverging bandwidths.\n\n</details> \n\n\\\n---\n\n### Kernels\n\nOne of the advantages of the non-parametric RDD is that observations can be weighted based on their proximity to the cutoff. This is based on the idea that those closer to the cut-off provide better comparisons than those further away. Think of victory margins as a running variable: If we look at fractions of %-points around the cut-off, it is fair to say that victory is _as-if random_. The farther away we move, the less plausible this statement becomes.\n\nAs you learned in the lecture, there are different ways to do so. The default kernel used by the `rdrobust` package is a _triangular_ kernel - which continuously assigns higher weight to observations closer to the cut-off. This is the one we have been using so far as we didn't explicitly specify the kernel. As the figure below shows, other possible options include _epachnechnikov_ and _uniform_ kernels. These can be specified via the `kernel` argument.\n\\\n\n![](kernel.JPG)\n\\\n\n\n**Exercise 14: Estimate the LATE using `rdrobust` and a single MSE-optimal bandwidths but specifying a uniform kernel. Interpret your results.**\n\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrobust_uniform=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"mserd\", kernel=\"uniform\")\nsummary(robust_uniform)\n```\n\nAs compared to the _triangular_ kernel, our point estimate is slightly larger and we find that our robust measures of uncertainty show that the effect is significant at the 95% level. Note that the optimal bandwidth is slightly narrower in the model with a _uniform_ kernel. We have to be aware that this model is less conservative though: We assign the same weight to each observation, independent of its distance to the cutoff - which introduces growing bias over increasing bandwidths.\n\n</details> \n\n\\\n\n---\n\n## Falsification Checks\n\n### Sensitivity\n\nIf you apply an RDD, you should always make sure that results are robust to different specifications of the model. Importantly, this involves _sensitivity checks_. This means you should make sure that results are robust across model specifications and, importantly, different bandwidths.\n\nWhile there are packages that help produce useful plots that visualise robustness across bandwidths, they can be cumbersome to adjust and are not very flexible. Hence, one avenue to create such a plot can be to loop `rdrobust` models over different bandwidths. This solution involves somewhat more coding, but gives you a lot of flexibility as to what you want to display and highlight.\n\n\\\n\n**Exercise 15: Create a plot that shows estimates and confidence intervals for estimates of the effect across different bandwidths.**\n\nThere are several ways to do this. Feel free to play around a bit and try to come up with your own way.\n\n<details>\n  <summary>*Reveal Hint*</summary>\n  \nHint: You could use the following approach:\n\\\n  - Create a data frame with all variables you need for the plot and a and observation for each bandwidth.\n\\\n  - Extract the values from the `rdrobust` output which you need for your plot.\n\\\n  - Loop the regression and the extraction of output over the bandwidths indicated in the initial data frame.\n\\\n  - Save the output in your loop to the respective row in the data frame.\n\\\n  - Plot the output from the newly created data frame.\n  \n</details> \n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\n\nbandwidth <-  seq(from = 0.05, to = 1, by = 0.05)  #create a vector with values for each bandwidth you want to estimate\n\ncoefficient<- NA \nse <- NA\nobs <- NA\nbw <- NA\nci_u <- NA\nci_l <- NA\n\ndata_extract <- data.frame(bandwidth, coefficient, se, obs, bw, ci_u, ci_l) # create a data.frame with all variables you want to incldue in your dataset\n\n# create a loop for each bandwidth that is indicated by 'i'\nfor(i in bandwidth){\n rdbw <- rdrobust(educ$school_women, educ$margin, c=0, h=i) # run the model\n                                      \n# extract the model output (make sure to extract *robust* statistics)\ndata_extract$coefficient[data_extract$bandwidth==i]  <- rdbw$coef[3] \ndata_extract$se[data_extract$bandwidth==i]  <- rdbw$se[3]\ndata_extract$obs[data_extract$bandwidth==i]  <- (rdbw$N_h[1] + rdbw$N_h[2]) \ndata_extract$bw[data_extract$bandwidth==i]  <- (rdbw$bws[1, 1]) \ndata_extract$ci_l[data_extract$bandwidth==i]  <- rdbw$ci[3,1]\ndata_extract$ci_u[data_extract$bandwidth==i]  <- rdbw$ci[3,2]\n                   }\n\n# Make sure the coefficient (and all other values) are numeric\ndata_extract$coefficient  <- as.numeric(data_extract$coefficient)\n\n# Plot the estimates across bandwidths\nggplot(data = data_extract,\n       aes(x = bandwidth, y = coefficient)) +\n  geom_point(size = 0.8) +\n  geom_ribbon(aes(ymin = ci_l, ymax = ci_u), alpha = 0.2) +\n  geom_hline(aes(yintercept = 0), col = \"red\", linetype = 2) +\n  coord_cartesian(ylim = c(-.05, 0.15)) +\n  theme_minimal() +\n  labs(y = \"LATE at Discontinuity\", x = \"Bandwidths (Vote Margin)\")\n\n```\n\n</details> \n\n\\\n\n--- \n### Sorting\n\nThe key identification assumption of RDDs is that there is no _sorting_ on the running variable. That is, the running variable must be continuous around the threshold. If this is not the case, we have a problem: Then there's a good chance that the observations close to the threshold (or on either side of the threshold) are not random. In other words: Presence of sorting is usually interpreted as empirical evidence of self-selection or non-random sorting of units into control and treatment status. Let’s check if sorting is problem here. We can do this using _density checks_. This means we're looking at the density of the running variable.\n\\\n\nThere are several ways to check for sorting. Let's start with _McCrary's density test_. We can use the `DCdenstiy` command to conduct both a visual test and statistical test. The syntax is the following:\n\n```{r, eval=F, echo=T}\n\nDCdensity(running_var ,cutpoint= [cutoff] ,plot=TRUE, ext.out = TRUE)\ntitle(xlab=\"X Lab\",ylab=\"Y Lab\")\nabline(v=0, lty=1) # Adding a vertical line at the cutoff\n\n```\n\n**Exercise 16: Examine the density of the running variable using the `DCdensity` command.**\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nDCdensity(educ$margin ,cutpoint= 0 ,plot=TRUE, ext.out = TRUE)\ntitle(xlab=\"Winning Margin\",ylab=\"Density\") # add labels (always add labels. Really. Always.)\nabline(v=0, lty=1)\n```\n\n\nThe test provides relatively clear visual evidence that sorting is not a problem in this case. There is a clear overlap of the confidence intervals at the cut-off, so the continuity assumption holds. This command provides also statistical tests. You can see that information on the individual cells - or bins - and the number of observations within them is provided. For now, we focus on the inference test for sorting. The p-value of about `0.52` makes clear that we do not reject the null hypothesis of continuity at the cut-off. \n\n</details> \n\n\\\n\nLet's further explore the structure of our data and check if sorting really isn't a problem here. Recall, we want to make sure the density of units should be continuous near the cut-off. We can use the `rddensity` package to do so. This package applies a different test than the one we used before. It also is somewhat more powerful and provides additional information.\n\nThe command is straightforward. You only have to insert the running variable as argument and, if different from 0, the cutoff. Note that you can optionally specify bandwidths, as above, using the `h` argument. Otherwise, an optimal bandwidth will be specified automatically. You can then save this as an object and run the `summary` command to see the output.\n\n\n**Exercise 17: Examine the density of the running variable using the `rddensity` command. Interpret your findings. Are they equivalent to those in the previous exercise?**\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nrdd <- rddensity(educ$margin, c=0)\nsummary(rdd)\n```\n\nThis output looks clearer. Note that the test is based on _quadratic_ local polynomial functions. We also get a different bandwidth - recall that we are solely looking at the density of the running variable here.\n\nImportantly, the test reveals a p-value of `0.357`. We again do not reject the null hypothesis of continuity around the cut-off. Both tests indicating the same result gives us confidence in doing so.\n\nThe second part of the output indicates the results of binomial tests for the allocation of observations around the cut-off for different windows. The number of observations is indicated as well as individual p-values for significance tests, with the null hypothesis being equal distribution on either side of the cut-off. Note that this can give us an indication, but modest unbalance - at such disaggregate level - is not evidence for sorting. \n\n</details> \n\n\\\n\nLet's now plot this density test, too. The `rdplotdensity` function allows us to do so easily. It uses the results of the test conducted in `rddensity`. The syntax is simple:\n\n\n```{r, eval=F, echo=T}\nrdplotdensity([rdd_object] ,[running_var])\n```\n\\\nWe can also add some useful arguments. Feel free to adjust your plot:\n\\\n\n| Argument          | Description          |\n|----------------------------|----------------------|\n| ```plotRange```               | Indicates starting and end point of plot  |\n| ```plotN```               | Number of grid points used on either side of the cutoff  |\n| ```CIuniform```               | True or False. If true, CI are displayed as continuous bands |\n\n\\\n\n**Exercise 18: Use the `rplotdensity` to conduct a visual density check in a data range from `-0.2` to `0.2`. **\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n```{r, eval=F, echo=T}\nplot_rdd <- rdplotdensity(rdd,educ$margin, plotRange = c(-.2, 0.2),  CIuniform = TRUE)\n```\n\\\n\nAgain, we find visual evidence for continuity around the cutoff. Now we can be confident about sorting not being a problem with this data set. \n\n\n</details> \n\n\\\n\n\n### Balance \n\nOne important falsification test is examining whether near the cut-off treated units are similar to control units in terms of observable characteristics. The logic of this test is to identify that if units cannot manipulate the score they receive, there should not be systematic differences between untreated and treated units with similar values of the score.\n\n**Exercise 19: Let's check for balance around the cutoff. Use the `rdrobust()` function. Set the cut-off argument `c` equal to 0, and the `bwselect` argument equal to \"mserd\". In this case, the outcome variables are the covariates that we want to check for balance. Check if there is balance for the following covariates: `log_pop`, `school_men`, `sex_ratio`, `log_area`. Remember to use the `summary()`.**\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\nsummary(rdrobust(educ$log_pop, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$school_men, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$sex_ratio, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$log_area, educ$margin, c=0, bwselect=\"mserd\"))\n\n\n```\n\nWe observe that there is balance across all the covariates of interest.  \n\n</details> \n\\\n\n\n### Placebo cut-offs\n\nAnother falsification test is to identify treatment effects at artificial or placebo cut-off values. Recall from the lecture that one of the identifying assumptions is the continuity (or lack of abrupt changes) of the regression functions for the treated and control units around the cut-off. While we cannot empirically test this assumption, we can test continuity apart from the threshold. Although this does not imply the continuity assumption hold at the cut-off, it helps to rule out discontinuities other than the cut-off.\n\nThis test implies looking at the outcome variable but using different cut-offs where we should not expect changes in the outcome. Thus, the estimates from this test should be near zero (and not statistically significant). One **important** step to conduct this test is that you need to split the sample for those observations that are above the cut-off and those that are below. We do this to avoid \"contamination\" due to the real treatment effects. It also ensures that the analysis of each placebo cut-off is conducted using only observations with the same treatment status. \n\n\n**Exercise 20: Conduct placebo cut-off test using the following placebo values: -0.2 and -0.25 for the placebo tests below the cut-off. Conduct the same test above the cut-off using the following placebo values: 0.20, and 0.25. Use the `rdrobust()` function to perform this test. Set the argument `bwselect` equal to \"mserd\". Replace the cut-off argument `c` with the values listed before. Use the `summary()` to report the results. Remember to create two new data frames. You can use the filter function from tidyverse to do this.**\n\n\n| Function/argument          | Description          |\n|----------------------------|----------------------|\n| ```new_data <- data ```               | Creates a new data frame using the conditions set before the pipeline operator ```%>>%```  |\n| ```filter(variable == \"condition\")```               | Subset the data based on a logical operation|\n\n```{r, eval=F, echo=T}\nnew_data <- data %>% \n  filter(variable >= 0)\n\n```\n\n\\\n\n<details>\n  <summary>*Reveal Answer*</summary>\n\n```{r, eval=F, echo=T}\neduc_above <- educ %>% \n  filter(margin >= 0)\n\neduc_below <- educ %>% \n  filter(margin < 0)\n\nsummary(rdrobust(educ_above$school_women, educ_above$margin, c=0.2, bwselect = \"mserd\")) # -0.044\nsummary(rdrobust(educ_above$school_women, educ_above$margin, c=0.25, bwselect=\"mserd\")) #-0.025\n\nsummary(rdrobust(educ_below$school_women, educ_below$margin, c=-0.2, bwselect=\"mserd\")) # 0.063\nsummary(rdrobust(educ_below$school_women, educ_below$margin, c=-0.25, bwselect=\"mserd\")) # -0.008\n```\n\nWe find that for almost all the placebo cut-off values, the coefficients are all nearly zero and not statistically significant (with the exception of -0.2, which we would want to investigate further). This evidence suggests that the assumption of continuity is likely to hold in this case. \n\n</details> \n\\\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"rdd.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","book":{"title":"Causal Inference","author":"Felipe Torres Raposo","date":"18/09/2024","chapters":["index.qmd","intro.qmd","experiments.qmd","matching.qmd","iv.qmd","diff.qmd","panel.qmd","rdd.qmd","frdd.qmd","summary.qmd","references.qmd"]},"bibliography":["references.bib"],"editor":"visual","theme":"flatly"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"rdd.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"book":{"title":"Causal Inference","author":"Felipe Torres Raposo","date":"18/09/2024","chapters":["index.qmd","intro.qmd","experiments.qmd","matching.qmd","iv.qmd","diff.qmd","panel.qmd","rdd.qmd","frdd.qmd","summary.qmd","references.qmd"]},"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}