[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Preface\nWelcome to this Quarto Book. This book contains some of the seminars I conducted when I taught Causal Inference at the University of Oxford. This book is a work in progress, so any feedback is more than welcome."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "2  Experiments",
    "section": "",
    "text": "3 Details of the experiment\nYou can use the following shortcuts to insert a r chunk: Ctrl + Alt + I or Cmd + Option + I\nbrands=read.csv(\"http://www.spyroskosmidis.net/wp-content/uploads/2018/01/lupubrands.csv\")\n\n# tidy version:\nbrands_tidy &lt;- as_tibble(brands)\nWhat variables does the data include?\nhead(brands) #Checking first 6 rows of the data \n\n# tidy version:\nglimpse(brands_tidy)\n\nbrands_tidy %&gt;%\n  head() %&gt;%\n  knitr::kable()\nAmong these variables, let’s focus on the treatment variables and the outcome. There are four variables associated with the treatments and they are captured by the categorical variableformulario\nThis variable is recoded into the dummies frm2=1 which takes the value 1 if the respondent received the platform information treatment, frm3=1 for alliance switching and frm4=1 for the full text. Note the omitted dummy represents the control group.\nFor now, however, let’s use the original, categorical variable. As it is conventional, we aim to have the control group assigned to value 0.\nbrands$treat=brands$formulario-1  #Assigning value 0 to the control group\n\n# tidy version: \nbrands_tidy &lt;- brands_tidy %&gt;%\n  mutate(treat = formulario - 1)\nDid we define the treatment variable correctly? Let’s check again:\nhead(cbind(brands$formulario, brands$treat, brands$frm2,brands$frm3,brands$frm4))\n\nassignment &lt;- brands_tidy %&gt;%\n  select(formulario, treat, frm2, frm3, frm4)\n\nhead(assignment)\nEverything looks good! Taking the first row as example, when formulario==3 (column1), the treatment variable we generated takes the value 2 (formulario-1) and the dummy for value 3 (frm3) take the value 1, whereas the others are 0.\nThe Model: OLS Estimations\nLupu is interested in the effect of the three treatment conditions on partisanship. Among the two outcomes, pid (whether the individual identifies with a party) and pidstr (a 10 point scale representing the strength of the respondent’s party identification), we will focus below mainly on the latter.\n#3.Let’s beging with a simple OLS model with no covariates:\nsummary(lm(pidstr~treat, data=brands))\n\n# tidy version: \nlin_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(pidstr ~ treat, data = brands_tidy)\n\ntidy(lin_fit)\nDoes this regression make sense? Hint: class(brands$treat).\n#Let's assign labels to each group. This will help with the output!\n\nbrands$treat[brands$treat==0]=\"Control\"\nbrands$treat[brands$treat==1]=\"Platform\"\nbrands$treat[brands$treat==2]=\"Switch\"\nbrands$treat[brands$treat==3]=\"Full\"\n\n\n# tidy version:\nbrands_tidy &lt;- brands_tidy %&gt;%\n  mutate(treat = recode(\n    treat,\n    `0` = \"Control\",\n    `1` = \"Platform\",\n    `2` = \"Switch\",\n    `3` = \"Full\"\n  ))\nRe-running the regression from above and telling R to model treat as a factor variable:\nsummary(lm(pidstr~factor(treat), data=brands))\n\n# tidy version: \nlin_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(pidstr ~ factor(treat), data = brands_tidy)\n\ntidy(lin_fit)\nThe OLS regression results look like mere mean differences. Let’s see if that is true:\npid.control=mean(brands$pidstr[brands$treat==\"Control\"], na.rm=T)\npid.platform=mean(brands$pidstr[brands$treat==\"Platform\"],na.rm=T)\npid.switch=mean(brands$pidstr[brands$treat==\"Switch\"],na.rm=T)\npid.full=mean(brands$pidstr[brands$treat==\"Full\"],na.rm=T)\n\nate.platform=pid.platform-pid.control\nate.switch= pid.switch-pid.control\nate.full=pid.full-pid.control\n# tidy version:\n\npid.control &lt;- brands_tidy %&gt;% # E_Y_X_control\n  filter(treat == \"Control\") %&gt;%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.platform &lt;- brands_tidy %&gt;% # E_Y_X_platform\n  filter(treat == \"Platform\") %&gt;%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.switch &lt;- brands_tidy %&gt;% # E_Y_X_switch\n  filter(treat == \"Switch\") %&gt;%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE))\n\npid.switch &lt;- brands_tidy %&gt;% # E_Y_X_1_full\n  filter(treat == \"Full\") %&gt;%\n  summarise(conditional_mean = mean(pidstr, na.rm = TRUE)) \n\nate.platform=pid.platform-pid.control\nate.switch= pid.switch-pid.control\nate.full=pid.full-pid.control\n\nate.platform\nate.switch\nate.full\nThe ATEs we calculated just now are identical to the OLS slopes we calculated before. So, why do OLS?\nFirstly, we prefer models that can calculate measures of dispersion for the estimates. In other words, through substracting the means by treatment group we would not be able to know whether the ATEs are statistically significant. Secondly, the advantage of using OLS is that we can include -pre-treatment- covariates.\nStill, can you think of other ways to estimate statistical uncertainty without using OLS?\nLet’s see what happens when we add covariates:\nsummary(lm(pidstr~factor(treat)+age+income+educgrp+info, data=brands))\nAre the results the same?\n#Re-estimate the main model\natemod=lm(pidstr~factor(treat), data=brands)\nsummary(atemod)\nnobs(atemod)\n\n#Add covariates\natemodcont=lm(pidstr~factor(treat)+age+income+educgrp+info, data=brands)\nsummary(atemodcont)\nnobs(atemodcont)\n#Why do we lose so many observations? Is this is a fair comparison of the ATE?\n\n#Let's constrain the estimation to the N of the model with the added covariates\nesample=rownames(as.matrix(resid(atemodcont)))\n\natemodsample=lm(pidstr~factor(treat), data=brands[esample,])\nsummary(atemodsample)\nnobs(atemodsample)\n\n#install.packages(\"stargazer\")\nlibrary(stargazer)\n\nstargazer(atemodcont, atemodsample,  type = \"text\")\n\n# sort of a tidy version:\n\nconstrain &lt;- brands_tidy %&gt;% \n  filter(row.names(brands_tidy) %in% esample) \n\natemodsample=lm(pidstr~factor(treat), data=constrain)\nsummary(atemodsample)\nnobs(atemodsample)\nRandomization Checks\nLet’s do some randomization checks. Is the mean value of age similar across treatment groups?\nt.test(brands$age~brands$frm2, data=brands, subset=c(brands$frm3!=1,brands$frm4!=1))\n#and, similarly, for the other treatments\nt.test(brands$age~brands$frm3, data=brands, subset=c(brands$frm2!=1,brands$frm4!=1))\nt.test(brands$age~brands$frm4, data=brands, subset=c(brands$frm3!=1,brands$frm2!=1))\nHow about income?\nt.test(brands$income~brands$frm2, data=brands, subset=c(brands$frm3!=1,brands$frm4!=1))\nt.test(brands$income~brands$frm3, data=brands, subset=c(brands$frm2!=1,brands$frm4!=1))\nt.test(brands$income~brands$frm4, data=brands, subset=c(brands$frm3!=1,brands$frm2!=1))\nOr gender?\ntable(brands$frm2,brands$female[brands$treat!=3 & brands$treat!=2])\nprop.test(table(brands$frm2,brands$female[brands$treat!=3 & brands$treat!=2]))\nprop.test(table(brands$frm3,brands$female[brands$treat!=3 & brands$treat!=1]))\nprop.test(table(brands$frm4,brands$female[brands$treat!=3 & brands$treat!=2]))\n\n# tidy version - need to double check: \nlibrary(broom)\n\nbrands_tidy %&gt;%\n  filter(treat ==  c(\"Switch\", \"Control\")) %&gt;% \n  group_by(frm4, female) %&gt;%\n  summarise(cases = n()) %&gt;%\n  mutate(pop = sum(cases)) %&gt;%\n  rowwise() %&gt;%\n  mutate(tst = list(broom::tidy(prop.test(\n    cases, pop, conf.level = 0.95\n  )))) %&gt;%\n  tidyr::unnest(tst)\nAn equivalent to a ttest is the kolomogorov smirnov test that compares distributions (only works for continuous variables)\n\\[\nD = \\max_{1 \\leq i \\leq N} (F(Y_i) - \\frac{i - 1}{N}, \\frac{i}{N} - F(Y_i))\n\\]"
  },
  {
    "objectID": "experiments.html#what-experiment",
    "href": "experiments.html#what-experiment",
    "title": "2  Experiments",
    "section": "4.1 What Experiment?",
    "text": "4.1 What Experiment?\nYou already know various types of experiments, such as survey experiments, lab experiments, field experiments.\nHow do natural experiments fit in?"
  },
  {
    "objectID": "experiments.html#what-experiment-1",
    "href": "experiments.html#what-experiment-1",
    "title": "2  Experiments",
    "section": "4.2 What Experiment?",
    "text": "4.2 What Experiment?\nYou already know various types of experiments, such as survey experiments, lab experiments, field experiments.\nHow do natural experiments fit in?\n\nIR: Rotating Presidency of the Council of the EU (Carnegie/Marinov 2017)\nCP: Population size & electoral system (Eggers 2015)\nmany others…\nIR/CP: Election Observation (Hyde 2007)"
  },
  {
    "objectID": "experiments.html#data-election-observation",
    "href": "experiments.html#data-election-observation",
    "title": "2  Experiments",
    "section": "4.3 1. Data: Election Observation",
    "text": "4.3 1. Data: Election Observation\nToday we are using data from Susan Hyde’s work on the observer effect in international politics in Armenia.\n\nResearch Question: Do international monitoring missions have an effect on electoral outcomes?\nHyde’s reasoning: Cross-sectional comparisons are not suitable as there will be endogeneity\nHyde’s solution: Moving to the microlevel! (What effect do observers have on election-day fraud?)\n\n2003 Presidential elections in Armenia\n\nThe Experiment: Analysing observed and unobserved polling stations, relying on OSCE missions in terms of incumbent’s vote share\nOutcome: Hyde concludes there was a 5.9% difference in first round and a 2% difference in the second round (simple differences in means)."
  },
  {
    "objectID": "experiments.html#lets-check-this",
    "href": "experiments.html#lets-check-this",
    "title": "2  Experiments",
    "section": "4.4 Let’s check this",
    "text": "4.4 Let’s check this\n\nDo we buy that?\n\nWhat might be the problems associated with this natural experiment?"
  },
  {
    "objectID": "experiments.html#lets-check-this-1",
    "href": "experiments.html#lets-check-this-1",
    "title": "2  Experiments",
    "section": "4.5 Let’s check this",
    "text": "4.5 Let’s check this\nFirst, as always, we load the file (this time from Stata)…\n\nlibrary(readstata13)\nhyde2007 &lt;- read.dta13(file = '/cloud/project/hyde2007_uploadR.dta')\n\n…and look at the data, that is at some variables of interest:\n\nhead(cbind(hyde2007$pollingstation, hyde2007$urban, hyde2007$voterspersqkm, hyde2007$kocharian, hyde2007$mon_voting, hyde2007$mon_voting_R2, hyde2007$KocharianR2, hyde2007$osce_assignment))\n\nExercise: For the first round results, (i) calculate the difference in means of the incumbent’s vote share (kocharian) between observed and unobserved polling stations and (ii) find out whether randomization worked regarding the rural-urban divide (urban)."
  },
  {
    "objectID": "experiments.html#ate-randomization-round-1",
    "href": "experiments.html#ate-randomization-round-1",
    "title": "2  Experiments",
    "section": "4.6 ATE & Randomization: Round 1",
    "text": "4.6 ATE & Randomization: Round 1\n\nplot(density(hyde2007$kocharian[hyde2007$mon_voting==1], na.rm=T, from=min(0, na.rm=T), to=max(1, na.rm=T)))\nlines(density(hyde2007$kocharian[hyde2007$mon_voting==0], na.rm=T, from=min(0, na.rm=T), to=max(1, na.rm=T)))"
  },
  {
    "objectID": "experiments.html#ate-randomization-round-1-1",
    "href": "experiments.html#ate-randomization-round-1-1",
    "title": "2  Experiments",
    "section": "4.7 ATE & Randomization: Round 1",
    "text": "4.7 ATE & Randomization: Round 1\nTo calculate the difference in means, we could use\n\na t.test\n\n\nt.test(hyde2007$kocharian ~ hyde2007$mon_voting)\n\n\nan OLS regression\n\n\nols1=lm(kocharian ~ mon_voting, data=hyde2007)\nsummary(ols1)\n\nWe can do the same (using a prop.test instead of a t.test) in order to check whether randomization worked.\n\nprop.test(table(hyde2007$urban, hyde2007$mon_voting))\n\nols_urban=lm(urban ~ mon_voting, data=hyde2007)\nsummary(ols_urban)\n\nWhat do we conclude? Do we have reason to think this might affect the validity of the results?"
  },
  {
    "objectID": "experiments.html#round-2",
    "href": "experiments.html#round-2",
    "title": "2  Experiments",
    "section": "4.8 Round 2",
    "text": "4.8 Round 2\nLet’s quickly do the same for Round 2:\n\nols2=lm(KocharianR2 ~ mon_voting_R2, data=hyde2007)\nsummary(ols2)\n\nThis looks like a somewhat weaker effect of the observers…\n\nols2_urban=lm(urban ~ mon_voting_R2, data=hyde2007)\nsummary(ols2_urban)\n\nRound 2 seems to be even less balanced in terms of the representation of urban and rural polling stations.\n\nWhat else could be relevant in Round 2? Why should we account for this?"
  },
  {
    "objectID": "experiments.html#round-2-1",
    "href": "experiments.html#round-2-1",
    "title": "2  Experiments",
    "section": "4.9 Round 2",
    "text": "4.9 Round 2\n\nHyde concludes that her “results suggest that if first-round monitoring took place then second-round monitoring had only a marginal additional deterrent effect.” (p.56). She claims that first-round observation has a persistant effect.\nYour thoughts on this?"
  },
  {
    "objectID": "experiments.html#round-2-2",
    "href": "experiments.html#round-2-2",
    "title": "2  Experiments",
    "section": "4.10 Round 2",
    "text": "4.10 Round 2\n\nHyde concludes that her “results suggest that if first-round monitoring took place then second-round monitoring had only a marginal additional deterrent effect.” (p.56). She claims that first-round observation has a persistant effect.\nYour thoughts on this?\n\nLet’s see whether monitoring in the two rounds was independent. First, we check the number of polling stations observed per round.\n\nsum(with(hyde2007, mon_voting==0 & mon_voting_R2==0))\nsum(with(hyde2007, mon_voting==1 & mon_voting_R2==0))\nsum(with(hyde2007, mon_voting==0 & mon_voting_R2==1))\nsum(with(hyde2007, mon_voting==1 & mon_voting_R2==1))\n\nWell, in Round 2 more stations were observed that had been monitored in Round 1 than those that had not been monitored. Let’s confirm significance.\n\nprop.test(table(hyde2007$mon_voting_R2, hyde2007$mon_voting))\n\n\nols2_full=lm(mon_voting_R2 ~ urban + mon_voting, data=hyde2007)\nsummary(ols2_full)\n\nWhat do we conclude?\n\nRound 2 monitoring was not randomized either.\nIf we apply experimental standards (which is what we should do), we conclude that Round 2 monitoring was not independent from Round 1 monitoring."
  },
  {
    "objectID": "experiments.html#non-compliance",
    "href": "experiments.html#non-compliance",
    "title": "2  Experiments",
    "section": "4.11 2. Non-Compliance",
    "text": "4.11 2. Non-Compliance\nOne of the reasons for the imbalance between rural and urban polling station could be that observers did not adhere to their assigned polling stations. Let’s now assume, the OSCE did a good job and assigned polling stations randomly (osce_assignment).\nIn that case, one-sided non-compliance would be a problem. Polling stations that were supposed to be monitored were, in fact, not monitored on election day. Let’s check Hyde’s results once we consider non-compliance to be a problem\n\nattach(hyde2007) # it helps to avoid using the dollar sign\n\nLet’s start with a (some naive) simple differences in means:\n\n#As-treated Analysis\n(mean(KocharianR2[mon_voting_R2==1], na.rm = TRUE) - mean(KocharianR2[mon_voting_R2==0], na.rm = TRUE))\n\n\nlibrary(tidyverse)\n\n\nhyde2007 &lt;- hyde2007 %&gt;%  # remove any NAs\n  drop_na(KocharianR2)\n\nLet’s look at a simple crosstab, showing assigned treatment and actual treatment.\n\n#Compliance Status - Binary\ntab.1 &lt;- table(osce_assignment, mon_voting_R2)\nprint(tab.1)\n\nNow, we calculate the ITT.y. We can do this using differences in means or simply running an OLS.\n\n#ITT_Y\nitt.y &lt;- (mean(KocharianR2[osce_assignment==1], na.rm = TRUE) - mean(KocharianR2[osce_assignment==0], na.rm = TRUE))\nitt.y \n\n\n# ITT_Y using OLS\nlibrary(sandwich)\nlibrary(lmtest)\n\n\nitt_fit &lt;- lm(KocharianR2 ~ osce_assignment)\nsummary(itt_fit)\n#coeftest(itt_fit, vcovHC(itt_fit))  # Heteroscedasticity-Consistent Covariance Matrix Estimation-Consistent Covariance \n\nAccordingly, we calculate the ITT.d\n\n#ITT_D\nitt.d &lt;- (mean(mon_voting_R2[osce_assignment==1], na.rm = TRUE) - mean(mon_voting_R2[osce_assignment==0], na.rm = TRUE))\nitt.d\n\n\n## ITT_D using OLS \nitt_d_fit &lt;- lm(mon_voting_R2 ~ osce_assignment)\nsummary(itt_d_fit)\n# coeftest(itt_d_fit, vcovHC(itt_d_fit)) # Heteroscedasticity-Consistent Covariance Matrix Estimation\n\nLet’s quickly have a look at the compliance status and the broad picture.\n\n#Let's remember the compliance status [1] 883 [2]258 [3] 0 [4]623\ntab.1 &lt;- table(osce_assignment, mon_voting_R2)\nprint(tab.1)\n\n#Always-Takers  \n# Under one-sided non-compliance there are no Always takers, so this should be zero\n# 0 / (883 + 0) = 0 \nat &lt;- tab.1[3]/(tab.1[1] + tab.1[3])  \nat\n\n\n#Never Takers\n# 258/(258 + 623) = 0.29\n# This means that from those polling stations that were assigned to treatment, 29% will not be monitored\nnt &lt;- tab.1[2]/(tab.1[2] + tab.1[4])\nnt\n\n\n#Compliers (No Defiers By Assumption)\n1 - at - nt\n\nFinally, using the ITT.y and the ITT.d , we can calculate the CACE:\n\n#CACE: ITT_Y divided by ITT_D # COMPLIER AVERAGE CAUSAL EFFECT\n(mean(KocharianR2[osce_assignment==1], na.rm = TRUE) - mean(KocharianR2[osce_assignment==0], na.rm = TRUE))/(mean(mon_voting_R2[osce_assignment==1], na.rm = TRUE) - mean(mon_voting_R2[osce_assignment==0], na.rm = TRUE))\n\ncace &lt;- itt.y/itt.d # This should be the same! \nprint(cace)\n\nWe can also calculate the significance of the CACE. To do so, we calculate the standard errors of the CACE (this is in fact, an approximation):\nSE(CACE) ≈ SE(ITT.Y) / ITT.D\n\nse_cace &lt;- summary(itt_fit)$coefficients[4]/itt_d_fit$coefficients \nprint(se_cace)\n\nWe then calculate the p-value as we did last week:\n\nt_cace &lt;- cace / se_cace\nprint(t_cace)\n\np &lt;- (1 - pnorm(abs(t_cace), 0, 1)) * 2\np\n\nAs we will be seeing in a few weeks time, we can also use 2SLS regressions to calculate the CACE.\n\nlibrary(AER)\n\n\n#Via TSLS\nsummary(ivreg(KocharianR2 ~ mon_voting_R2|osce_assignment, data=hyde2007))"
  },
  {
    "objectID": "experiments.html#so",
    "href": "experiments.html#so",
    "title": "2  Experiments",
    "section": "4.12 So?",
    "text": "4.12 So?\n\nWhat do we make of Susan Hyde’s paper?\nIs this a “bad” experiment? Why?\nCan there be “perfect” natural experiments?"
  },
  {
    "objectID": "matching.html#propensity-score-with-replacement",
    "href": "matching.html#propensity-score-with-replacement",
    "title": "3  Matching",
    "section": "3.1 3. Propensity Score, With Replacement",
    "text": "3.1 3. Propensity Score, With Replacement\nNow, let’s use propensity score WITH replacement. Remember, this means that a single control unit can be matched to several treated units. Matching without replacement can yield very bad matches if the number of comparison observations comparable to the treated observations is small. It keeps variance low at the cost of potential bias. Matching with replacement keeps bias low at the cost of a larger variance since you are using the same subjects again and again.\nHow many neighbors should you use? It’s usually better to oversample – use more neighbors. This, however, involves a trade-off between bias and variance: it trades lower variance with less potential bias.\n\nset.seed(7777)\nmatch.ps2&lt;- Match(Y=lalonde$re78,   Tr=lalonde$treat,   X=ps$fitted,\n                  replace=T)\nsummary(match.ps2)\n\n$1314, not great, not terrible. And how’s the balance?\n\nbalancepost=MatchBalance(treat~., match.out=match.ps2,data=lalonde[,-c(1,11)])\n\nLet’s plot the standardized mean differences:\n\nlove.plot(bal.tab(match.ps2, treat~.,data=lalonde[,-c(1,11)]),stat = \"mean.diffs\", \n          threshold = .1, var.order = \"unadjusted\" ,abs=T)\n\n#4. Exercise 2\nSo far the closest we’ve got to the experimental benchmark was with 1642. Let’s try Caliper Matching Now. Caliper Matching is a variant of NN method that attempts to avoid ‘bad’ matches (\\(\\bar{P}_j\\) that are far from our \\(\\bar{P}_i\\). In order to do this, Caliper Matching imposes a tolerance on the maximum distance between the two. If, for some participants, no matches are found within the specified caliper (distance) then they will be excluded from the analysis. Theoretically, Caliper should be better than NN, since it corrects for ‘bad’ matches. Compared to NN, Caliper (also Kernel, LLR) use weighted averages. Caliper for example uses the weighted average over multiple persons within the caliper.\nLet’s estimate Caliper with a distance of 0.1 standard deviations.\n\nset.seed(1111)\nmatch.ps.cal0.1=Match(Y=lalonde$re78,   Tr=lalonde$treat,   \n                          X=ps$fitted, caliper=0.1, replace=F)\nsummary(match.ps.cal0.1)\n\nWe got $1374 That’s worse than before. How’s the balance though?\n\nbal.ps.cal0.1=MatchBalance(treat~+age + educ + black + hispan + nodegree + \n             married + re74+re75+unem74+unem75,\n             match.out=match.ps.cal0.1,data=lalonde)\n\nand plot that\n\nlove.plot(bal.tab(match.ps.cal0.1, treat~age + educ + black + hispan +\n                    nodegree + married + re74 + re75+unem74+unem75,\n                  data = lalonde),stat = \"mean.diffs\", \n          threshold = .1, var.order = \"unadjusted\" ,abs=T)\n\nDoes this look good? Remember we wanted to get as close as possible to $1794. Try again by changing the caliper.\n\n# let's set seed equal to 5555\n# let's try a caliper equal to 0.25  \n# let's output of our Match function  \"match.ps.cal25\"\n\nset.seed(5555)\nmatch.ps.cal25 &lt;- Match(Y = lalonde$re78, Tr = lalonde$treat, X = ps$fitted,\n                        caliper = 0.25, replace = F)\n\nsummary(match.ps.cal25)\n\n\n# let's check for balance before and after matching using a 0.25 caliper. \n\nbal.ps.cal25 = MatchBalance(treat~+age + educ + black + hispan + nodegree +\n                              married + re74 + re75 + unem74 + unem75,\n                            match.out = match.ps.cal25, data = lalonde)\n\n\n# let's generate the balance plot\n\n#5. Mahalanobis Distance\nLike propensity score matching, Mahalanobis distance matching is built on the notion of distance between observations of pretreatment covariates. It uses the complete variance covariance matrix, which means that the relationship between variables is included in the analysis. The contribution to the distance calculation of two highly correlated covariates will then be lower than that of less correlated ones. In essence Mahalanobis distance matching scales the distance by the inverse of the covariance matrix.\n\\[MD(X_i, X_j) = \\sqrt{(X_i - X_j)'\\Sigma^{-1}(X_i - X_j)}\\]\nFor exact match, \\[MD = (X_i, X_j) = 0\\]\n\nlibrary(tidyverse)\nlalonde_cov &lt;- lalonde %&gt;%\n  dplyr::select(-X, -treat, -re78)\n\nset.seed(2222)\nmatch.maha  &lt;-  Match(Y=lalonde$re78,Tr=lalonde$treat,X=lalonde_cov,\n                     BiasAdjust=F,estimand=\"ATT\",M=1, Weight = 2)\n\n\nsummary(match.maha)\n\nWith Mahalanobis, we get $1715, so it’s slightly better than the caliper =0.1 and both NNs matching\nAnd checking balance:\n\nbal.maha  &lt;- MatchBalance(lalonde$treat~.,match.out = match.maha, data=lalonde_cov,ks=FALSE)\n\nVisualizing that in a plot\n\nlove.plot(bal.tab(match.maha, treat~.,data=lalonde[,-c(1,11)]),stat = \"mean.diffs\", \n          threshold = 0.1, var.order = \"unadjusted\" ,abs=T)\n\n\n\n\nMatching technique\nEstimation\nDiff to benchmark\n\n\n\n\nBenchmark\n1794\n0\n\n\nRegression\n-8506\n10300\n\n\nRegression + cov\n1068\n726\n\n\nNN without rep\n1642\n152\n\n\nNN with rep\n1714\n80\n\n\nCaliper (0.1)\n1374\n420\n\n\nCapiler (0.25)\n1839\n45\n\n\nMahalanobis\n1715\n79"
  },
  {
    "objectID": "iv.html",
    "href": "iv.html",
    "title": "4  Instrumental Variables",
    "section": "",
    "text": "5 Seminar Overview\nIn this seminar, we will cover the following topics:\n1. Manually estimate the treatment effect using an instrumental variable and the lm() function\n2. Run an IV regression using ivreg(), iv_robust() and iv_feols()\n3. Present the output of 2SLS regressions\n4. Manually calculate the Wald estimator\n5. Use Placebo tests to support the validity of the IV design.\n6. Check for weak instruments"
  },
  {
    "objectID": "iv.html#lecture-slides-data",
    "href": "iv.html#lecture-slides-data",
    "title": "4  Instrumental Variables",
    "section": "4.1 Lecture Slides & Data",
    "text": "4.1 Lecture Slides & Data\n\n\n\n\n\n#install.packages(\"downloadthis\")\nlibrary(downloadthis)\n\ndownload_link(\n  link = \"https://bayreuth-politics.github.io/CI22/data/dinas.csv\",\n  output_name = \"dinas\",\n  output_extension = \".csv\",\n  button_label = \"Lab 7 Data\",\n  button_type = \"success\",\n  has_icon = TRUE,\n  self_contained = TRUE\n)\n\n#download_link(\n # link = \"https://github.com/bayreuth-politics/CI22/raw/gh-pages/docs/lectures/lecture6.pdf\",\n#  output_name = \"week6\",\n#  output_extension = \".pdf\",\n#  button_label = \"Lecture Slides\",\n#  button_type = \"default\",\n#  has_icon = FALSE,\n # self_contained = FALSE\n#)"
  },
  {
    "objectID": "iv.html#recap",
    "href": "iv.html#recap",
    "title": "4  Instrumental Variables",
    "section": "4.2 Recap",
    "text": "4.2 Recap\nThe Problem of Unobservables\nSo far, we discussed randomised experiments and selection on observables. But what about cases in which we do not (cannot) observe covariates? In such case, the conditional independence assumption does not hold. Take, for example, the following scenario: We would like to estimate the effect of D on Y, but are not able to observe the confounding variable U. Since U affects both the independent and dependent variable of interest, any naive estimate of the effect of D will be biased."
  },
  {
    "objectID": "iv.html#instrumental-variables",
    "href": "iv.html#instrumental-variables",
    "title": "4  Instrumental Variables",
    "section": "4.3 Instrumental Variables",
    "text": "4.3 Instrumental Variables\nAn instrumental variable (IV) design helps us circumvent this problem. If D is partly determined by Z, our instrument, we can estimate the effect of D on Y, despite being unable to observe U. To do so, Z must be determined as-if random and only affect Y through D, i.e. affect the outcome only through the treatment. Thus, we can take advantage of the variance randomly introduced by Z. In other words, IVs only allow us to estimate the effect for compliers - that is, those units whose D is affected by Z. The local average treatment effect - or complier average causal effect is then as follows:\n\\[ LATE = \\frac{E[Y_i|Z_i=1] - E[Y_i|Z_i=0]}{E[D_i|Z_i=1] - E[D_i|Z_i=0]} = \\frac{ITT_Y}{ITT_D} \\]"
  },
  {
    "objectID": "iv.html#two-stages-least-squares-2sls",
    "href": "iv.html#two-stages-least-squares-2sls",
    "title": "4  Instrumental Variables",
    "section": "4.4 Two Stages Least Squares (2SLS)",
    "text": "4.4 Two Stages Least Squares (2SLS)\nIn practice, IV designs are often estimated using 2SLS regressions. As opposed to manual calculations of the treatment effect, these estimators provide correct and robust measures of uncertainty and allow for the inclusion of covariates. The principle is simple: In the first stage, the treatment is regressed on the instrument and, possibly, covariates. The predicted values are then used in the second stage to fit the model. Importantly, both stages always need to include exactly the same covariates.\nFirst Stage : \\[ D_i = \\alpha_1 + \\phi Z_i + \\beta_1 X_{1i} + \\gamma_1 X_{2i} + e_{1i} \\]\nSecond Stage: \\[ Y_i = \\alpha_2 + \\lambda  \\hat{D}_i   + \\beta_2 X_{1i} + \\gamma_2 X_{2i} + e_{2i} \\]"
  },
  {
    "objectID": "iv.html#iv-assumptions",
    "href": "iv.html#iv-assumptions",
    "title": "4  Instrumental Variables",
    "section": "4.5 IV Assumptions",
    "text": "4.5 IV Assumptions\nFor an IV design to be valid, several assumptions have to be met. In practice, this can be very hard to achieve. The five assumptions are:\n\n\nMonotonicity: There are no defiers\n\nExclusion Restriction: The instrument affects the outcome only through the treatment\n\nNon-Zero Complier Proportion: The instrument affects the treatment\n\nRandom Assignment of Z: The instrument is unrelated to potential outcomes\n\nSUTVA\n\nTo satisfy these assumptions, usually good knowledge of context and the particular mechanisms is required. This is particularly the case for the exclusion restriction, which cannot be tested statistically. Accordingly, we must be able to make a convincing case for the assumption to hold. If there are good reasons to believe that the assumption does not hold, the IV design is likely invalid.\n\n\nBefore starting this seminar\n\nCreate a folder called “lab7”\nDownload the data (you can use the button or the one at the top, or read csv files directly from github):\nOpen an R script (or Markdown file) and save it in our “lab7” folder.\nSet your working directory using the setwd() function or by clicking on “More“. For example setwd(“~/Desktop/Causal Inference/2022/Lab7”)\nLet’s install an load packages that we will be using in this lab:\n\n\nlibrary(stargazer) # generate formated regression tables \nlibrary(texreg) # generate formatted regression tables\nlibrary(tidyverse) # to conduct some tidy operations\nlibrary(plm) # conduct one-way and two-way fixed effects \nlibrary(estimatr) #  to conduct ols and provides robust standard errors\nlibrary(lmtest) # calculates the variance-covariance matrix to be clustered by group.\nlibrary(multiwayvcov) # To cluster SEs\nlibrary(ivpack) # Calculates IV models\nlibrary(ivreg)\nlibrary(modelsummary)\nlibrary(fixest)"
  },
  {
    "objectID": "iv.html#does-choice-bring-loyalty",
    "href": "iv.html#does-choice-bring-loyalty",
    "title": "4  Instrumental Variables",
    "section": "5.1 Does Choice Bring Loyalty?",
    "text": "5.1 Does Choice Bring Loyalty?\nToday will work with data from Elias Dinas’ work on Does Choice Bring Loyalty?. In this paper, the author seeks to understand the foundation of partisan strength. There is a general debate over party identification (PID) in the literature. Some scholars claim that party identification strengthens with age. Others, including the author, suggest that voting for a party brings about loyalty and strengthens political attachment. A straightforward but naive empirical strategy would be to estimate the effect of having voted in one election on the strength of party identification a couple of years further down the line. However, both PID and vote choices are predicted by similar confounders - such research design would inevitably face the problem of unobserved covariates.\n\nTo address the research question without uing such naive design, the author takes advantage of a comprehensive panel dataset. The original data include four waves - 1965/1973/1982/1997 -, of which the author uses two (1965 and 1973). In a smart move, Dinas then makes use of the timing of elections and the characteristics of participants in the panel: Elections took place in 1968 and in 1972. To be able to use the effect of voting, Dinas exploited the age of respondents. Importantly, respondents who were born in 1947 (76% of the sample) share a very important characteristic. What is it? They turned 21 - which was the voting age at the time - in 1968. Those who turned 21 before election day were able to vote in 1968, those who did not were only eligible to vote in 1972. This allows the author to exploit respondents’ birthdays - which are random - to causally estimate the effect of voting on the strength of party identification. Obviously, however, not everyone who was eligible to vote in 1968 did vote.\n\n\n\n\nBesides various covariates, we will be using the following key variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\neligible68\nDummy for eligibility to vote in 1968 election (Instrument).\n\n\nvoted68\nDummy indicating whether participant voted in 1968 election (Treatment)\n\n\nstrngpid73\nStrength of party identification in 1973 on an ordinal scale (Outcome)\n\n\nknowledge65\nPolitical knowledge in 1965\n\n\nstrngpid65\nStrength of party identification in 1965\n\n\nelig2false\nDummy variable for placebo tests: 0 for young eligible and 1 for old eligible participants\n\n\nv7\nNumerical code for school (which we’ll use for clustering SEs)\n\n\n\n\n\nNow let’s load the data. There are two ways to do this:\nYou can load the brands dataset from your laptop using the read.csv() function.\n\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab7\")\n# \nlibrary(haven)\n#dinas &lt;- read.csv(\"~/dinas.csv\")\n\nOr you can download the data from the course website from following url: https://bayreuth-politics.github.io/CI22/data/dinas.csv.\n\nExercise 1: Use the head() function to familiarise yourself with the data set.\n\n\n\nReveal Answer\n\n\nhead(dinas)\n\nThis looks wild. The data set includes various variables which are not labeled really well. For now, let’s focus on the key variables presented above.\n\n\n\nExercise 2: Regress the outcome (strngpid73) on the treatment (voted68) using lm(). Does the OLS provide a causal estimate?\n\n\n\nReveal Answer\n\n\nols &lt;- lm(strngpid73 ~  voted68, data= dinas)\nsummary(ols)\n\n\n\nThe naive OLS provides an estimate of 0.19, which means that having voted in the 1968 election is associated with an increase of party identification of 0.19 on the PID scale. However, this does not provide a causal estimate: As we know, there are various factors that are likely to affect both the outcome and treatment. All we can say based on the OLS is the size of the bivariate correlation between these two variables.\nLet’s also look at the visual relationship between these two variables:\n\n# Using ggplot\nggplot(dinas, aes(x=voted68, y=strngpid73)) + \n  geom_point()+\n  geom_smooth(method=lm) +\n  xlab(\"Voted in 1968\") + \n  ylab(\"PID Strength in 1973\")\n\nWe can actually see a slight but significant increase in PID strength for those who voted in 1968. The decisive question is: Can we causally say that having voted is the reason for this?"
  },
  {
    "objectID": "iv.html#iv-regression-2sls",
    "href": "iv.html#iv-regression-2sls",
    "title": "4  Instrumental Variables",
    "section": "5.2 IV Regression: 2SLS",
    "text": "5.2 IV Regression: 2SLS\nWe now know that a simple OLS doen’t provide any causal estimate. Let us now try to estimate the true treatment effect using an instrumental variable design. Following the author, we will be using the eligibility of respondents to vote in the 1968 election (eligible68) as instrument: That is, we exploit the randomness of respondents’ birthdays that determine their eligibility to vote in 1968. To do so, let’s separately look at the first and second stage.\n\nExercise 3: Investigate the relationship between treatment (voted68) and instrument (eligible68)\nThere are several ways to do this. Feel free to pick the option you deem most appropriate.\n\n\n\nReveal Answer\n\n\ntable(dinas$eligible68, dinas$voted68)\n\n\n\nThis looks ok. Of the 623 respondents who were eligible to vote in 1968, 373 did so. 250 decided not to vote. 25 respondents indicated that they voted although they were not eligible. How can that be? Most likely, they simply reported they voted even though they did not. They might have done so intentionally or misremembered the election - which might happen in such a long panel. It’s a bit annoying, but there’s not much we can do about it.\n\n\n\nLet’s now calculate the first stage.\n\nExercise 4: Regress the treatment on the instrument and extract the predicted values\n\nNote: Make sure to add the argument na.action=na.exclude to your lm() function in order to deal with missing values. You can use predicted_values &lt;- predict(OLS_model) to extract the predicted values.\n\n\nReveal Answer\n\n\n# Calculating the first stage. Note `na.action=na.exclude` deals with NAs so we can use the predicted values for the second stage\nfirst=lm(voted68~eligible68, data=dinas, na.action=na.exclude)\n\n# Extracting predicted values\nvote_pred=predict(first)\n\n# Displaying regression output\nsummary(first)\n\n\n\nWe can see that the instrument (eligible68) is indeed a strong and significant predictor of the treatment. That is what we expect and hope for. It’s also convincing to think that eligibility - i.e. respondents’ birthdays - is fully random.\n\nUnfortunately, the first stage cannot tell you whether an instrument is appropriate. However, it can tell you something about inappropriate instruments. A common problem in IV designs are weak instruments. That is, if your instrument is only weakly correlated with the endogenous variable (i.e. the treatment), it is likely to render biased results. The F-Statistic of the first stage can be used to identify weak instruments. As a rule of thumb, your instrument is likely to be problematic if the F-Statistic of your first stage regression is below 10.\nGoing back to the regression output, we see that our F-Statistic here is about 110 - so nowhere near the conventional threshold. Our instrument is strongly correlated with the treatment as it should be - but note that this does not necessarily mean that it is a valid instrument.\n\n\n\nLet’s now proceed to test the exclusion restriction.\nExercise 5: Test the exclusion restriction for the instrument.\n\nHint: Show that the instrument affects the outcome only through the treatment.\n\n\nReveal Answer\n\n\n\nIf you have regressed the outcome on the instrument (and the treatment), this might help familiarise yourself with the data - but it does not provide a test of the exclusion restriction. In fact, it is impossible to statistically test the exclusion restriction. All we can do is rely on theory and build a convincing case for alternative effects not taking place. The problem with a regression of Y on Z (and D) is that we still cannot observe further confounders and account for their effects. We can’t know if their effect does not come into play in such a regression.\nLet’s plot the relationship between the outcome and the instrument nonetheless. As stated above, we can’t tell whether the assumption holds, but we could find that the exclusion restriction is likely to be violated.\n\nExercise 6: Plot the relationship between the outcome and instrument.\nThere are several ways to do this. Feel free to pick the option you deem most appropriate.\n\n\nReveal Answer\n\n\n# Using ggplot\nggplot(dinas, aes(x=eligible68, y=strngpid73)) + \n  geom_point()+\n  geom_smooth(method=lm) +\n  xlab(\"Elegibility in 1968\") + \n  ylab(\"PID Strength in 1973\")\n\n\n\nThis looks as expected. There is no clear and significant association between the two variables. Recall that eligibility itself should not affect party identification strength unless respondents have voted in 1968 as only voting should affect the outcome.\n\n\n\n\n\n\nLet’s now return to our IV model by calculating the second stage of our 2SLS model.\n\nExercise 7: Regress the outcome on the predicted values from the first stage\n\n\n\nReveal Answer\n\n\n# Calculating the first stage\nsecond_wrongSE=lm(strngpid73~vote_pred, data=dinas)\n\n# Displaying regression output\nsummary(second_wrongSE)\n\n\n\nThe second stage uses the predicted values for the treatment from the first stage. Calculating the second stage, the output indicates that - once we instrument for voting in 1968 - the decision to cast a vote in 1968 does not have a significant effect on party identification.\nHowever, calculating the two stages separately we have not adjusted standard errors and measures of uncertainty. Accordingly, hypothesis testing is likely to provide false results if we rely on such biased measures.\n\n\n\n2 IV Regression using 2SL2 in one step\n\nThere are several packages that we could use to retrieve a two-stage least squares instrumental variables estimator. Let’s now conduct 2SLS using the ivreg(), iv_robust(), iv_feols(). See the below the syntax for each of these functions below:\nExercise 8: Conduct a two-stage least squares instrumental variable using strngpid73 as the outcome. voted68 as the endogenous predictor and eligible68 as the instrument. Use the ivreg(), iv_robust() functions. Store these models in a list (list()) and report them in a table. Interpret the results.\n\n\n\nVariable\nDescription\n\n\n\n\nO\nOutcome variable\n\n\nE\nEndogenous variable.\n\n\nI\nInstrument variable.\n\n\nFE\nFixed Effect variable\n\n\n\n\nivreg(O ~ E | I, data = data ) # ivreg package\n\niv_robust(O ~ E | I, data = data) # estimatr package\n\niv_feols(O  ~ E | FE | I, data = data) # \n\n\n\n\n\nReveal Answer\n\n\n## ivreg ## \nivreg_model &lt;- ivreg(strngpid73 ~ voted68 | eligible68, data = dinas)\n\nivreg_model_clustered &lt;- cluster.vcov(ivreg_model, dinas$v7) #This restimates the model and uses clustered SEs.\n\niv_clustered &lt;- coeftest(ivreg_model, ivreg_model_clustered)\n\n## iv_robust ## \niv_robust_model &lt;- iv_robust(strngpid73 ~ voted68 | eligible68, data = dinas, cluster = v7) # cluster by \n\nivmodels &lt;- list(ivreg_model, iv_robust_model)\n\n\nrows &lt;- tribble(~term,  ~ OLS1,  ~OLS2,\n                'Covariates', 'No', 'No') # add one row reporting covariates\nattr(rows, 'position') &lt;- c(5)  ### Change location accordingly  \n\ntitle &lt;- 'Two-stage Least Squares Models' # add the title to your model\n\ncoeffs &lt;- c('(Intercept)'= 'Intercept',\n                     'voted68' = 'Voted') # rename coefficients \n\n# regression table \nmodelsummary(ivmodels, estimate = \"{estimate}{stars}\",coef_map = coeffs, gof_omit = 'DF|se_type', add_rows = rows, title = title)\n\nWe find that both functions generate the same results - note that SEs vary as the first model indicates the unclustered regression output for comparison. Note how the difference is not incredibly large - yet can impact make a difference for hyptohesis testing. The Local Average Treatment Effect is 0.319. [Note that modelsummary struggles to process manually clustered regression output - feel free to use stargazer screenreg instead.]\n\n\n\nIn 2SLS we can include covariates to capture the covariate-adjusted LATE. Let’s include some covariates to the 2SLS. We can also add additional instruments to our model.\nExercise 9: Use the ivreg() function and include the following covariates: col1 and col2. Use the same endogenous treatment variable voted68. Include eligible68 and as.factor(knowledge1965) as instruments. Report the results of this estimation using the summary() function. Include the arguments in the table below to the summary function. Report what is the F-Statistics for this specification. Are the instruments that we using strong or weak instruments?\n\n\n\n\n\n\n\nFunction/argument\nDescription\n\n\n\n\nSummary()\nGeneric function to produce results summaries of fitting functions\n\n\ndiagnostics\nSet equal to TRUE it provides a number of diagnostic test.\n\n\n\n\n\n\n\nReveal Answer\n\n\nivreg_covariates &lt;- ivreg(strngpid73 ~ col1 + col2 + voted68 | \n             col1 + col2 + as.factor(knowledge65) + eligible68, data = dinas)\n\nsummary_ivreg &lt;- summary(ivreg_covariates, diagnostics = TRUE)\nsummary_ivreg\n# Add clustered robust standard errors\nivreg_covariates_clustered &lt;- cluster.vcov(ivreg_covariates, dinas$v7)\ncoeftest(ivreg_covariates, ivreg_covariates_clustered)\n\nBased on the modified speification, we observed that voting in 1968 has a positive and statistically significant effect on partisanship strength. Also from the summary function, we see several diagnostic tests generated once we set diagnostic argument equal to TRUE.\nIf the are more instruments than causal parameters the model is overidentified. If there are as many instruments as causal parameters, the model is just identified. However, if we include more instruments, it is harder to meet the exclusion restriction. One test that we can conduct is the Sagan-Hausman test. This test compares the overidentified model versus a model with a subset of instruments (under the assumption that at least one instrument is valid). The null hypothesis here is that all instruments are valid. In our case, we do not reject this - and conclude that all instruments are valid.\nThe weak instruments test means that the instrument has a low correlation with the endogenous explanatory variable - which would be problematic for the IV design. The null hypothesis is that the instrument insufficiently predicts treatment - which can be rejected in this case. The Wu-Hausman test performs an efficiency test that reports whether the IV estimation is just as consistent as OLS. Therefore the null hypothesis is that OLS estimates are consistent. In this case, we do not reject the null hypothesis. Therefore, we claim that our IV model is as good as OLS - and the latter being preferable as it is more efficient.\n\n\n\nBesides 2SLS, we can also obtain the Local Average Treatment Effect (LATE) by computing the difference of the conditional expectations of the outcome on the instrument (reduced form) divided by the difference of the conditional expectations of the treatment take-up on the instrument (first stage). Put simply, calculating the difference in the mean of the outcome between units assigned to the treatment minus those units not assigned to the treatment. Then, we divide this number by the difference in the share of received treatment in those two groups (i.e. compliance rates).\n\n\n\n\n\n\n\nVariable/Average\nDescription\n\n\n\n\nY\nOutcome\n\n\nZ\nInstrument\n\n\nD\nEndogenous treatment\n\n\nY[Z=1]\nAverage outcome conditional for units offered the treatment\n\n\nY[Z=0]\nAverage outcome conditional for unit not offered the treatment\n\n\nD[Z=1]\nProportion of units receiving the treatment for those assigned to the treatment\n\n\nD[Z=0]\nProportion of units receiving the treatment for those not offered the treatment\n\n\n\nThe Wald Estimator is then:\n\\[\\tau=\\frac{Y[Z=1]-Y[Z=0]}{D[Z=1]-D[Z=0]}\\]\nExercise 10: Manually calculate the Wald Estimator. Use the mean(x, na.rm = T) to calculate the means of each group. You can use the following syntax to obtain the conditional means.\n\nmean(data$outcome[data$instrument == 1], na.rm = TRUE) # 1 for those that voted, 0 for those that didn't vote \n\nmean(data$endongeous_variable[data$instrument == 1], na.rm = TRUE)  # 1 for those that were eligible, 0 for those that were not eligible. \n\n\n\n\n\nReveal Answer\n\n\n#Numerator\nmean(dinas$strngpid73[dinas$eligible68==1], na.rm=T)\nmean(dinas$strngpid73[dinas$eligible68==0], na.rm=T)\n#Denominator\nmean(dinas$voted68[dinas$eligible68==1], na.rm=T)\nmean(dinas$voted68[dinas$eligible68==0], na.rm=T)\n\nThen, \\(\\tau=\\) is equal to:\n\n(mean(dinas$strngpid73[dinas$eligible68==1], na.rm=T) - mean(dinas$strngpid73[dinas$eligible68==0], na.rm=T)) / (mean(dinas$voted68[dinas$eligible68==1], na.rm=T) - mean(dinas$voted68[dinas$eligible68==0], na.rm=T))\n\nWe see that the estimate of the Wald estimator is 0.32, which is pretty close to the estimate obtained from the ivreg() function.\n\n\n\nHow would you compute the Wald estimator for a binary endogenous variable and a binary instrument, but that includes covariates - or for non-binary instruments or endogenous variables?\n\n\n\n\nReveal Hint 1\n\nRemember that the beta coefficient of your variable of interest (let’s call it \\(X_{1i}\\)) and the control variable \\(X_{2i}\\) is equal to:\n\\[\\beta_1 = \\frac{Cov(Y_i, \\tilde{X_{1i}})}{V(\\tilde{X_{1i}})}\\]\n\n\n\n\n\n\n\nReveal Hint 2\n\nThe 2SLS estimator is the ratio of the reduced form divided by the first stage, where \\(\\tilde{Z_i}\\) is the residual from the regression of \\(Z_i\\) on the covariate(s). (The variances are the same, thus they cancel out).\n\\[\\lambda_{\\text{2SLS}} = \\frac{Cov(Y_i, \\tilde{Z_i})}{Cov(D_i, \\tilde{Z_i})}\\] Here we can use the cov() function. You can see the arguments of this function below:\n\n\n\n\n\n\n\nFunction/argument\nDescription\n\n\n\n\ncov(x, y)\nCalculates the covariance between two variables x and y\n\n\nuse\ncharacter indicating how missing values should be treated\n\n\npairwise.complete.obs\nDetermines how the parameters of the covariance function are computed. More details below\n\n\n\nSetting use equal to pairwise.complete.obs computes the mean and variance of x and y using all the non-missing observations. In other words, the correlation between the two variables is calculated using only those observations that both variables have non-missing values.\n\ntau_cov =cov(dinas$eligible68,dinas$strngpid73, use = \"pairwise.complete.obs\")/\n  cov(dinas$eligible68,dinas$voted68, use = \"pairwise.complete.obs\")\ntau_cov\n\n\n\n\nKeep in mind that using IV we can ‘only’ estimate the Local Average Treatment Effect. This means that we are estimating the causal effect for one particular group of treated units, which are the compliers.\n\nExercise 11: Calculate the proportions of compliers, defiers, always-takers, and never takers. Give some labels to the variables, so we can easily identify each group. You can use the factor() function. You can see a description of the syntax below. Assign the following labels to the eligible68 variable: “Not eligible” and “Eligible”. For the voted68 variable “Not voted”, “Voted”. Lastly: why do we actually impose the monotonicity assumption on IV?*\n\n\n\n\n\n\n\nFunction/argument\nDescription\n\n\n\n\nfactor\nTo encode a vector as a factor\n\n\nlevels\nAn optional vector of the unique values\n\n\nlabels\nAn optional character vector of labels for the levels\n\n\n\n\ndata$variable = factor(data$variable, levels = c(1, 2, 3,..,5), \n                       labels = c(\"One\", \"Two\", \"Three\"...\"Five\")) \n\n\n\n\n\nReveal Answer\n\n\ndinas$eligible68n=factor(dinas$eligible68,\n                        levels=c(0,1),\n                        labels=c( \"Not Eligible\", \"Eligible\"))\ndinas$voted68n=factor(dinas$voted68,\n                     levels=c(0,1),\n                     labels=c(\"Not Voted\",\"Voted\"))\n\n\ntable(dinas$eligible68n, dinas$voted68n)\n\nFrom the table above, we can see that the number of respondents that were not eligible and didn’t vote is 132, this group is composed of never-takers and compliers. The 25 subjects are respondents that were not eligible and voted anyway. This group is comprised of always takers and defiers. The 250 are respondents that were eligible but didn’t vote nonetheless. This group is composed of never takers, plus defiers. Finally, we have 373 respondents that were eligible and indeed voted. This group is composed of always takers and compliers.\nBy imposing the monotonicity assumption, we rule out the existence of defiers. This means that 25 respondents that were not eligible to vote and voted anyway are indeed always takers (25/157=0.16). Similarly, there are 250 respondents who are never takers (250/(373+250) = 0.40). Finally, the proportion of compliers in the control group is 1-0.16 = 0.84 and in the treatment group are 1-0.4=0.6. If you remember from the Wald estimator, the proportion of compliers (in the denominator) was 0.44, which is equivalent to 0.6-0.16 = 0.44.\n\n\n\nThere are several diagnostics that we could conduct in order to the validity of an instrument. In particular, we can conduct what is called a placebo test. To test whether the difference in partisan strength is driven by the age gap, the author of this study suggests the following: They split all eligible voters into two groups: the “young” eligibles and the “old” eligibles. The young voters are the ones that were born before May 1947 and the old voters are those that were born since June 1947. It is important to stress that both groups are eligible to vote - so there should not be any difference. Then, the younger group is treated as placebo control group.\nExercise 12: Conduct a placebo test. Use the lm() function and as the main outcome the partisan strength measured in 1973, strngpid73, as well as in 1965, strngpid65. Use elig2false as the placebo treatment variable. Remember to cluster the standard errors. What’s your interpretation of the findings?\n\n\n\n\nReveal Answer\n\n\nplac &lt;- lm(strngpid73 ~ elig2false, data=dinas)\n# Cluster standard errors\nplac.vcovCL &lt;- cluster.vcov(plac, dinas$v7)\ncoeftest(plac, plac.vcovCL)\n\n\nplac2 &lt;- lm(strngpid65 ~ eligible68, data=dinas)\nplac2.vcovCL &lt;- cluster.vcov(plac2, dinas$v7)\ncoeftest(plac2, plac2.vcovCL)\n\nWe see that there is no statistically significant difference between young and old eligible voters. Thus, this suggests that age does not drive the differences in partisanship strength we unravelled earlier.\n\n\n\nExercise 13: Try to think of possible ways how the exclusion restriction could be violated in this setting. Through Which other paths could the instrument affect the outcome, apart from the endogenous treatment?"
  },
  {
    "objectID": "diff.html",
    "href": "diff.html",
    "title": "5  Difference-in-difference",
    "section": "",
    "text": "6 Pararell trends\nAs we know, we cannot test whether the parallel trends, but we can conduct some visual inspects and statistical analyses. One way to do this is to plot the outcome variable for both treated and untreated units overtime before the intervention.\nLet’s first get the mean vote share for the Golden Dawn for the treated and untreated units. Let’s do using Tidyverse using the group_by() and summarise() functions.\nExercise 9: Calculate the mean vote share for the Golden Dawn for the treated and the untreated units for all the elections. Store this into a new data frame and called plot.parallel. Use tidy to create this new data frame.The syntax and the description of the functions below:\nnew.data &lt;- data %&gt;% \n  group_by(variable_1, variable_2) %&gt;% \n  summarise(new_variable = mean(outcome, na,rm = TRUE)) %&gt;% \n  mutate(condition = ifelse(evertr == 1, \"Treatment\", \"Control\") )\nRight, so we see that we have the average vote share for treated and untreated units for each election from 2012 to 2016. Now, let’s plot the trends before and after the intervention.\nExercise 10: Plot the parallel trends. Set vote share for the Golden Dawn in the y axis, year in the x-axis. Connect the data points of the two groups using a line. Place the legend of your plot at the bottom. Change the default colour and use the Wes Anderson Palette. Below you will find all the functions necessary to generate this plot. Remember to use the plus sign between functions.\nNow let’s look at the leads to identify any anticipatory effects. Let’s imagine that the Golden Dawn back in 2012 believed that there was going to be a major humanitarian in the future. Then, they thought that they could exploit this situation to increase their electoral gains. In that case, we wouldn’t be able to disentangle whether changes in vote share are due to the previous campaigning efforts on the part of the Golden Dawn or due to the influx of alyssum seekers to Greece. We can use leads to identify if there are any anticipatory effects. If we find systematic differences between treated and untreated units, this would suggest that units in one or both groups are responding to the treatment before receiving it.\nExercise 11: Create dummy year variables equal to 1 for every year and only for the treated municipality. Call this variable leads, plus the year. For example, the lead2012, will take value 1 only for treated municipalities and only for observations of these municipalities in the year 2012. You can see an example below. use the mutate() function to create these new variables. You can also use the ifelse() function to create these dummy variables. The syntax of the ifelse() function is the following: new variable = ifelse(condition, \"value if condition is met\", \"value if the condition is not met\"). Create these dummy variables for the elections in 2012, 2013, and 2015.\ngreekislands &lt;- greekislands %&gt;% \n  mutate(lead2012 = ifelse(evertr == 1 & year == 2012, 1, 0))\nNow that we have created these dummy variables, we can run a two-way fixed effect model and see if they are anticipatory effects.\nExercise 12: Conduct the same two-way fixed-effect model that we used before, but rather than using the treatment variable, replace this variable with the new leads variables that you created. Ran separate estimations for each lead. Store the outputs of these regressions into different objects. Does the evidence suggest that there are any anticipatory effects? Are the results of these three models statistically significant? You can use the summary() or screenreg() functions to take a look at your results.\nNow, let’s plot all the two-way fixed effects models where we used the plm() function into a single figure.\nExercise 13: Plot the coefficients from the leads models, plus the two-way fixed model for the 2016 election that you used in Question 7 (“twoway_FE”). Use the plot_coef() function to generate this plot. Add the argument scale and set it equal to TRUE. Also, include the argument robust and set it equal to TRUE. In addition to the plot_coefs() include the coord_flip() function. This function will flip the Cartesian coordinates of the plot, so we have the models (years) in the x-axis and the coefficients in the y axis. Remember to add plus sign operator between two functions. You can also add the xlab(\"Year\") function to a label in the x-axis."
  },
  {
    "objectID": "diff.html#data",
    "href": "diff.html#data",
    "title": "5  Difference-in-difference",
    "section": "5.1 Data",
    "text": "5.1 Data\n\n\n\n#install.packages(\"downloadthis\")\nlibrary(downloadthis)\n\ndownload_link(\n  link = \"https://github.com/bayreuth-politics-ci/CI22/raw/gh-pages/docs/data/greekislands.dta.dta\",\n  output_name = \"greekislands\",\n  output_extension = \".dta\",\n  button_label = \"Lab 6 Data\",\n  button_type = \"success\",\n  has_icon = TRUE,\n  self_contained = TRUE\n)\n\n#download_link(\n#  link = \"https://github.com/dpir-ci/CI22/raw/gh-pages/docs/lectures/lecture4.pdf\",\n#  output_name = \"week5\",\n#  output_extension = \".pdf\",\n#  button_label = \"Lecture Slides\",\n#  button_type = \"default\",\n#  has_icon = FALSE,\n#  self_contained = FALSE\n#)"
  },
  {
    "objectID": "diff.html#recap",
    "href": "diff.html#recap",
    "title": "5  Difference-in-difference",
    "section": "5.2 Recap",
    "text": "5.2 Recap\nThe intuition of the DD strategy is to combine two simpler approaches. The first difference (before and after) eliminates unit-specific fixed effects. Then, the second difference eliminates time fixed effects. With this approach, we get an unbiased estimate of a (policy) intervention.\nWe learned that we can break down the difference between treated units and untreated units in post-treatment as the Average Treatment Effect Amongst the Treated (ATT), different time trends and selection bias. However, we can impose additional estimation assumptions to retrieve a credible estimate of the effect of treatment. The key assumption in diff-in-diff studies is the so-called Parallel Trends or Common Trends assumption. This assumption states that in the absence of the treatment/policy, we should expect to see that treated units would follow similar trends to the untreated ones. Unfortunately, we cannot test whether this assumption holds, but we can at least conduct some tests that would be indicated that the Parallel Trends holds.\nWe also learned that we can calculate the ATT in different ways. We learned that we can manually calculate the difference-in-difference estimator.\n\nGroup-period interactions: Here the treatment variable is equal to 1 for all the years since the unit received the treatment. Then, our coefficient of interest is captured by the interaction between the treatment and the time variable.\n\nIf you go back to the Golden Dawn, we can see how we retrieve the beta coefficient from the interaction.\n\\[gdper_{mt} = \\beta_0 + \\beta_1eventr_m + \\beta_2post_t + \\beta_3evertr_m \\times post_t + u_{mt}\\]\n\n\n\n\n\n\n\n\n\nPost = 0\nPost = 1\n\n\n\n\nTreat = 0\n\\(\\beta_0 + u_{mt}\\)\n\\(\\beta_0 + \\beta_2 + u_{mt}\\)\n\n\nTreat = 1\n\\(\\beta_0 +\\beta_1 + u_mt\\)\n\\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 + u_{mt}\\)\n\n\n\nThen, we can get our estimate by the calculating the difference of the outcome variable for both treated and untreated units and, and then subtract these differences:\n\\[((\\beta_0 + \\beta_1  + \\beta_2 + \\beta_3 + u_{mt}) - (\\beta_0 + \\beta_1 + u_mt)) -((\\beta_0 + \\beta_2 + u_{mt})-(\\beta_0 + u_{mt}))\\] \\[(\\beta_2 + \\beta_3) -(\\beta_2)\\] \\[\\beta_3\\] - Unit and time dummies and a treatment indicator: Again, in the case of the Golden Dawn example, we can represent this estimation strategy using the following regression model:\n\\[gdper_{mt} =  \\beta_1treatment_{mt} + \\alpha_m \\text{unit dummy} + \\gamma_t \\text{time dummy} +  u_{mt}\\] For treated before treated \\(Treatment = 0\\):\n\\[gdper_{mt} =  \\beta_1\\times 0 + \\alpha_m \\times 1 + \\gamma_t \\times +  u_{mt}\\] \\[gdper_{mt} = \\alpha_m + \\gamma_t + u_{mt}\\] For treated after treated \\(Treatment = 1\\):\n\\[gdper_{mt} = \\beta_1 +  \\alpha_m + \\gamma_t + u_{mt}\\] Then, we can take the difference before and after:\n\\[(\\beta_1 +  \\alpha_m + \\gamma_t + u_{mt}) - (\\alpha_m + \\gamma_t + u_{mt})\\] \\[\\beta_1\\] Again we are levering the parallel trends assumption by assuming that time-trends are the same for both treated and untreated units.\nFinally, we discussed inference and in particular standard errors. Given that we have repeated observations, this type of data exhibits serially correlated regressors and residuals, we need to make the appropriate adjustments to calculate standard errors. One way is to address this is to use clustered standard errors.\n\nBefore starting this seminar\n\nCreate a folder called “lab6”\nDownload the data (you can use this button or the one at the top, or read csv files directly from github):\nOpen an R script (or Markdown file) and save it in our “lab6” folder.\nSet your working directory using the setwd() function or by clicking on “More“. For example setwd(“~/Desktop/Causal Inference/2022/Lab6”)\nLet’s install an load the packages that we will be using in this lab:\n\n\n\n\nlibrary(jtools) # generate plots with regression coefficients\nlibrary(stargazer) # generate formated regression tables \nlibrary(texreg) # generate formatted regression tables\nlibrary(tidyverse) # to conduct some tidy operations\nlibrary(ggplot2) # to generate plots \nlibrary(plm) # conduct one-way and two-way fixed effects \nlibrary(estimatr) #  to conduct ols and provides robust standard errors\nlibrary(lmtest) # calculates the variance-covariance matrix to be clustered by group.\nlibrary(sandwich) # to calculate heteroscedasticity-Consistent Covariance Matrix Estimation\nlibrary(haven) # upload dta files \n#install.packages(\"wesanderson\")\nlibrary(wesanderson) # Wes Anderson palette (Let's make our plots as Indie as possible)\n#install.packages(\"modelsummary\")\nlibrary(modelsummary)"
  },
  {
    "objectID": "diff.html#seminar-overview",
    "href": "diff.html#seminar-overview",
    "title": "5  Difference-in-difference",
    "section": "5.3 Seminar Overview",
    "text": "5.3 Seminar Overview\nIn this seminar, we will cover the following topics:\n1. “Manually” calculate the difference-in-difference estimator”\n2. Obtaining the difference in difference estimator using the lm() function\n3. Check for parallel trends\n4. Conduct a fixed effect in difference in difference estimation using both lm_robust() and plm() function.\n4. Conduct a placebo test"
  },
  {
    "objectID": "diff.html#waking-up-the-golden-dawn-does-exposure-to-the-refugee-crisis-increase-support-for-extreme-right-parties-dinas-et-al-2018",
    "href": "diff.html#waking-up-the-golden-dawn-does-exposure-to-the-refugee-crisis-increase-support-for-extreme-right-parties-dinas-et-al-2018",
    "title": "5  Difference-in-difference",
    "section": "5.4 Waking Up the Golden Dawn: Does Exposure to the Refugee Crisis Increase Support for Extreme-Right Parties? (Dinas et al, 2018)",
    "text": "5.4 Waking Up the Golden Dawn: Does Exposure to the Refugee Crisis Increase Support for Extreme-Right Parties? (Dinas et al, 2018)\nThe main question of this paper is the following: Did the influx of refugees in Greece increase support for the right-wing Golden Dawn party in 2015?\nThe authors exploit that the Aegean islands close to the Turkish border experienced sudden and drastic increases in the number of Syrian refugees while other islands slightly farther away—but with otherwise similar institutional and socio-economic characteristics—did not.\nWe can see here on the Figure in the left that level of exposure to refugees across the Aegean islands. The figure on the right shows how sudden was the influx of refugees.\n\n5.4.1 Timing\n\nThe refugee crisis started in the spring of 2015\nGreece held an election September, 2015, right after the first wave of refugee arrival, where in the data is coded as 1 for the post variable and coded as year 2016\nThe previous election had taken place only eight months prior in January 2015, before significant number of refugees arrived, which is coded in the data as year 2015.\nTwo more elections were held in 2012. The election in May 2012 is coded as 2012 and the election in June 2013 is coded as 2013.\n\nThe unit of analysis are municipalities\n\nNow let’s familiarise ourselves with the data. A description of some of the variables that we will use today is below:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nYear\nElection Year from 2012 to 2016\n\n\nmunicipality\nMunicipality id\n\n\npost\nA dummy variable that switches on year 2016\n\n\ntreatment\nA dummy variable indicating if the municipality received refugees on the 2016 election\n\n\nevertr\nTreatment variable indicating if the municipality received refugees\n\n\ngdper\nVote share for the Golden Dawn at the municipality level as percentage of total votes\n\n\n\nNow let’s load the data. There are two ways to do this:\nYou can load the dataset from your laptop using the read_dta() function. We will call this data frame greekislands.\n\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab6\")\n# \n#setwd(\"/cloud/project\")\n\nOr you can download the data from the course website from following url: https://github.com/bayreuth-politics/CI22/raw/gh-pages/docs/data/greekislands.dta.dta.\nLet’s start by checking our data as always.\nExercise 1: Use the head() function to familiarise yourself with the data set.\n\n\n\nReveal Answer\n\n\ngreekislands &lt;- greekislands %&gt;%\n   group_by(muni) %&gt;%\n   mutate(treat = max(treatment))\n\n\nhead(greekislands)\n\nThis is what we would expect. Recall that the treat variable indicates municipalities that are treated at some point - independent of the timing -, while the treatment variable marks municipalities after they were treated.\n\n\n\nAs we can see, the data set covers multiple election years. Before working with the data, let’s make sure we know how many and which elections are included in the data.\n\nExercise 2: How many elections, i.e. years are covered by the data?\n\n\n\nReveal Answer\n\n\nunique(greekislands$year)\n\n\nUsing the unique command, we find that four elections are covered by the data: The elections that took place in 2012, 2013, 2015, and 2016.\n\n\n\nLet’s have a look at general voting patterns of the Golden Dawn over time, irrespective of treatment status of municipalities.\nExercise 3: Plot the vote share of the Golden Dawn Party (gdper) over time.\nThere are several ways to do this. For instance, you could plot the dispersion across municipalities by using the boxplot command or, alternatively, calculate average values per year. Feel free to pick the option you deem most appropriate.\n\n\n\nReveal Answer\n\n\n# First option: Boxplot\nboxplot(gdper ~ year, \n        data = greekislands, \n        xlab = \"Election Year\", \n        ylab = \"Golden Dawn Vote Share\", \n        las = 2) # This argument rotates the axis labels\n\n# Second Option: Plot averages\n# Calculating and storing means\naverage_data &lt;- greekislands %&gt;% \n            group_by(year) %&gt;% \n            summarize(gd_averages = mean(gdper))\n\n# Plotting means using ggplot\n\nggplot(average_data, aes(x = year, y = gd_averages)) + \n  geom_point() + geom_line() + xlab(\"Election Year\") + ylab(\"Average GD Vote Share\") + ylim(0,6.5)\n\n\n\nWe can se that the vote share for the Golden Dawn party has remained somewhat stable between the 2012 and 2013 elections, and dropped in the 2015 election to an average value (per municipality) of 4.46%. In the 2016 election, the party’s vote share rose substantively - to a new high of 6%."
  },
  {
    "objectID": "diff.html#differences-in-differences",
    "href": "diff.html#differences-in-differences",
    "title": "5  Difference-in-difference",
    "section": "5.5 Differences-in-Differences",
    "text": "5.5 Differences-in-Differences\n\n\nBeing aware of the general trends of the Golden Dawn’s vote share is an important information about context and the party’s history. However, it cannot tell us anything about the treatment effect we seek to analyse: The arrival of refugees in some Greek municipalities in the summer of 2015.\n\nA naive observer might propose identifying this effect by looking at the differences between treated and untreated units in the post-treatment periods. Would this, however, be an appropriate representation of a possible treatment effect? It clearly would not! Comparing post-treatment differences only doesn’t allows us to account for unit/municipality-specific effects and voting patterns. Treatment, after all, was not assigned randomly. We would not be able to say what the effect of the treatment is unless we can make a statement about how the treatment changed the outcome or resulted in the diversion from a previous trajectory. Using a differences-in-differences design allows us to do so.\nExercise 4: Estimate the treatment effect by calculating differences-in-differences between 2015 and 2016 using the mean() function.\n\nHint: Calculate the differences between treated and untreated units for the years 2015 and 2016 first.\n\n\nReveal Answer\n\n\n# Difference in means between treated and untreated in 2016 (pot-treatment).\npost_difference &lt;- mean(greekislands$gdper[greekislands$treat == 1 & greekislands$year == 2016]) -  mean(greekislands$gdper[greekislands$treat == 0 & greekislands$year == 2016])\npost_difference\n\n# Difference in means between treated and untreated in 2015 (pre-treatment).\npre_difference &lt;- mean(greekislands$gdper[greekislands$treat == 1 & greekislands$year == 2015]) -    mean(greekislands$gdper[greekislands$treat == 0 & greekislands$year == 2015])\n\npre_difference\n\n# Now calculate the difference between the two differences above\ndiff_in_diff &lt;- post_difference - pre_difference\ndiff_in_diff\n\n \nThe difference in the Golden Dawn’s vote share between treated and untreated municipalities has increased in 2016. The differences-in-differences amount to 2.12. This suggests that the treatment (i.e. the arrival of refugees) increased the vote share of the golden dawn by roughly 2%points in the affected municipalities.\n\n\n\nWhile it is important to understand what exactly the difference-in-difference means, we usually do not have to calculate it manually. In fact, we can simply use an OLS to estimate differences-in-differences.\n\nExercise 5: Estimate the difference-in-difference between 2015 and 2016 using an OLS regression.\n\n\nHint\n\nYou can run a simple OLS with the interaction term of treat and a dummy variable for the post-treatment period as independent variables. However, you should restrict the data to the years 2015 and 2016, specifying data = greekislands[greekislands$year&gt;=2015,] as argument for your OLS.\n\n\n\n\n\nReveal Answer\n\n\n# Creating a post-treatment period (i.e. 2016) dummy\ngreekislands$post_treatment &lt;- greekislands$year == 2016\n\n\nols_did &lt;- lm_robust(gdper ~ evertr * post_treatment, data = greekislands[greekislands$year&gt;=2015,])\nsummary(ols_did)\n\nlibrary(broom)\n\nprint(summary(ols_did),digits=max(3, getOption(\"digits\") - 3))\n\nThe estimate for the interaction term, i.e. treated units after they were treated, corresponds to the difference-in-differences. As you can see, the OLS provides exactly the same point estimate as the manual calculation of the difference-in-differences, 2.12 - however, the OLS also provides measures of uncertainty, showing us that the estimate is significant to conventional levels. The OLS provides quite intuitive estimates: The intercept corresponds to the Golden Dawn’s vote share in in untreated municipalities in 2015. Treated municipalities had, on average, a higher vote share for the party by roughly 0.62%points. The parties vote share increased by about 1.27%points in untreated municipalities in 2016.\n\n\n\nNote that there are multiple ways to calculate the difference-in-differences. Fixed effects are usually the preferred approach as they provide the most flexible (e.g. in terms of multiple time periods) and most efficient estimator. Let’s therefore calculate a fixed effects model for the same difference-in-differences as in Exercise 5.\n\nExercise 6: Estimate the difference-in-differences between 2015 and 2016 using a Fixed Effects regression.\n\n_Hint: You can use either lm_robust(), plm() or lm() with dummy variables.\n\n\n\nReveal Answer\n\n\n# lm_robust\nfe_robust &lt;- lm_robust(gdper ~ evertr*post_treatment, fixed_effects= factor(muni), data=greekislands[greekislands$year&gt;=2015,])\nsummary(fe_robust)\n\n# plm\nfe_plm &lt;- plm(gdper ~ evertr*post_treatment, model = \"within\", index = c(\"muni\"), data=greekislands[greekislands$year&gt;=2015,])\nsummary(fe_plm)\n\nfe_dummy &lt;- lm(gdper ~ evertr * post_treatment + factor(muni), data = greekislands[greekislands$year&gt;=2015,])\nsummary(fe_dummy)\n\n\n\nAll three fixed effects estimators provide the same estimate - which corresponds to the one we manually estimated at the beginning of this lab. Note that we could also use the first differences estimator - in the case of two time periods it is equivalent to the fixed effects estimator. However, the fixed effects estimator provides much more flexibility when more than two time periods are being analysed."
  },
  {
    "objectID": "diff.html#generalised-diff-in-diff",
    "href": "diff.html#generalised-diff-in-diff",
    "title": "5  Difference-in-difference",
    "section": "5.6 Generalised Diff-in-Diff",
    "text": "5.6 Generalised Diff-in-Diff\nLet’s now extend our analysis by including all pre-treatment periods in our analysis. The easiest way to do so is running a two-way fixed effects regression.\n\nExercise 7: Estimate the difference-in-differences using a two-way Fixed Effects regression with all time periods and treatment as independent variable.\n\n\n\nReveal Answer\n\n\ntwoway_FE &lt;- plm(gdper ~ treatment, effect=\"twoways\",model=\"within\",index=c(\"muni\",\"year\"), data=greekislands)\nsummary(twoway_FE)\n\n\n\nAs you can see, the estimate is now slightly smaller than the one we got by comaparing 2015 and 2016 only. This is not surprising as the regression considers all pre-treatment periods now and, therefore, attributes some of the variance to other factors such as unit-specific trends. However, it is noteworthy that the change is negligible. We still estimate that the treatment effect, or ATT, is roughly above 2%points.\n\n\n\n\nExercise 8: Calculate robust standard errors for (i) the plm FE model, (ii) the two-way FE model and present the regression output in a single table. Include the simple OLS model in the table.\n\nNote: There is no need to adjust standard errors after using lm_robust() as the command automatically does that.\n\n\n\nReveal Answer\n\n\n### ADJUSTING STANDARD ERRORS\n## plm FE model\nfe_plm_se &lt;- coeftest(fe_plm, vcov = vcovHC(fe_plm, type = \"HC1\",\n                                         cluster = \"group\"))\nfe_plm_se\n\n## two-way FE model\n\ntwoway_FE_se &lt;- coeftest(twoway_FE, vcov = vcovHC(twoway_FE, type = \"HC1\",\n                                         cluster = c(\"group\", \"time\")))\ntwoway_FE_se\n\n\n## CREATING A REGRESSION TABLE\n\nscreenreg(l=list(ols_did, fe_plm_se, twoway_FE_se), custom.header = list(\"OLS (2-period)\" = 1, \"FE (2-period)\" = 2, \"2-way FE\" = 3), custom.coef.map = list(\"treatment\" = \"Treatment\", \"evertr:post_treatmentTRUE\" = \"Treatment (Interaction)\"))"
  },
  {
    "objectID": "diff.html#placebo-test",
    "href": "diff.html#placebo-test",
    "title": "5  Difference-in-difference",
    "section": "6.1 Placebo test",
    "text": "6.1 Placebo test\nWe can conduct a placebo test to evaluate whether the parallel trend holds. We are trying to prove that there is no clear difference in trending tendencies between treated and untreated municipalities.\nThe steps to conduct are the following:\n\nUse data for periods that came before the treatment was implemented/happened 2 Create a dummy variable for each before treatment that is equal to 1 only for that specific year and only for treated units. (As we did before)\nEstimate the difference-in-difference using plm() or lm_robust() function.\nIf you find statistically significant results, this may suggest a violation of parallel trends.\n\nExercise 14: Drop all the observations of the year of the intervention (2016). Do this using the filter() function. Then, create a fake treatment variable and call it post2 and set it equal to 1 for all observations in year 2015. This variable would indicate as the hypothetical case that the municipality would received refugees in 2015. You can see an example below.\n\ngreekislands &lt;- greekislands %&gt;% \n  filter(variable != 2016) %&gt;% \n  mutate(new variable = year variable == \"year\")\n\n## or alternatively \n\ngreekislands &lt;- greekislands %&gt;% \n  filter(variable != 2016) %&gt;% \n  mutate(new.variable = ifelse(year variable == \"year\", 1, 0))\n\n\n\n\n\nReveal Answer\n\n\n\n\ntable(greekislands$year)\n\ngreekislands &lt;- greekislands %&gt;%\n  filter(year != 2016) %&gt;% \n  mutate(post2 = ifelse(year == \"2015\", 1, 0))\n\nGreat! Now let’s use this dummy variable to perform the placebo test. We are essentially going to test is that there are no differential trends between treated and untreated, as we did before when we looked at anticipatory trends.\n\n\n\nExercise 15: Conduct the same two-way fixed effect model using the lm() and use the post2 variable. Store all the models in a list and plug this list inside of the modelsummary(list) function to report your results. Did you find statistically significant differences between treated and untreated units pre-treatment?. You can see an example of how to store multiple models in a list list(). Also, subset the data so one model will only conduct the placebo test using the 2012 and 2015 elections, and another model only using the observations from the 2013 and 2015 elections.\n\nmodels &lt;- list(\n    \"plm \"= lm(outcome ~ treatment, model = \"within\", effect = \"twoways\", index = c(\"muni\", \"year\"), data = data[data$year != year,]), \n    \"lm\" = lm(outcome ~ outcome, data = data, subset=(year!=year)), # set year equal to year that you want to exclude. \n    \"lm_robust\" = lm_robust(outcome ~ treatment, data = data[data$year != year,], cluster = unit)) # set year equal to year that you want to exclude. \n\n\n\n\n\nReveal Answer\n\n\n\n\nmodels &lt;- list(\n     \"1215\"=lm_robust(gdper ~ evertr + post2+evertr*post2, data = greekislands, subset=(year!=2013), cluster=muni), # placebo test 2012 and 2015\n    \"1314\" = lm_robust(gdper ~ evertr + post2+evertr*post2, data = greekislands, subset=(year!=2012),cluster=muni)) # placeblo test 2013 and 2014\n\n\nmodelsummary(models)\n\nWe find that there are no systematic differences between treated and untreated municipalities. We find near to zero coefficients.\n\n\n\n\n\n\n\nPre\nPost\nDifference\n\n\n\n\nFar\n\n\n\n\n\nTreated\n4.18\n5.01\n0.83\n\n\nControl\n5.12\n5.68\n0.56\n\n\nDifference\n-0.94\n-0.67\n\n\n\nDiff-in-Diff\n0.27\n\n\n\n\n\n\n\n0.95\n\n\nClose\n\n\n\n\n\nTreated\n5.87\n8.71\n2.84\n\n\nControl\n3.72\n5.34\n1.62\n\n\nDifference\n2.15\n3.37\n\n\n\nDiff-in-Diff\n1.22"
  },
  {
    "objectID": "panel.html",
    "href": "panel.html",
    "title": "6  Panel Data",
    "section": "",
    "text": "7 Recap\nIn this seminar, we will cover the following topics:\n1. Conduct a time fixed effects model using the lm() function\n2. Conduct a time and unit fixed model using the lm() function\n3. Conduct a time fixed effects model using the plm() function\n4. Conduct a time and unit fixed effect model using the plm() function.\n4. Correct standard errors obtained from the plm() function.\n5. Conduct a Hausmann test and F-test"
  },
  {
    "objectID": "panel.html#omitted-variable-bias",
    "href": "panel.html#omitted-variable-bias",
    "title": "6  Panel Data",
    "section": "7.1 Omitted variable bias",
    "text": "7.1 Omitted variable bias\nUsing panel data helps us partly circumvent the issue of omitted variable bias, rather than controlling for all potential time-invariant confounders. We can simply include fixed effects that capture all of these confounders - whether observed or unobserved."
  },
  {
    "objectID": "panel.html#time-fixed-effects",
    "href": "panel.html#time-fixed-effects",
    "title": "6  Panel Data",
    "section": "7.2 Time-fixed effects",
    "text": "7.2 Time-fixed effects\nWe know that bias can come from variables that change over time across all units. To minimise bias from these factors, we can include in our model time-effect controls for any common impact that affects all units. Put differently, time-fixed effects are time-specific factors that affect all entities at the same point intime but evolve over time."
  },
  {
    "objectID": "panel.html#entityunit-fixed-effects",
    "href": "panel.html#entityunit-fixed-effects",
    "title": "6  Panel Data",
    "section": "7.3 Entity/Unit-fixed effects",
    "text": "7.3 Entity/Unit-fixed effects\nA model “unit-fixed effects” is a model to which we have added a dummy variable for every unit (county, individual, or state). By adding these to our model, we control for all differences between units that do not change over time. Unit fixed effects are essentially factors that change from one unit to another, but remain constant over time - this includes all time-invariant unobserved (and, importantly, unobservable) heterogeneity. Put differently, entity or unit-fixed effects are factors that are constant over time, but vary within entities.\nThe coefficient of each dummy variable shifts the intercept, but does not affect the slope.\n\\[Y_{i,t} = \\beta_1 X_{i,t} + \\alpha_i + \\mu_{i,t}\\]"
  },
  {
    "objectID": "panel.html#two-way-fixed-effects",
    "href": "panel.html#two-way-fixed-effects",
    "title": "6  Panel Data",
    "section": "7.4 Two-way fixed effects",
    "text": "7.4 Two-way fixed effects\nWe can combine both time and unit fixed effects at the same time. These models are referred as Two-way fixed effects models. By including both fixed effects we are reducing bias that comes from:\n\nFactors that are constant, but differ across each unit.\nFactor that vary over time, but affect all units at the same time."
  },
  {
    "objectID": "panel.html#diagnosis-test-and-standard-errors",
    "href": "panel.html#diagnosis-test-and-standard-errors",
    "title": "6  Panel Data",
    "section": "7.5 Diagnosis Test and Standard errors",
    "text": "7.5 Diagnosis Test and Standard errors\nUsing panel data we cannot assume that regression errors are independent of each other. Indeed, standard errors are likely correlated within the same unit or the same year. Therefore, we need to cluster standard errors to adjust/account for this correlation. Using Clustered standard errors we allow for heteroskedasticity and autocorrelation within an entity, but treat the errors as uncorrelated across entities.\n\nBefore starting this seminar\n\nCreate a folder called “lab5”\nDownload the data (you can use this button or the one at the top, or read csv files directly from github):\nOpen an R script (or Markdown file) and save it in our “lab4” folder.\nSet your working directory using the setwd() function or by clicking on “More“. For example setwd(“~/Desktop/Causal Inference/2022/Lab4”)\nLet’s install an load the packages that we will be using in this lab:\n\n\nlibrary(jtools) # generate plots with regression coefficients\nlibrary(stargazer) # generate formated regression tables \nlibrary(texreg) # generate formatted regression tables\nlibrary(tidyverse) # to conduct some tidy operations\nlibrary(ggplot2) # to generate plots \nlibrary(plm) # conduct one-way and two-way fixed effects \nlibrary(estimatr) #  to conduct ols and provides robust standard errors\nlibrary(lmtest) # calculates the variance-covariance matrix to be clustered by group.\nlibrary(sandwich) # to calculate heteroscedasticity-Consistent Covariance Matrix Estimation\nlibrary(haven) # upload dta files"
  },
  {
    "objectID": "panel.html#the-republicans-should-pray-for-rain-wheather-turnout-and-voting-in-u.s.-presidential-elections",
    "href": "panel.html#the-republicans-should-pray-for-rain-wheather-turnout-and-voting-in-u.s.-presidential-elections",
    "title": "6  Panel Data",
    "section": "8.1 The Republicans Should Pray for Rain: Wheather, Turnout, and Voting in U.S. Presidential Elections",
    "text": "8.1 The Republicans Should Pray for Rain: Wheather, Turnout, and Voting in U.S. Presidential Elections\nToday, we will work with data from Gomez et al work on The Republicans Should Pray for Rain: Wheather, Turnout, and Voting in U.S. Presidential Elections. In this paper, Gomez et al seek to answer the following question: Does bad weather affect turnout? To address this question, they looked at 14 U.S presidential elections. Using GIS interpolations and weather data from 22,000 U.S weather stations, they provide election day estimates of rain and snow at the county level.\n\n\nThe authors find that rain significantly reduces voter participation by a rate of just less than 1% per inch. Meanwhile, an inch of snowfall decreases turnout by almost .5%. The authors also find that poor weather benefits Republicans. The authors’ main outcome of interest is vote turnout at the county level.\n\n\nTheir data consists of over 3,000 counties in the continental United States for each presidential election from 1948 to 2000. The meteorological variables were drawn from the National Climatic Data Center that contained Day data report of various measures of the day’s weather, including rainfall and snow, for over 20,000 weather stations. In total the authors collected up to 43,340 observations (N = 3,115, max, and T = 14 elections).\n\n\nThe main “treatment” variable of this study is estimated rainfall and snowfall measured in inches (1 in = 2,54 cm). The authors used two alternative measures of weather. First, they calculated the normal (average) rainfalls and snowfalls for each election data for each county using data from 1948-2000 time span. Then, they subtracted the appropriate daily normal value from the rainfall or snowfall estimated to have occurred on each election day under analysis.\n\n\nThe authors control for a number of socio-economic factors that are associated with voter turnout such as High School Graduates in the county, median household income, and how rural the county is.\n\n\nThe authors find when measured as deviations from their normal values, rain and snow elicit a negative and statistically significant effect on voter turnout. Thus, evidence supports the claim that bad weather lowers voters turnout. The authors also find that when rain and snow increase above their respective election day normal, the vote share of Republican presidential candidates increases.\n\nA description of some of the variables that we will use today is below:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nYear\nElection Year\n\n\nState\nState name\n\n\nCounty\nCounty name\n\n\nFIPS_County\nCounty id\n\n\nTurnout\nTurnout presidential election as a percentage of Voting-age population\n\n\nRain\nGIS rainfall estimate\n\n\nSnow\nGIS snowfall estimate\n\n\nRain_Dev\nDeviation from normal rainfall\n\n\nSnow_Dev\nDeviation from normal snowfall\n\n\nGOPVoteShare\nRepublican Candidate’s Vote Share\n\n\nPcntBlack\nEstimated percentage of Black population\n\n\nZPcntHSGrad\nEstimated percentage high school grads, standarised by years at the county level\n\n\nFarmsPerCap\nFarms per capita\n\n\nUnemploy\nEstimated Unemployment Rate\n\n\nAdjIncome\nCPI Adj Median Household Income (1982-84=100), in 10000s\n\n\nClosing\nlength of time between date voter must register and election day\n\n\nLiteracy\nProportion of legislator’s children that are female\n\n\nPoll Tax\nLiteracy test\n\n\nClosing\nPoll tax\n\n\nYr52\nDummy election 1952\n\n\nYr56\nDummy election 1956\n\n\nYr..\nDummy election …\n\n\nYr2000\nDummy election 2000\n\n\n\nNow let’s load the data. There are two ways to do this:\nYou can load the dataset from your laptop using the read_dta() function. We will call this data frame weather.\n\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab5\")\n# \n#setwd(\"/cloud/project\")\n\nOr you can download the data from the course website from following url: https://github.com/bayreuth-politics/CI22/raw/gh-pages/docs/data/Weather_publicfile.dta.\n\nLet us start with a few descriptive statistics to become familiar with the data. First, as always, we use the head() function to get an impression of the data\n\nhead(weather)\n\n\n\nNow, let’s have a look at the number of elections that are included in the data set. We know that the authors are looking at presidential elections, so each unique year in the data can cover only one and the same presidential election. We use the unique() command to retrieve unique values of the Year variable. Then, you can use the length() command to display the number of unique values.\n\n\n# Retrieving unique values of the variable\nunique(weather$Year)\n\n# Retrieving the number of the unique values of the variable.\nlength(unique(weather$Year))\n\n\n\nSo the data set covers 14 different presidential elections - as the unique() command shows us, these are all elections that took place between 1948 and 2000. Let’s do the same now for the number of states.\n\nExercise 1: How many US states are covered by the data - which ones are missing?\n\n\n\nReveal Answer\n\n\n# Retrieving the number of the unique values of the variable.\nlength(unique(weather$State))\n\n# Retrieving unique values of the variable\nunique(weather$State)\n\n\n\nThe data cover 48 US states, so two are missing. We can see that the data cover all contiguous US states - the two missing ones are Alaska and Hawaii. Is there a reason to exclude them?\n\nThere is! Alaska and Hawaii joined the Union only in 1959 - so the authors would have had shorter time-series for these two states."
  },
  {
    "objectID": "panel.html#the-times-are-a-changin-pooled-ols-v-fixed-effects",
    "href": "panel.html#the-times-are-a-changin-pooled-ols-v-fixed-effects",
    "title": "6  Panel Data",
    "section": "8.2 The Times are a changin’: Pooled OLS v Fixed Effects",
    "text": "8.2 The Times are a changin’: Pooled OLS v Fixed Effects\n\n\nLet’s start by paying attention to the temporal dimension of our voting data. Conventional wisdom has it that each and every election is different. Accordingly, we want to acknowledge these differences across elections and take them into account when estimating our models.\nLet’s first have a look at the broad picture by plotting the outcome of interest across time. We can use the boxplot() function to this. The syntax is the following boxplot(Y ~ X, data= data, xlab=\"X Label\", ylab=\"Y Label\").\n\nExercise 2: Plot GOPVoteShare over time\n\n\n\n\nReveal Answer\n\n\nboxplot(GOPVoteShare ~ Year, \n        data = weather, \n        xlab = \"Election Year\", \n        ylab = \"GOP Vote Share\", \n        las = 2) # This argument rotates the axis labels\n\n\n\nThe vote share cast for the GOP varies widely. Note that the data set includes an observation for each county. The box indicates the interquartile range (i.e. the difference between the third and first quartile of data points) and the solid lines the median value for each year. There are meaningful differences: Note how GOP vote share is larger in 1972, when McGovern (ever heard of him?) had no chance against Nixon. Importantly, we still see a rather broad range of GOP vote shares - this is the variation introduced by individual counties and their specifics. Considering only temporal differences here, we do not distinguish any other effects.\n\n\n\n\nLet’s now run a simple OLS model without considering any variation introduced by temporal differences or unit (i.e. county) specifics. Such a model is called pooled OLS as we simply pool all observations in our data set to estimate a coefficient. By design, all observations are considered to be independent of each other in such model.\n\nExercise 3: Regress GOPVoteShare on Rain\n\nEstimate two models: First, a simple bivariate model and second a model that includes important covariates: PcntBlack, ZPcntHSGrad, FarmsPerCap, AdjIncome and Literacy. Interpret your model estimates.\n\n\n\n\nReveal Answer\n\n\n# Bivariate pooled OLS\nols_bivariate &lt;- lm(GOPVoteShare ~ Rain, data=weather)\nsummary(ols_bivariate)\n\n# Multivariate pooled OLS\nols_covariates &lt;- lm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, data=weather)\nsummary(ols_covariates)\n\nThe bivariate model estimates a - hardly significant - association between rain and GOP vote share of 0.5%-points per inch. As we know, however, such bivariate association does not account for several other effects. Indeed, all covariates included in the multivariate pooled OLS are highly correlated with GOP vote share - and the estimate of rain on election day doubles to about 1%-point in the multivariate model.\n\n\n\nLet’s now move away from the pooled - or naive - OLS model. We know that despite accounting for the covariates, the multivariate model does not account for variance from two important sources: (i) general temporal development, (ii) unit-specific time-invariant variance. In a first step, we account for the former. Let’s see how acknowledging general election effects changes our estimate.\nIn other words, we will now estimate a fixed-effects model. You can use the lm() function to include time-fixed effects. If you add the Year variable as factorised variable, a dummy variable for each year is added to the regression equation. In other words, we allow each presidential election to have a different intercept but estimate a single slop for the independent variables.\n\nExercise 4: Estimate the same bivariate and multivariate models as above including time-fixed effects.\n\n\n\n\nReveal Answer\n\n\n\nNote that this is not the same as simply adding the Year variable as an independent variable. If you did the latter, - with Year being a continuous variable - R would simply understand it to be a single, general and continuous time trend.\n\n\n# Bivariate time FE\nFE_bivariate &lt;- lm(GOPVoteShare ~ Rain + factor(Year), data=weather)\nsummary(FE_bivariate)\n\n# Multivariate time FE\nFE_covariates &lt;- lm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy  + factor(Year), data=weather)\nsummary(FE_covariates)\n\n\n\nDo you notice the vast differences of our estimated effect of Rain? In the bivariate FE model, the estimated effect is negative, so one inch of rain would be associated with a 1%-point decrease in GOP vote share. Once we control for the same set of covariates as above, the model estimates that an increase in one unit ofRain is not associated with GOP vote share at all!\n\n\n\nLet’s pause a moment to make sure we grasp the difference between a pooled OLS and the FE estimator. Simply speaking, the pooled OLS considers each observation but does not differentiate them in any particular way. Thus, the bivariate model simply estimates the mathematical association between the two variables, using all data points:\n\n\nrequire(modelr)\nrequire(ggplot2)\ngrid &lt;- data.frame(Intercept=1, Rain=seq_range(weather$Rain, 10))\ngrid$pred &lt;- predict(ols_bivariate,grid)\nggplot(weather, aes(Rain)) +\n  geom_point(aes(y = GOPVoteShare)) +\n  geom_line(aes(y = pred), data = grid, colour = \"red\", size = 1)\n\n\n\nThe logic of the FE estimator is different, however. This estimator looks at and compares data points within a certain period. It estimates a single coefficient (or slope) for independent variables, but allows each period to have an individual intercept, thus capturing general time trends that affect all units. Let’s look at the coefficient of Rain in the bivariate FE model now:\n\n\nweather$fYear &lt;- factor(weather$Year) # Creating a factor variable makes it easier to plot\n\n\ngrid_fixdum &lt;- expand(data.frame(Intercept=1, Rain=seq_range(weather$Rain, 14), Year=unique(weather$fYear)),\n                      Rain, Year)\ngrid_fixdum$pred &lt;- predict(FE_bivariate,grid_fixdum)\nggplot(weather, aes(Rain)) +\n  geom_point(aes(y = GOPVoteShare, colour=fYear)) +\n  geom_smooth(aes(y = GOPVoteShare), method='lm', se=FALSE, colour=\"black\")+\n  geom_line(data=grid_fixdum, aes(x=Rain, y=pred, colour=Year))\n\n\n\nIn fact, looking only at the effects within Years, we can see that the aggregate slope is even slightly negative!\n\nEven with our FE estimates, however, there is an important problem. Using the lm() command, R estimates the time dummies as additional variables. This affects the measures of uncertainty and, therefore, statistical significance as observations within the same year are not independent. In fact, we want to estimate FE models precisely because we have reason to believe that they are not independent of each other.\n\nTo adjust for this dependence of observations, we must calculate robust - or clustered or panel-corrected - standard errors. This is important and should always be done when estimating FE models - otherwise standard errors will be (heavily) biased. In turn, this would mean we make wrong inferences.\n\nLuckily, we can easily adjust standard errors, either manually using special packages such as the sandwich package or automatically as part of the regression estimation. To do the latter, we can use the lm_robust() command from the estimatr package. It’s syntax is similar to the general lm() command, but it allows you to simply specify fixed effects by adding a fixed_effects= argument, followed by the variable indicating FE. Robust standard errors are automatically calculated when you use that function.\n\nExercise 5: Estimate the same time-FE models as above with robust standard errors using the lm_robust() function.\n\n\n\n\nReveal Answer\n\n\nrequire(estimatr)\n\n# Bivariate time-FE with  roust SEs\nFE_robust_bivariate &lt;- lm_robust(GOPVoteShare ~ Rain, fixed_effects= factor(Year), data=weather)\nsummary(FE_robust_bivariate)\n\n# Multivariate time-FE with  roust SEs\nFE_robust_covariates &lt;- lm_robust(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, fixed_effects= factor(Year), data=weather)\nsummary(FE_robust_covariates)\n\n\n\nThis is better - now we have unbiased SEs. Note that calculating robust standard errors does not affect the estimates - but it does change SEs and all statistics that are dependent on them. Since we calculate models in order to make causal statements about effects, it is absolutely crucial to make sure these statistics are unbiased.\n\n\n\nExercise 6: Display the regression output of all six models in a single table. Interpret the models.\n\n\n\n\nReveal Answer\n\n\nrequire(texreg)\n# This is one possible option, there are many others. \n\nscreenreg(l=list(ols_bivariate, ols_covariates, FE_bivariate, FE_covariates, FE_robust_bivariate, FE_robust_covariates),\n          custom.header = list(\"Pooled OLS\" = 1:2, \"Time FE\" = 3:4, \"Robust Time FE\" = 5:6),\n          omit.coef = \"(Intercept)\")\n\n\n\nNote that the default display of the lm_robust() differs from the standard OLS output we encountered so far. Instead of standard errors, the table indicates the 95% confidence intervals of the estimates - this is indeed an increasingly popular way to present results as confidence intervals give a clearer indication of the effect’s uncertainty. Also, all significant results are marked by only one asterisk, which indicates that the value of the null hypothesis (i.e. 0) falls outside the confidence interval.\n\nPay attention to the significant differences between the OLS models and the FE models. Based on the pooled OLS model, we would have thought that rain is associated with an increase in GOP vote share once we control for covariates. However, doing so does not account for temporal changes and is based on comparisons across different elections. In other words, a pooled OLS compares different observations from different points in time of different units to each other.\n\nAccounting for time by calculating time-FE models changes the picture. While we find a bivariate relationship, there is no significant association between Rain (or AdjIncome) and GOPVoteShare once we include a list of (time-variant) covariates into our model. Note that in the FE models we are estimating effects of Rain within each individual election year and do not simply compare observations from different election years to each other.\n\n\n\n\n\n\n8.2.1 PLM Function\n\nRather than using lm() or lm_robust(). We can use the plm() function from the plm package that allows conducting unit and time fixed effect. The syntax is quite simple: plm(formula, effect = \"\", model = \"\", index = c(\"unit\", \"year\"), data = data). The effect argument sets what type of model you want to run, some of the options are “individual”, which is unit-fixed effect, “time” for time-fixed effects, and “twoways” for two-way fixed effects. The model argument determines the estimation method. If you want to calculate the coefficients by demeaning both dependent and independent variables, and then calculate estimates using deviations from the mean, you need to set model argument equal to \"within\". What essentially happens with the “within” estimation is that subtracts group-specific means values from the dependent variable and explanatory variables, which removes unit-specific and time-specific effects. If you want to estimate the coefficients using the first difference estimator, you can set the model argument equal to \"fd\". The index argument tells the function what is the structure of the data i.e. what variable contains the time periods and what variable contains the units, so we need to tell the function what variable is the entity identifiers and the time identifier.\nNow, let’s estimate this using a fixed-effects model, using the plm() function. Let’s start conducting time FE model for the Turnout variable for both “treatment” variables Rain_Dev and Snow_Dev. Let’s conduct the simplest version without covariates.\nExercise 7: Run time FE model for both weather variables (Rain_Dev and Snow_Dev in separate models), set effects equal to “time”. Set the model argument equal to \"within\" set the index argument equal to c(\"FIPS_County\", \"Year\"). Store the output of your time FE model into an object and then use the summary() to check for your results. Interpret the results of both estimations.\n\n\n\n\nReveal Answer\n\n\nturnout_time_plm_rain &lt;- plm(Turnout ~ Rain_Dev, effect = \"time\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_time_plm_rain)\n\nturnout_time_plm_snow &lt;- plm(Turnout ~ Snow_Dev, effect = \"time\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_time_plm_snow)\n\nWe find that a unit increase on deviation of rainfall from a “normal” election day weather is associated with a 2.16 increase in turnout, once we include time-fixed effects. The effect of snowfalls seems to be smaller at 1.0105. Note that the results of these bivariate models are in the opposite direction of what the authors found in their study. Let’s do the same but now let’s look at vote share for Republicans.\n\n\n\nExercise 8: Run a time FE model using the plm() function, but look at the effect of rainfall and snowfall on the vote share for Republicans (GOPVoteShare). Remember to set the effect argument equal to \"time\" and time model argument equal to within. Store the output from the plm() function into an object. Use the summary() command check the results. Interpret the coefficient from the Rain_Dev and Snow_Dev “treatment” variables. Doabove average rainfalls and snowfalls increase or decrease GOP vote share? Are the results statistically significant?\n\n\n\n\nReveal Answer\n\n\ngop_time_plm_rain &lt;- plm(GOPVoteShare ~ Rain_Dev, effect = \"time\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\n\nsummary(gop_time_plm_rain)\n\ngop_time_plm_snow &lt;- plm(GOPVoteShare ~ Snow_Dev, effect = \"time\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(gop_time_plm_snow)\n\nWe find that a unit increase in the deviation from the average rainfalls on election day is associated with an increase in vote share for the Republicans by 1.93 percent, once we control for time-FE. Meanwhile, a unit increase in the deviation from the average snowfall is associated with a 0.01 increase in vote share for Republicans. However, this last result is not statistically significant. These results are more in line with what the authors found in their study.\n\n\n\n\n\n\n8.2.2 Unit-FE\nWith the plm() function we can also conduct unit-FE. We only need to do is to set the effect argument equal to \"individual\". Let’s do that:\nExercise 9: Conduct a unit FE looking at the effect of weather on voter turnout (Turnout) and vote share for Republicans (GOPVoteShare). Use both measures of weather Rain_Dev and Snow_Dev. Store the output of your panel data estimations into an object and use the summary() function to check the results.\n\n\n\n\nReveal Answer\n\n\n# Turnout - rain\nturnout_unit_plm_rain &lt;- plm(Turnout ~ Rain_Dev, effect = \"individual\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_unit_plm_rain)\n\n\n# Turnout - snow\nturnout_unit_plm_snow &lt;- plm(Turnout ~ Snow_Dev, effect = \"individual\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_unit_plm_snow)\n\n\n# GOP vote share - rain \ngop_unit_plm_rain &lt;- plm(GOPVoteShare ~ Rain_Dev, effect = \"individual\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(gop_unit_plm_rain)\n\n\n# GOP vote share - snow  \ngop_unit_plm_snow &lt;- plm(GOPVoteShare ~ Snow_Dev, effect = \"individual\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(gop_unit_plm_snow)\n\nUsing County-level FE, we find results that are in line with the findings reported by Gomez et al. We find that a unit increase in the deviation from the average weather on election day is associated with a decrease in voter turnout by 2.68 per cent. We find similar results for snow (-0.87). We also find having relatively bad weather favours Republicans by a sizeable 4.96 percentage point increase in their vote share.\n\n\n\n\n8.2.3 Two-Way Fixed Effects\n\n\nNow that we have conducted time and unit FE separately, but we can conduct a two-way FE model that controls for unit and time FE at the same time. In a two-ways FE model, we are controlling for all unit-specific time-invariant confounders and year-specific confounders that affect all units at the same time. To do this using the plm() is very simple, we only need to change the effect argument and set it equal to twoways.\nExercise 10: Conduct a two-way FE model looking at the effect of weather on voter turnout and GOP vote share. Use both measures of weather Rain_Dev and Snow_Dev. Set the effect argument equal to \"twoways\". Store the outputs into objects and then use the summary() to check the results. \n\n\n\n\nReveal Answer\n\n\n# two-way Turnout - rain\nturnout_twoway_plm_rain &lt;- plm(Turnout ~ Rain_Dev, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_twoway_plm_rain)\n\n\n# two-way Turnout - snow\nturnout_twoway_plm_snow &lt;- plm(Turnout ~ Snow_Dev, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(turnout_twoway_plm_snow)\n\n\n# two-way GOP vote share - rain\ngop_twoway_plm_rain &lt;- plm(GOPVoteShare ~ Rain_Dev, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(gop_twoway_plm_rain)\n\n\n# two-way GOP vote share - rain\ngop_twoway_plm_snow &lt;- plm(GOPVoteShare ~ Snow_Dev, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\nsummary(gop_twoway_plm_snow)\n\nWe again find evidence that weather is associated with a decrease in turnout and an increase in the GOP vote share once we include time and unit fixed effects. With time and unit fixed effects we are controlling for all time-invariant confounders. We are also controlling for time-specific confounders, but a particular type of time-specific confounders, only those that affect all units in the same period of analysis. Thus, we need to further control for time-varying unit-specific confounders such as the average income or unemployment rates at the county levels, which are different from one County to another and also vary from one presidential cycle to another.\n\n\n\nExercise 11: Run a two-way FE model looking at the effect of weather only on Turnout. The syntax is the same as lm(), you can include additional variables into your model by adding the variable name followed by a + sign between variables. Include the following covariates: PcntBlack, ZPcntHSGrad, FarmsPerCap, AdjIncome and Literacy. Store the output from your plm() estimations into separate objects and then use the summary() function to check the results. Again, conduct separate estimations one only using the Rain_Dev and another model using the Snow_Dev variable.\n\n\n\n\nReveal Answer\n\n\n# turnout - rain + covariates \nturnout_twoway_plm_rain_cov &lt;- plm(Turnout ~ Rain_Dev + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\n\nsummary(turnout_twoway_plm_rain_cov)\n\n\n# turnout - snow + covariates \nturnout_twoway_plm_snow_cov &lt;- plm(Turnout ~ Snow_Dev + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\n\nsummary(turnout_twoway_plm_snow_cov)\n\n\n\n\nNow let’s report the FE models only for the turnout outcome variable into a single regression table. Let’s also only report for the rainfall variable (otherwise we will have 14 models). Let’s use screenreg() and let’s compare the results. You can store all of the outputs into one single list() and give them names, as you can see below.\n\nplm_models &lt;- list(\"Time fixed effect\" = turnout_time_plm_rain,\n                   \"Unit fixed effect\" =  turnout_unit_plm_rain,\n                   \"Twoway\" = turnout_twoway_plm_rain,\n                   \"Twoway + cov\" = turnout_twoway_plm_rain_cov)\n\nExercise 12: Report in a regression table the following modes: time FE, unit FE, Two ways and two ways FE models for the rainfall variable (Rain_Dev) and only the voter turnout (Turnout) outcome. Use the screenreg() function.\n\n\n\n\nReveal Answer\n\n\nscreenreg(plm_models,\n          stars = 0.05,\n          digits = 2,\n          custom.coef.names = c(\"Rain\", \"% Black \", \"Graduates\", \"Rural\", \"Income\", \"Literacy\"),\n          reorder.coef = c(1, 3, 4, 5, 2 ,6),\n          caption = \"Fixed effects models - Voter turnout\",\n          caption.above = TRUE)\n\n\n\n\nOne limitation with the plm() function is that it calculates clustered standard errors without correcting for small samples and not reducing downward bias. The data that we are using is far from small, but when you are dealing with small panels, you may want to try to conduct this small sample correction. For that, we can use the coeftest() and vcovHC() function to obtain cluster-robust standard errors and draw inference based on robust standard errors.\nThe syntax’s a little bit tricky, but let’s look at the code below from the inside out. The first function is vcovHC() function. vcovHC() calculates heteroscedasticity-consistent standard errors or aka “HC1”. Then, once we calculate our standard errors, we can perform a z/quasi-t Wald test using the coeftest() using the standards errors that we estimated from the vcovHC(). The first argument inside of the vcovHC() function is the object where you stored the output from your fixed-effect model from plm(). The second argument is type. This is the type of small sample adjustment that we want to make. “HC1” is the commonly used approach to correct for small samples, so we can set type equal to \"HC1\". Finally, the third argument is cluster is an argument that we can use to determine whether we want to obtain the variance-covariance matrix clustered by units (\"groups\") either or by time (\"time\"). You can set both by setting the argument cluster = c(\"group\", \"time\"). Then, the second function is coeftest(). The first argument of this function is the object where you stored your output from plm(). The second argument vcov is the variance-covariance matrix of the estimated coefficients. In this case, rather than using the non-robust variance-covariance matrix from the plm() function, we will use the White’s heteroscedasticity-consistent covariance matrix that we obtained from the vcovHC() function. Using this matrix also allow us to overcome with serial correlation, cross-sectional dependent and heteroscedasticity across groups and time. You see the syntax below for the two-way fixed effect model where we looked at the effect of rainfalls on voter turnout, including covariates.\n\nout_rain_cov &lt;- coeftest(turnout_twoway_plm_rain_cov, vcov = vcovHC(turnout_twoway_plm_rain_cov, type = \"HC1\", cluster = c(\"group\", \"time\")))\nout_rain_cov\n\nIt’s important to separate estimation from inference, so first we got our point estimate, which is equal -1.23, but when we want to make inference from this point estimate, we need to make the appropriate adjustments to yield accurate standard errors. If you see the “unadjusted” standard errors from plm() of the Rainfall variable were 0.159, but once we did this small sample bias correct, they changed to 0.170, which is not that much, a roughly 6% increase. Presumably the standard errors did not change substantially given that we are working with a relatively large sample. In other cases, the change might well have a meaningful impact on the interpretation of your findings.\nNow obtain the estimates of the two-way fixed effect model of snowfalls on voter turnout and adjust your standard errors.\nExercise 13: Perform the small sample correction to the standard errors obtained from the two-way FE with covariates, do this for the model that looks at the effect of snowfall (Snow_Dev) on voter turnout (Turnout). Compare the standard errors to the “unadjusted” standard errors obtained from plm().\n\n\n\n\nReveal Answer\n\n\nsummary(turnout_twoway_plm_snow_cov)\n\nout_snow_cov &lt;- coeftest(turnout_twoway_plm_snow_cov, vcov = vcovHC(turnout_twoway_plm_snow_cov, type = \"HC1\", cluster = c(\"group\", \"time\")))\nout_snow_cov\n\nWe can see that without the small sample adjustment, the standard errors were 0.136, but once we did this small sample adjustment we got slightly larger standard errors (0.167). They don’t seemed to change that much, but actually this is a 22% increase, which may radically change whether your results are statistically significant or not!\n\n\n\n\n\n\n8.2.4 Fixed Effects or Random Effects?\nSo far we have looked at fixed effects using the lm() and plm() functions, assuming that we should run a fixed effect model. But which model should we go with? Let’s now test whether time-FE or random effects is the more appropriate model.Recall the differences between the two estimators.\n\nThe fixed effects estimator - or within estimator accounts for unobserved heterogeneity that is correlated with the independent covariates. In this model, the group means are fixed, with each one having a separate intercept - recall the year-dummies in the models above. For the time-fixed effects in our example this means we are essentially looking at the unit-demeaned data for each time period. In other words, we are looking at the variation across units within a certain year - i.e. for each presidential election In other words, we are estimating our coefficient while time is fixed.\n\n\\[FE:  Y_{i,t} = \\alpha_i + \\beta_1 X_{i,t} + u_{i,t}\\]\nIn the case of a random effects model, we assume that \\(\\alpha_i\\) has zero mean. In other words, instead of using fixed means per group, the group means are a random sample of a population. While the fixed effects model ‘gets rid’ of unobserved time-invariant heterogeneity (\\(\\alpha_i\\)), the random effects model makes the assumption that the unobserved effect is uncorrelated with the explanatory variables. If the assumption holds, the random effects estimator is more efficient as it accounts for \\(\\alpha_i\\) or time-invariant covariates. In essence, the random effects model is a weighted average of the within estimator and a between estimator that looks at effects between units.\n\n\\[RE: Y_{i,t} = \\alpha_i + \\beta_0 + \\beta_1 X_{i,t,1} + \\beta_2 X_{i,t,2} + u_{i,t}\\]\nWe can use two different tests to check our data in order to find out which model is more appropriate. First, the Hausman-Test (sometimes also called Durbin–Wu–Hausman test) analyses the difference of the vectors of the coefficients of the two models under the null hypothesis that the models’ estimates are consistent. If the test finds that the models are consistent, we choose the random effects model as it is more efficient than the fixed effects model. However, the random effects model is inconsistent if there indeed is unobserved heterogeneity correlated with the covariates due to omitted variables, in which case the FE estimator should always be preferred.\n\nAn F-Test looks at whether any additional effects - such as one-way or two-way effects - are predictive of the outcome. That is, it tests the effects introduced in a within-model as compared to another model such as a random effects model. If the effects of the within-estimator are not significant, we can, again, use the random effects model to increase the efficiency of our estimators. If this is not the case, we go with FE, which is always unbiased with regard to unobserved heterogeneity that is constant over time.\n\nLet’s now conduct the tests. To do so, we first specify fixed-effects and random effects models using the plm package. The syntax is essentially the same as for lm(). However, you need to add the model argument as well as the index argument to specify the time variable (for fixed effects) and the unit and time variables (for random effects).\n\nExercise 14: Specify a time-fixed effects and random effects model using plm().Run this panel data estimation for the GOP vote share outcome. Include the same covariates that we included before. Store the outputs from these models into different objects.\n\n\n\n\nReveal Answer\n\n\nrequire(plm)\n\n# Time-FE. \nFE_time &lt;- plm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, model= \"within\", index=\"Year\", data=weather)\n\n#Note: The identical model can be specified by explicitly indicating time-FE:\n#FE_time2 &lt;- plm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, model= \"within\", effect=\"time\", index=c(\"FIPS_County\",\"Year\"), data=weather)\n\n\n# Random Effects\nrandom &lt;- plm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, model= \"random\", index=c(\"FIPS_County\", \"Year\"), data=weather)\n\n\n\n\nExercise 15: Conduct both a Hausman test and F-test using the models you just specified. Interpret your results.\n\nYou can use the phtest and pFtestcommands for Hausman and F-Tests, respectively. The syntax is simple and similar for both: command(Model1, Model2).\n\n\n\n\nReveal Answer\n\n\nphtest(FE_time, random)\n\npFtest(FE_time, random)\n\nWhat do the tests tell us? In the case of the Hausman test, we can reject the null hypothesis of consistency and find that the models are inconsistent. The F-test shows us that we can reject the null hypothesis of no significant effects in the fixed-effects model: The FE model indeed adds significant effects. So, both models would caution us against using the random effects model and make us choose the FE estimator. Both tests giving us the same indication is the best outcome as we can be pretty sure about the need to choose the FE model. However, in some cases you might get two different indications - in which case it usually is a good choice to choose the conservative option: the unbiased but (possibly) inefficient FE estimator.\n\n\n\n\n\n\n8.2.5 First Differences\nLet’s now finally look at another estimator that can be of use when working with panel data. The First Difference Estimator looks at the differences between time units. Similar to the FE estimator, time invariant unit-specific covariates are dropped from the model as it only considers changes from one time period to the other:\n\\[FD: \\Delta Y_i = \\delta +  \\beta_1 \\Delta X_{i1} + \\beta_2 \\Delta X_{i2} + \\Delta v_i\\]\nYou might have noticed a similarity to the FE estimator. While the FE estimator demeans over units or times, the FD estimator considers differences over time periods. Accordingly, the first difference estimator is identical to the FE estimator if the number of time periods is two: Whenever this is the case, the differencing of the FD estimator and the demeaning of the FE estimator are equivalent.\nExercise 16: Estimate the first difference estimator using the same model specification as in Exercise 6.\n\nMake sure to use the plm package and the specify the FD estimator with the argument model=\"fd\" and to specify the unit and time variable using the index argument.\n\n\n\n\nReveal Answer\n\n\nfd_model  &lt;- plm(GOPVoteShare ~ Rain + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy,data=weather,effect=\"individual\",model=\"fd\",index=c(\"FIPS_County\",\"Year\"))\nsummary(fd_model)\n\n\n\nThe output of the FD estimator indicates the association of the effect of a change in an independent variable from one time period to the the following period for the same unit. That is, if Rain in a county increase by one inch, Republican vote share is estimated be 8%-points higher on average in the next period. Note that the FD estimator does not consider longer time effects, i.e. those that have an effect beyond the following time period. This might be useful if we have strong reasons to believe this is what happens, otherwise the FE estimator is generally the more powerful estimator.\n\n\n\n\n\n\n8.2.6 HOMEWORK:\nNow let’s go back estimating the effect of weather on voter turnout. We also can look at the extensive and intensive margin of weather on vote turnout. Extensive margin refers to a discrete change in the level to which an activity or intervention takes place. For example, going from normal daily election weather to a day that is above this average. Let’s dichotomise the Rain_Dev and Snow_Dev variable. Let’s set 1 for any day that the weather is worse than normal (Rain_Dev &gt; 0 or Snow_Dev &gt; 0) .Meanwhile, intensive margin refers to the degree (intensity) to which a resource is utilised, but more broadly on how strong the intervention is, which is basically what we did before.\nHomework 1: Create a dummy variable \n\n\n\n\nHomework 1\n\n\nweather &lt;- weather %&gt;% \n  mutate(dummy_rainfall = ifelse(Rain_Dev &gt; 0, 1, 0),\n         dummy_snowfall = ifelse(Snow_Dev &gt; 0, 1, 0))\n\n# 9616 County - election day with rainfall above election day county average\ntable(weather$dummy_rainfall)\n # 2439 County - election day with snowfalls above election day county average\ntable(weather$dummy_snowfall)\n\n\n\n\nHomework 2: Run plm() looking at the extensive margin of weather on voter turnout. Use the plm() function. Include the same covariates that we used in the previous exercise. Set the model argument equal to \"within\" and set the effect argument equal to \"twoways\" so we can get the estimates of a two-way FE.\n\n\n\n\nHomework 2\n\n\nextensive_rain_cov &lt;- plm(Turnout ~ dummy_rainfall + PcntBlack + ZPcntHSGrad + FarmsPerCap + AdjIncome + Literacy, effect = \"twoways\", model = \"within\", index = c(\"FIPS_County\", \"Year\"), data = weather)\n\nsummary(extensive_rain_cov)"
  },
  {
    "objectID": "rdd.html",
    "href": "rdd.html",
    "title": "7  Regression Discontinuity Design",
    "section": "",
    "text": "8 Recap\nIn this seminar, we will cover the following topics:\n1. Conduct regression discontinuity using global polynomial estimation using lm() function\n2. Calculate LATE using non-parametric estimations using rdrobust().\n3. Conduct various robustness/falsification tests such as balance test, placebo outcome, density test, and falsification test."
  },
  {
    "objectID": "rdd.html#islamic-rule-and-the-empowerment-of-the-poor-and-pious---meyerson-2014",
    "href": "rdd.html#islamic-rule-and-the-empowerment-of-the-poor-and-pious---meyerson-2014",
    "title": "7  Regression Discontinuity Design",
    "section": "9.1 Islamic Rule and the Empowerment of the Poor and Pious - Meyerson (2014)",
    "text": "9.1 Islamic Rule and the Empowerment of the Poor and Pious - Meyerson (2014)\nIn this paper, Meyerson is interested in the effects of Islamic parties’ control of local governments on women’s rights. He focuses on the educational attainment of young women. Meyerson conducts a Sharp RD design, based on close elections in Turkey. The challenge here is to compare municipalities where the support for Islamic parties is high and win the election, versus those that elected a secular mayor.\nYou would expect that municipalities controlled by Islamic parties would systematically differ from those that are controlled by a secular mayor. Particularly, if religious conservatism affects the educational outcomes of women. However, we can use RDD to isolate the treatment effect of interest from all systematic differences between treated and untreated units.\nWe can compare municipalities the Islamic party barely won the election versus municipalities where the Islamic party barely lost. This reveals the causal (local) effect of Islamic party control on women’s educational attainment a few years later. One crucial condition to meet in this setup is that parties cannot systematically manipulate the vote share they obtain.\nThe data used in this study is from the 2014 mayoral election in Turkey. The unit of analysis is the municipality, and the running variable is the margin of victory. The outcome of interest is the educational attainment of women who attended high school during 1994-2000, calculated as a percentage of the cohort of women aged 15 to 20 in 2000 who had completed high school by 2000.\nWe will be using the following variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nmargin\nThis variable represents the margin of victory of Islamic parties in the 1994 election. A positive margin means that an Islamic party won.\n\n\nschool_men\nsecondary school completion rate for men aged between 15 and 20\n\n\nschool_women\nthe secondary school completion rate for women aged 15-20\n\n\nlog_pop\nlog of the municipality population in 1994\n\n\nsex_ratio\ngender ratio of the municipality in 1994\n\n\nlog_area\nlog of the municipality area in 1994\n\n\n\n\n\nNow let’s load the data. There are two ways to do this:\nYou can load the dataset from your laptop using the read.csv() function. Here the dataset is called educ - but feel free to give it a different name if you prefer.\n\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab8\")\n# \nlibrary(readr)\n#educ &lt;- read.csv(\"~/islamic_women.csv\")\n\nhead(educ)\n\nLet’s start by visualising the data. We will use plot() function to do this. This is the simplest scatter plot that you can come up with.\nExercise 1: Generate a plot using the plot(X,Y) function. Replace X with educ$margin and Y with educ$school_women.\n\n\n\n\nReveal Answer\n\n\nplot(educ$margin, educ$school_women)\n\nThis is a very simple plot that shows the raw relationship between the margin of victory and the outcome variable. However, it conveys some important information. For example, the margin of victory is clustered around -0.5 and roughly 0.3. Also, the outcome variable, school attainment, usually goes from 0 to 40%. Now let’s generate a slightly fancier plot.\n\n\n\nExercise 2: Generate a scatter plot using ggplot() function. Use the functions below to add some additional features into this plot.\n\nggplot(aes(x = running variable, y = outcome, colour =outcome), data = data) +\n  # Make points small and semi-transparent since there are lots of them\n  geom_point(size = 0.5, alpha = 0.5, position = position_jitter(width = 0, height = 0.25, seed = 1234)) + \n  # Add vertical line\n  geom_vline(xintercept = 0) + \n  # Add labels\n  labs(x = \"Label X\", y = \"Label Y\") + \n  # Turn off the color legend, since it's redundant\n  guides(color = FALSE)\n\n\n\n\n\nReveal Answer\n\n\n# Let's check if this is a sharp RD. \nggplot(educ, aes(x = margin, y = school_women, colour =school_women)) +\n  # Make points small and semi-transparent since there are lots of them\n  geom_point(size = 0.5, alpha = 0.5, \n             position = position_jitter(width = 0, height = 0.25, seed = 1234)) + \n  # Add vertical line\n  geom_vline(xintercept = 0) + \n  # Add labels\n  labs(x = \"Vote share\", y = \"Womens' Educational Attainment (Proportion)\") + \n  # Turn off the color legend, since it's redundant\n  guides(color = FALSE)\n\nWe can now see more clearly that most municipalities elected a secular mayor. Recall that we are using the margin of victory for the Islamic parties - a positive margin of victory is positive means that the Islamic parties won the election in that municipality.\n\n\n\nExercise 3: Let’s generate a dummy variable that is equal to “Treated” if the margin of victory is equal or greater than zero and “Untreated” otherwise. You can use the mutate() function and the ifelse() functions to do this. Remember to use the pipeline operator to store the variable into your existing data frame. See below the syntax for more information.\n\n\n\n\n\n\n\nFunction/argument\nDescription\n\n\n\n\ndata &lt;- data %&gt;%\nPipeline operator to assign the new operation into a new data or existing data frame\n\n\nmutate(new variable = ifelse(variable &gt;= \"condition\", \"Treated\", \"Untreated\")\nIf the condition is met, the new variable takes value equal to “Treated” and “Untreated” otherwise\n\n\n\n\ndata &lt;- data %&gt;% \n  mutate(newvariable = ifelse(variable &gt;= 0 , \"Treated\", \"Untreated\"))\n\n\n\n\n\nReveal Answer\n\n\neduc &lt;- educ %&gt;% \n  mutate(treat = ifelse(margin &gt;= 0 , \"Treated\", \"Untreated\"))\n\nNow that we have created our treatment condition variable, let’s generate an additional plot that conveys information on the distribution of the running variable for both treated and untreated municipalities.\n\n\n\nExercise 4: Generate a plot looking at the distribution margin of victory variable. Using the ggplot() function, set the x argument equal to the margin of victory variable. Set fill equal the new treatment variable treat. There is no need to add the y argument given that we expect to generate a histogram that will give us the number of observations for each value in the margin of victory variable. Add the geom_histogram() function. Set the argument binwidth in this function equal to 0.01 and set colour argument equal to “dark”. Let’s add the geom_vline() function and add a vertical line by setting the xintercept argument equal to zero. Add the labs() function and set x label equal to “Margin of victory” and the y label equal to “count”, and the fill argument equal to “Treatment Status”.\n\n\n\n\nReveal Answer\n\n\nggplot(educ, aes(x = margin, fill = treat)) +\n  geom_histogram(binwidth = 0.01, color = \"white\") + \n  geom_vline(xintercept = 0) + \n  labs(x = \"Margin of Victory\", y = \"Count\", fill = \"Treatment Status\")\n\n\n\nIn this plot, we can see the distribution of the running variable (margin of victory). Again, we can see that in the majority of the municipalities a secular mayor was elected."
  },
  {
    "objectID": "rdd.html#global-parametric-estimation",
    "href": "rdd.html#global-parametric-estimation",
    "title": "7  Regression Discontinuity Design",
    "section": "9.2 Global Parametric Estimation",
    "text": "9.2 Global Parametric Estimation\nAs we discussed in the lecture, there are different ways to estimate the causal effect using RD. These different approaches differ in the range of observations they include as well as how they estimate the average outcome for those units just above the cut-off and below the cut-off. One way to estimate the effect of the intervention in RDD is using OLS, but only using the running variable as the main predictor. In this case, we use the running variable measured as the distance from the cut-off. Stated formally: \\(\\tilde{X_l} = X - c\\).\nIn this case, the regression in the left-hand side would be equal to:\n\\[Y= \\alpha_l + \\tilde{X} + \\epsilon\\] Whereas the regression above the cut-off is equal to:\n\\[Y= \\alpha_r + \\tilde{X} + \\epsilon\\] It’s important to point out that for all estimations the treatment effect is equal to the differences of the intercepts of the regressions above and below the cut-off.\n\\[\\tau = \\alpha_r - \\alpha_l\\]\nLet’s do this manually. To do so, subset the data for those observations above and below the threshold. Then, regress the outcome on the running variable. Finally, subtract the intercepts from each regression. Let’s do that in the following exercise.\nExercise 5: Run a regression using only margin variable as the predictor. Set your outcome variable equal to school_women variable. Set the data argument equal to educ (unless you called your data frame differently). Add the subset function inside of the lm() function and set it equal to margin &gt;= 0 for the regression above the cut-off and margin &lt; 0 for the regression below the cut-off. Use the summary() function to report your results, but also store the output of this function into an object. You can retrieve the intercept from the object that you stored the output from the summary() function this way:\n\nobject &lt;- summary(lm(outcome ~ variable, data = data, subset = variable &gt;= condition))\n\n# intercept\nobject$coefficient[1]\n\n\n\n\n\nReveal Answer\n\n\n# above\nabove &lt;- summary(lm(school_women~margin, data = educ, subset = margin &gt;= 0))\n\nabove$coefficients[1]\n\n# above\nbelow &lt;- summary(lm(school_women~margin, data = educ, subset = margin &lt; 0))\n\nbelow$coefficients[1]\n\n# 0.156453 - 0.162002\ntau_rd = above$coefficients[1] - below$coefficients[1]\ntau_rd\n\nBased on this approach the intercepts of the two regressions yield the estimated value of the average outcome at the cut-off point for the treated and untreated units. This difference of intercepts is the estimated effect of an Islamic party being in power (at the municipal level) - it suggests there is a 0.5 percentage decrease in women’s educational attainment.\n\n\n\nA more direct way of estimating the treatment effect is to run a pooled regression on both sides of the cut-off point, using the following specification: \\(Y = \\alpha + \\tau D + \\beta \\tilde{X} + \\epsilon\\)\nWhere \\(\\tau\\) is the coefficient of interest. Here again LATE is the difference between the two intercepts: \\(\\tau = \\alpha_r -\\alpha_l\\). When \\(D\\) switches off and we are also controlling the different values of the forcing variable, \\(\\tilde{X}\\), we get the slope of the regression below the threshold. Conversely, for units above the cut-off, \\(D\\) switches on, and we control for different values of the forcing variable, we get the slope of the regression above the cut-off. The estimated effect of the treatment at \\(\\tilde{X}\\) then provides the treatment effect (\\(\\tau\\)).\nNote that you are constraining the slope of the regression lines to the same on both sides of the cut-off. (\\(\\beta_l = \\beta_r\\)) This might not be consistent if the data structure varies and the single slope fails to appropriately approximating each side to the cutoff.\nExercise 6: Calculate the effect of the intervention using a regression model including the margin and treat variables. Use thelm() function to conduct this analysis. Store this regression into an object and call it global2. Use the summary() to inspect your results. Interpret the coefficient of interest.\n\n\n\n\nReveal Answer\n\n\n# a more direct way is to run a pooled regression on both sides of the cut-off (constraining the slopes)\n\nglobal2 &lt;- lm(school_women~treat+margin, data = educ)\nsummary(global2)\n\nUsing this estimation approach, we obtain that, on average, the effect of a municipality in control of an Islamic party leads to a 1.7%-point increase in women’s educational attainment.\n\n\n\nWe can also allow the regression function to differ on both sides of the cut-off by including interaction terms between \\(D\\) and \\(\\tilde{X}\\). This would be as follows:\n\\[Y = \\alpha_l + \\tau D + \\beta_0\\tilde{X} + \\beta_1 (D \\times\\tilde{X})+ \\epsilon\\]\nLet’s do that in the following exercise.\nExercise 7: Calculate the effect of the intervention using a regression model including the margin and treat variables and the interaction between these two variables. Use thelm() function to conduct this analysis. Store this regression into an object and call it global3. Use the summary() to inspect your results. Interpret the coefficient of interest.\n\n\n\n\nReveal Answer\n\n\n# a more direct way is to run a pooled regression on both sides of the cut-off (constraining the slopes)\n\nglobal3 &lt;- lm(school_women~treat+margin + treat*margin, data = educ)\nsummary(global3)\n\nHere, we find that the coefficient of interest is positive (0.005) yet insiginifcant. This means that, based on this model, the effect of an Islamic party in control of the municipal government does not lead to a change in women’s educational attainment.\n\n\n\nGlobal parametric models have a severe shortcoming - they rely upon observations that are far away from the cut-off. Indeed, the evidence against using a global polynomial approach is quite substantial. According to Cattaneo et al (2019), this estimation technique does not provide accurate point estimators and inference procedures with good statistical properties.\nExercise 7 (no coding required): Think about how the global polynomial approach weights each observation when it calculates the coefficient of interest?\n\n\n\n\nReveal Answer\n\n\n\nOLS will estimate \\(\\tau\\) based on all observations across the score. This means that the observations’ very far from the cut-off weight is equal to that of very close ones’. In the worst-case scenario, if the observations are clustered far from the cut-off, the estimation of \\(\\tau\\) would be heavily influenced by those values rather than those close.\n\n\n\nWe can also use high-order polynomial to retrieve LATE. However, the evidence against using high-order polynomial seems to be quite robust see here for a discussion on high-order polynomials). In short, the issues with using high-order polynomials is that they leads to noisy estimates, they are sensitivity to the degree of the polynomial, and they have poor coverage of confidence intervals.\nExercise 8: Well - let’s give it a try nonetheless. Conduct a third-order polynomial regression function. Include in this model the treat and the margin variables. Also, add the margin variable raised at the power of 2 and then at the power of 3. We also need to include the I() function or insulate function for the margin variable that is raised at the power of 2 and 3. The I() function insulates whatever is inside this function. It creates a new predictor that is the product of the margin variable by itself two and three times. Store this output in an object and call that object global4. Then, use the summary() to check the results of this specification. Interpret the results. \n\n\n\n\nReveal Answer\n\n\nglobal4 &lt;- lm(school_women~ treat + margin+I(margin^2)+ I(margin^3) , data = educ)\n\nsummary(global4)\n\nUsing a high-order polynomial function, we find that womens’ educational attainment decreases, on average, by roughly 2.1 per cent when an Islamic party is in power - yet the result is insignificant.\n\n\n\nYou might have reason to prefer parametric approaches - or deem them more appropriate in some cases. Then, you should not resort to a global model. It’s more appropriate to only use observations that are close to the cut-off (above and below). Let’s run the unconstrained model from Exercise 7, but this time we only use observations that are within 0.5 percentage points above and below the threshold.\nExercise 9: Run the same model used in Exercise 7, but subset your data taking only observations that are above and below 0.5 points from the threshold. Store the results from this regression into an object and call it local. Use the summary() to check your results. If you don’t remember how to subset data in the lm() function. See the syntax below\n\nlm(outcome ~ variable1 + variable2 + variable1 * variable2, data=data,\n           subset=(running_variable&gt;=-0.5 & running_variable&lt;=0.5))\n\n\n\n\n\nReveal Answer\n\n\nlocal &lt;- lm(school_women ~ margin + treat + treat * margin, data=educ,\n           subset=(margin&gt;=-0.5 & margin&lt;=0.5))\nsummary(local)\n\nWe can see that using a local polynomial function, we find that the effect of Islamic rule is inconclusive in this case.\n\n\n\nIt is important to stress that modern empirical work using RDDs empirical work employs local polynomial methods. In this case, we are estimating the average outcomes for treated and untreated units using observations that are near the cut-off. This approach tends to be more robust and less sensitive to boundary and overfitting problems. In local polynomial point estimation, we are still using linear regression, but within a specific bandwidth near the threshold. In the following section, we will look at how to use the approach using a non-parametric estimation strategy."
  },
  {
    "objectID": "rdd.html#non-parametric-estimation",
    "href": "rdd.html#non-parametric-estimation",
    "title": "7  Regression Discontinuity Design",
    "section": "9.3 Non-Parametric Estimation",
    "text": "9.3 Non-Parametric Estimation\nLet’s now estimate the LATE using a non-parametric estimator. Conveniently, we can easily do so by using the rdrobust package. As the name indicates, the package allows us to estimate robust measurements of uncertainty such as standard errors and confidence intervals. It is based on theoretical and technical work by Calonico, Cattaneo and Titiunik. rdrobust estimates robust bias-corrected confidence intervals that address the problem of undersmoothing conventional confidence intervals face in RDDs. In other words, a small bias would be required for them to be valid, which might not be the case. Moreover, they also address the poor performance of (non-robust) bias-corrected confidence intervals.\nAs suggested by the authors and somewhat counter-intuitively, we therefore use the point estimate provided by the conventional estimator, but robust standard errors, confidence intervals and p-values to make statistical inferences.\nThe rdrobust command has the following minimal syntax. You can use a uniform bandwidth or specify two different ones. We will work with further arguments later. Note that you do not need to specify if the running variable is centred on the cut-off as you can manually specify the cut-off using the c-argument.\n\nrobust_model = rdrobust(data$running_var, data$dependent_var, \n                       c=[cutoff], h=[bandwidth])\n\nExercise 10: Estimate the LATE using rdrobust with a bandwidth of 5% on either side of the cutoff. Interpret your result.\n\n\n\nReveal Answer\n\n\nrobust_5=rdrobust(educ$school_women, educ$margin, c=0, h=0.05)\nsummary(robust_5)\n\nOur point estimate is 0.023. That is, a victory of an Islamic party would be associated with an increase in the rate of women who complete school by 2.3% - however, the p-value and confidence intervals indicate that the estimate is not statistically significant. Therefore, based on this model, we would conclude that winning an election does not have a an effect on women schooling.\n\n\n\nA bandwidth of 5% seems about reasonable. But we should better check different ones, too. Let’s see what happens if we halve the bandwidth.\nExercise 11: Estimate the same model as before with a bandwidth of 2.5%. Report and interpret your results.\n\n\n\nReveal Answer\n\n\nrobust_25=rdrobust(educ$school_women, educ$margin, c=0, h=0.025)\nsummary(robust_25)\n\nWell, we can see that the number of observations used to estimate the effect has been reduced - which is reasonable and we should be aware of. The point estimate did not change much and, as before, we find that there is in fact no significant effect at the cutoff.\n\n\n\nBandwidths of 5% or 2.5% around the cutoff seem somewhat reasonable in this case - but so would several others. How can we know what bandwidth we should use to estimate our effect?\nRecall the trade-off we are facing when choosing bandwidths that was discussed in the lecture: On the one hand we know that more narrow bandwidths are associated with less biased estimates - we rely on units that are indeed comparable: their distance in the running variable being as-if random the closer we get to the cut-off. On the other hand: the wider the bandwidths, the smaller the variance. As in several cases before, the structure of our data, such as the number of observations, plays an important role. Even if small bandwidths are desirable, it can be hard [impossible] to estimate a robust and significant effect if the number of observations around the cut-off is very small - even if there is a true effect.\nLuckily, the rdrobust package provides a remedy for this. The packages allows us to specify that we want to use bandwidths that are optimal given the data input. The rdrobust command then picks the bandwidth that optimises the mean square error - in other words, MSE-optimal bandwidths. Note that this is the default bandwidth if you don’t specify any. Let’s try to find out what this would be in our case.\nExercise 12: Estimate the LATE using rdrobust and MSE-optimal bandwidths. To specify the model, replace the h argument with bwselect=\"mserd\".\n\n\n\nReveal Answer\n\n\nrobust_mserd=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"mserd\")\nsummary(robust_mserd)\n\nThis now looks very different. We estimate a LATE of about 3%, which is significant at the 90% lvel. Note that the optimal bandwidths has been estimated to be 17.2%-points on either side of the cut-off, with a separate optimal bandwidth for bias correction. Note that the MSE-optimal bandwidth is optimal in statistical terms - we should always make sure to asses the bandwidths against our theory. Here, we’d compare parties’ winning margins up to 17%-points.\n\n\n\nNote that, so far, we have used a single bandwidth for data below and above the cut-off. We can also specify different ones - both manually and in terms of optimal bandwidths. As the structure of our data might differ, different bandwidths might be optimal. We can specify two different MSE-optimal bandwidth selectors by specifying bwselect=\"msetwo\".\nExercise 13: Estimate the LATE using rdrobust and two MSE-optimal bandwidths. Interpret your results and compare it the model with a single optimal bandwidth.\n\n\nReveal Answer\n\n\nrobust_msetwo=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"msetwo\")\nsummary(robust_msetwo)\n\nThis now looks pretty similar to the single optimal bandwidth, which is a good sign. In fact, the optimal bandwidth for data points above the cut-off remains virtually unchanged. For the ones below the cut-off, the bandwidths is slightly extended. The point estimate and measures of uncertainty also remain virtually unchanged. If specifying two optimal bandwidths alters the results significantly, this is an indicator that the data should be inspected closely for the cause of the diverging bandwidths.\n\n\n\n\n\n9.3.1 Kernels\nOne of the advantages of the non-parametric RDD is that observations can be weighted based on their proximity to the cutoff. This is based on the idea that those closer to the cut-off provide better comparisons than those further away. Think of victory margins as a running variable: If we look at fractions of %-points around the cut-off, it is fair to say that victory is as-if random. The farther away we move, the less plausible this statement becomes.\nAs you learned in the lecture, there are different ways to do so. The default kernel used by the rdrobust package is a triangular kernel - which continuously assigns higher weight to observations closer to the cut-off. This is the one we have been using so far as we didn’t explicitly specify the kernel. As the figure below shows, other possible options include epachnechnikov and uniform kernels. These can be specified via the kernel argument.\n\n\n\nExercise 14: Estimate the LATE using rdrobust and a single MSE-optimal bandwidths but specifying a uniform kernel. Interpret your results.\n\n\nReveal Answer\n\n\nrobust_uniform=rdrobust(educ$school_women, educ$margin, c=0, bwselect=\"mserd\", kernel=\"uniform\")\nsummary(robust_uniform)\n\nAs compared to the triangular kernel, our point estimate is slightly larger and we find that our robust measures of uncertainty show that the effect is significant at the 95% level. Note that the optimal bandwidth is slightly narrower in the model with a uniform kernel. We have to be aware that this model is less conservative though: We assign the same weight to each observation, independent of its distance to the cutoff - which introduces growing bias over increasing bandwidths."
  },
  {
    "objectID": "rdd.html#falsification-checks",
    "href": "rdd.html#falsification-checks",
    "title": "7  Regression Discontinuity Design",
    "section": "9.4 Falsification Checks",
    "text": "9.4 Falsification Checks\n\n9.4.1 Sensitivity\nIf you apply an RDD, you should always make sure that results are robust to different specifications of the model. Importantly, this involves sensitivity checks. This means you should make sure that results are robust across model specifications and, importantly, different bandwidths.\nWhile there are packages that help produce useful plots that visualise robustness across bandwidths, they can be cumbersome to adjust and are not very flexible. Hence, one avenue to create such a plot can be to loop rdrobust models over different bandwidths. This solution involves somewhat more coding, but gives you a lot of flexibility as to what you want to display and highlight.\n\n\nExercise 15: Create a plot that shows estimates and confidence intervals for estimates of the effect across different bandwidths.\nThere are several ways to do this. Feel free to play around a bit and try to come up with your own way.\n\n\nReveal Hint\n\nHint: You could use the following approach:\n- Create a data frame with all variables you need for the plot and a and observation for each bandwidth.\n- Extract the values from the rdrobust output which you need for your plot.\n- Loop the regression and the extraction of output over the bandwidths indicated in the initial data frame.\n- Save the output in your loop to the respective row in the data frame.\n- Plot the output from the newly created data frame.\n\n\n\n\n\nReveal Answer\n\n\nbandwidth &lt;-  seq(from = 0.05, to = 1, by = 0.05)  #create a vector with values for each bandwidth you want to estimate\n\ncoefficient&lt;- NA \nse &lt;- NA\nobs &lt;- NA\nbw &lt;- NA\nci_u &lt;- NA\nci_l &lt;- NA\n\ndata_extract &lt;- data.frame(bandwidth, coefficient, se, obs, bw, ci_u, ci_l) # create a data.frame with all variables you want to incldue in your dataset\n\n# create a loop for each bandwidth that is indicated by 'i'\nfor(i in bandwidth){\n rdbw &lt;- rdrobust(educ$school_women, educ$margin, c=0, h=i) # run the model\n                                      \n# extract the model output (make sure to extract *robust* statistics)\ndata_extract$coefficient[data_extract$bandwidth==i]  &lt;- rdbw$coef[3] \ndata_extract$se[data_extract$bandwidth==i]  &lt;- rdbw$se[3]\ndata_extract$obs[data_extract$bandwidth==i]  &lt;- (rdbw$N_h[1] + rdbw$N_h[2]) \ndata_extract$bw[data_extract$bandwidth==i]  &lt;- (rdbw$bws[1, 1]) \ndata_extract$ci_l[data_extract$bandwidth==i]  &lt;- rdbw$ci[3,1]\ndata_extract$ci_u[data_extract$bandwidth==i]  &lt;- rdbw$ci[3,2]\n                   }\n\n# Make sure the coefficient (and all other values) are numeric\ndata_extract$coefficient  &lt;- as.numeric(data_extract$coefficient)\n\n# Plot the estimates across bandwidths\nggplot(data = data_extract,\n       aes(x = bandwidth, y = coefficient)) +\n  geom_point(size = 0.8) +\n  geom_ribbon(aes(ymin = ci_l, ymax = ci_u), alpha = 0.2) +\n  geom_hline(aes(yintercept = 0), col = \"red\", linetype = 2) +\n  coord_cartesian(ylim = c(-.05, 0.15)) +\n  theme_minimal() +\n  labs(y = \"LATE at Discontinuity\", x = \"Bandwidths (Vote Margin)\")\n\n\n\n\n\n\n\n9.4.2 Sorting\nThe key identification assumption of RDDs is that there is no sorting on the running variable. That is, the running variable must be continuous around the threshold. If this is not the case, we have a problem: Then there’s a good chance that the observations close to the threshold (or on either side of the threshold) are not random. In other words: Presence of sorting is usually interpreted as empirical evidence of self-selection or non-random sorting of units into control and treatment status. Let’s check if sorting is problem here. We can do this using density checks. This means we’re looking at the density of the running variable.\n\nThere are several ways to check for sorting. Let’s start with McCrary’s density test. We can use the DCdenstiy command to conduct both a visual test and statistical test. The syntax is the following:\n\nDCdensity(running_var ,cutpoint= [cutoff] ,plot=TRUE, ext.out = TRUE)\ntitle(xlab=\"X Lab\",ylab=\"Y Lab\")\nabline(v=0, lty=1) # Adding a vertical line at the cutoff\n\nExercise 16: Examine the density of the running variable using the DCdensity command.\n\n\nReveal Answer\n\n\nDCdensity(educ$margin ,cutpoint= 0 ,plot=TRUE, ext.out = TRUE)\ntitle(xlab=\"Winning Margin\",ylab=\"Density\") # add labels (always add labels. Really. Always.)\nabline(v=0, lty=1)\n\nThe test provides relatively clear visual evidence that sorting is not a problem in this case. There is a clear overlap of the confidence intervals at the cut-off, so the continuity assumption holds. This command provides also statistical tests. You can see that information on the individual cells - or bins - and the number of observations within them is provided. For now, we focus on the inference test for sorting. The p-value of about 0.52 makes clear that we do not reject the null hypothesis of continuity at the cut-off.\n\n\n\nLet’s further explore the structure of our data and check if sorting really isn’t a problem here. Recall, we want to make sure the density of units should be continuous near the cut-off. We can use the rddensity package to do so. This package applies a different test than the one we used before. It also is somewhat more powerful and provides additional information.\nThe command is straightforward. You only have to insert the running variable as argument and, if different from 0, the cutoff. Note that you can optionally specify bandwidths, as above, using the h argument. Otherwise, an optimal bandwidth will be specified automatically. You can then save this as an object and run the summary command to see the output.\nExercise 17: Examine the density of the running variable using the rddensity command. Interpret your findings. Are they equivalent to those in the previous exercise?\n\n\n\nReveal Answer\n\n\nrdd &lt;- rddensity(educ$margin, c=0)\nsummary(rdd)\n\nThis output looks clearer. Note that the test is based on quadratic local polynomial functions. We also get a different bandwidth - recall that we are solely looking at the density of the running variable here.\nImportantly, the test reveals a p-value of 0.357. We again do not reject the null hypothesis of continuity around the cut-off. Both tests indicating the same result gives us confidence in doing so.\nThe second part of the output indicates the results of binomial tests for the allocation of observations around the cut-off for different windows. The number of observations is indicated as well as individual p-values for significance tests, with the null hypothesis being equal distribution on either side of the cut-off. Note that this can give us an indication, but modest unbalance - at such disaggregate level - is not evidence for sorting.\n\n\n\nLet’s now plot this density test, too. The rdplotdensity function allows us to do so easily. It uses the results of the test conducted in rddensity. The syntax is simple:\n\nrdplotdensity([rdd_object] ,[running_var])\n\n\nWe can also add some useful arguments. Feel free to adjust your plot:\n\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nplotRange\nIndicates starting and end point of plot\n\n\nplotN\nNumber of grid points used on either side of the cutoff\n\n\nCIuniform\nTrue or False. If true, CI are displayed as continuous bands\n\n\n\n\n\nExercise 18: Use the rplotdensity to conduct a visual density check in a data range from -0.2 to 0.2. \n\n\n\nReveal Answer\n\n\nplot_rdd &lt;- rdplotdensity(rdd,educ$margin, plotRange = c(-.2, 0.2),  CIuniform = TRUE)\n\n\n\nAgain, we find visual evidence for continuity around the cutoff. Now we can be confident about sorting not being a problem with this data set.\n\n\n\n\n\n9.4.3 Balance\nOne important falsification test is examining whether near the cut-off treated units are similar to control units in terms of observable characteristics. The logic of this test is to identify that if units cannot manipulate the score they receive, there should not be systematic differences between untreated and treated units with similar values of the score.\nExercise 19: Let’s check for balance around the cutoff. Use the rdrobust() function. Set the cut-off argument c equal to 0, and the bwselect argument equal to “mserd”. In this case, the outcome variables are the covariates that we want to check for balance. Check if there is balance for the following covariates: log_pop, school_men, sex_ratio, log_area. Remember to use the summary().\n\n\n\n\nReveal Answer\n\n\nsummary(rdrobust(educ$log_pop, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$school_men, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$sex_ratio, educ$margin, c=0, bwselect=\"mserd\"))\nsummary(rdrobust(educ$log_area, educ$margin, c=0, bwselect=\"mserd\"))\n\nWe observe that there is balance across all the covariates of interest.\n\n\n\n\n\n9.4.4 Placebo cut-offs\nAnother falsification test is to identify treatment effects at artificial or placebo cut-off values. Recall from the lecture that one of the identifying assumptions is the continuity (or lack of abrupt changes) of the regression functions for the treated and control units around the cut-off. While we cannot empirically test this assumption, we can test continuity apart from the threshold. Although this does not imply the continuity assumption hold at the cut-off, it helps to rule out discontinuities other than the cut-off.\nThis test implies looking at the outcome variable but using different cut-offs where we should not expect changes in the outcome. Thus, the estimates from this test should be near zero (and not statistically significant). One important step to conduct this test is that you need to split the sample for those observations that are above the cut-off and those that are below. We do this to avoid “contamination” due to the real treatment effects. It also ensures that the analysis of each placebo cut-off is conducted using only observations with the same treatment status.\nExercise 20: Conduct placebo cut-off test using the following placebo values: -0.2 and -0.25 for the placebo tests below the cut-off. Conduct the same test above the cut-off using the following placebo values: 0.20, and 0.25. Use the rdrobust() function to perform this test. Set the argument bwselect equal to “mserd”. Replace the cut-off argument c with the values listed before. Use the summary() to report the results. Remember to create two new data frames. You can use the filter function from tidyverse to do this.\n\n\n\n\n\n\n\nFunction/argument\nDescription\n\n\n\n\nnew_data &lt;- data\nCreates a new data frame using the conditions set before the pipeline operator %&gt;&gt;%\n\n\nfilter(variable == \"condition\")\nSubset the data based on a logical operation\n\n\n\n\nnew_data &lt;- data %&gt;% \n  filter(variable &gt;= 0)\n\n\n\n\n\nReveal Answer\n\n\neduc_above &lt;- educ %&gt;% \n  filter(margin &gt;= 0)\n\neduc_below &lt;- educ %&gt;% \n  filter(margin &lt; 0)\n\nsummary(rdrobust(educ_above$school_women, educ_above$margin, c=0.2, bwselect = \"mserd\")) # -0.044\nsummary(rdrobust(educ_above$school_women, educ_above$margin, c=0.25, bwselect=\"mserd\")) #-0.025\n\nsummary(rdrobust(educ_below$school_women, educ_below$margin, c=-0.2, bwselect=\"mserd\")) # 0.063\nsummary(rdrobust(educ_below$school_women, educ_below$margin, c=-0.25, bwselect=\"mserd\")) # -0.008\n\nWe find that for almost all the placebo cut-off values, the coefficients are all nearly zero and not statistically significant (with the exception of -0.2, which we would want to investigate further). This evidence suggests that the assumption of continuity is likely to hold in this case."
  },
  {
    "objectID": "frdd.html",
    "href": "frdd.html",
    "title": "8  Fuzzy Regression Discontinuity",
    "section": "",
    "text": "9 Recap\nIn this seminar, we will cover the following topics:\n1. Familiarise ourselves with the data structure of fuzzy RDD and the substantive interpretation of the estimator.\n2. Visualise discontinuities in terms of treatment and outcome at the cut-off.\n3. Estimate parametric fuzzy RDDs using 2SLS regressions.\n4. Using the rdrobust package to estimate non-parametric fuzzy RDDs.\n5. Conduct the following robustness/falsification tests such as balance test, placebo outcome and density test.\nTo familiarize ourselves with fuzzy RDDs, we will be analysing data from the paper Staying in the First League: Parliamentary Representation and the Electoral Success of Small Parties authored by Elias Dinas, Pedro Riera and Nasos Roussias.\nIn this paper, the authors seek to contribute to answering one of the most essential research questions in party politics: Why are some small parties successful whereas other whither away? What accounts for the variation in the trajectories of small parties?\nThe authors theorise an organisational mechanism that contributes to parties’ success. According to their expectations, entering parliament should lead to an increase in a party’s vote share in the subsequent election. This is because being represented in parliament usually comes with a lot of benefits, which are not experienced by parties that are not represented: influence on policy-making, public funding, media visibility, an increase in organisational capacity (staff, etc.) as well as reduced uncertainty about electoral viability and ideological profiles. Conversely, parties that fail to enter parliament do not gain these benefits and might even be abandoned by promising personnel. As you notice, there might be a lot of reasons which make it difficult to isolate a causal effect without an appropriate identification strategy.\nIn many multi-party systems, there is a legal threshold for representation in parliament at the national level, usually in terms of a fixed vote share, that determined whether a party enters parliament or not. The authors exploit the randomness introduced by these electoral thresholds to circumvent endogeneity problems. Clearly, parties are not able to manipulate their vote share - at least in democracies. The authors look at countries that had employed a legal threshold of representation at the national level at least one since 1945.\nWhy do we need to estimate a fuzzy RDD here? Unlike the sharp RDD, in this case some candidates make it to parliament even though their party did not pass the threshold - in other words, there is non-compliance with treatment assignment as the discontinuity does not fully determine treatment status. Roughly 20% of electoral systems allow some parties to get into parliament even if they did not meet the threshold - can you think of reasons why?\nThe authors use a linear regression estimator with a triangular kernel and optimal bandwidths determined by Imbens/Kalyanaramn’s algorithm (h=2.4, after the cut-off is centred). The outcome is parties’ vote share at the subsequent election (\\(t_1\\)). They find a local average treatment effect among compliers of 1.9%-points: Parties that have not cleared the threshold received 3.9 percent at election \\(t_1\\), whereas those who overcome the threshold get 5.8 percent. This amounts to a meaningful and substantive effect of an increase of around 40% in their vote share.\nNote: Electoral thresholds vary by country, ranging from 0.67%-points to 10%-points. To make observations comparable, the authors standardise electoral results. They use 3.51 as common cut-off point and transform vote shares to that scale. Importantly, the relative distance from the threshold remains the same:\n\\[\\frac{Vote Share_i}{Threshold_i} = \\frac{VoteShare_{i, standardised}}{3.51} \\]\nWe will be using the following variables:\nNow let’s load the data. There are two ways to do this:\nYou can load the brands dataset from your laptop using the read.csv() function. You can call this data set enter.\n# Set your working directory\n#setwd(\"~/Desktop/Causal Inference/2022/Lab9\")\n# \nlibrary(readr)\nenter &lt;- read.csv(\"https://bayreuth-politics.github.io/CI22/data/enter.csv\")\n\n#head(enter)\nNow, let’s see how the data looks like. Generate a plot using the rdplot() function from the rdrobust package.\nThe syntax is quite simple - see it:\nExercise 1: Generate a plot using the rdplot(Y, X, ci = 95, binselect = \"\", subset = condition) function. Replace X with enter$performance and Y with enter$newsuccess. Set the argument ci equal to 95, so we can display confidence interval in each bin. Let’s create a RD plot with evenly-spaced bins by setting the argument binselect equal to “es”. Finally, let’s subset the data for parties whose standardised vote share was below 3.51. For this, set the argument subset equal to performance &lt; 3.51. For more details about this function please see below.\nrdplot(Y, X, c = 0, ci = 95, binselect = \"es\", subset = Z &lt; condition)\nNote that we use 3.51 to drop observations larger than that number (weighted threshold). This insures a balanced N below and above the cut-off. While the running variable is standardised, it cannot take on values smaller than 0 (which would correspond to a 0% vote share). To account for this, we mirror this natural boundary above the threshold.\nLet’s check that this indeed a fuzzy design. To do so let’s first create a new variable called treatment_received.\nExercise 2: Create a new variable that is equal to “Treated” if the treated variable is equal to 1, and “Not treated” otherwise. You can use the mutate() and ifelse() function to do this. You can see an example below.\ndata &lt;- data %&gt;% \n  mutate(new_variable = ifelse(variable == 1, \"Treated\", \"Not treated\"))\nNow that we have created this variable, let’s use it to generate other RD plots that will help us identify whether the units actually received the treatment condition that they were assigned to. Let’s use ggplot this time.\nExercise 3: Generate the following plot:\nLet’s now calculate the proportion of compliers above and below the threshold, just to know more about the extent of non-compliance we are dealing with.\nExercise 4: Calculate the proportion of compliers above and below the threshold. Use the instructions and functions below to do this.\ndata %&gt;%  \n  group_by(variable1, variable2) %&gt;%  # \n  summarise(count = n()) %&gt;% \n  group_by(variable1) %&gt;% \n  mutate(prop  = count/ sum(count))\nMore detail of these function can be found below:\nLet’s subset the data for the following analyses. We want to make sure we have a fair comparison between observations below and above the cut-off. Since the performance variable cannot have outcomes &lt;-3,51, we mirror the data by subsetting positive performance scores:\nenter=enter[enter$performance&lt;3.51,]"
  },
  {
    "objectID": "frdd.html#falsification-tests",
    "href": "frdd.html#falsification-tests",
    "title": "8  Fuzzy Regression Discontinuity",
    "section": "11.1 Falsification tests:",
    "text": "11.1 Falsification tests:"
  },
  {
    "objectID": "frdd.html#placebo-outcomes",
    "href": "frdd.html#placebo-outcomes",
    "title": "8  Fuzzy Regression Discontinuity",
    "section": "11.2 Placebo outcomes:",
    "text": "11.2 Placebo outcomes:\nWe can conduct multiple tests to find evidence that there is no manipulation of the score around the cut-off. If units cannot precisely manipulate their scores, therefore we should expect no differences in terms of covariates for units above and below the threshold (we will test this later). We should also expect that there should not be differences in outcomes that should not be affected by the treatment. These outcomes are sometimes called ‘pseudo-outcome’ in the RDD literature.\nOne pseudo-outcome that we can use in this study is electoral success (standardised vote share) in the previous election (\\(t_{-1}\\)). The idea here is that current parliamentary representation in period (\\(t_0\\)) would not affect electoral performance in \\(t_{-1}\\). Let’s see if this is true.\nExercise 12: Conduct a placebo outcome test using the placebo outcome variable oldsuccess. Use the rdrobust() function to perform this test. Set the argument bwselect equal to “mserd”. Set the cut-off argument c equal to zero. Use the summary() command to report the results. What can you conclude based on this test? Does the evidence support the continuity assumption?\n\n\n\n\nReveal Answer\n\n\nsummary(rdrobust(enter$oldsuccess, enter$performance, c=0, bwselect=\"mserd\"))\n\nWe find that the local regression estimate is -1.713 and the robust confidence interval goes from -5.158 to 2.153 (and the p-value is 0.420). We find that there no is evidence of manipulation of the running variable around the cut-off with respect ot previous electoral results.\n\n\n\n\n11.2.1 Placebo cut-offs\nAs we did last week, we can conduct a placebo cut-off test. This test implies looking at the outcome variable but using different cut-offs where we should not expect changes in the outcome. Thus, the estimates from this test should be near zero (and not statistically significant). Again, one important step to conduct this test is subsetting the sample for those observations that are above the cut-off and those that are below. We do this to avoid ‘contamination’ due to the real treatment effect. It also ensures that the analysis of each placebo cut-off is conducted using only observations with the same treatment status.\nRather than subsetting the data and creating a new data frame, let’s use the subset argument in the rdrobust() function.\nExercise 13: Conduct a placebo cut-off test using -2.95 for the placebo tests below the cut-off. Use the rdrobust() function to perform this test. Set the argument bwselect equal to “mserd”. Replace the cut-off argument c with the value of the placebo cut-off. Add the subset argument and set it equal to data$performance &lt; 0 & data$performance &gt;-3.51.  Use thesummary()` command to report the results. What can you conclude based on this test? Does the evidence support the continuity assumption?\n\n\n\n\nReveal Answer\n\n\nsummary(rdrobust(enter$newsuccess, enter$performance, c = -2.95, fuzzy=enter$treated, cluster=enter$countrycode,\n                 bwselect=\"mserd\", subset=enter$performance &lt; 0 & enter$performance &gt;= -3.51))\n\nWe find that the RD estimate is -0.100, which is pretty small. Also, the robust confidence interval reaches from -17.186 to 14.109, which clearly includes zero. Thus, this coefficient is not statistically significant. This provides evidence in favour of continuity assumption.\n\n\n\n\n\n11.2.2 Sorting\nAs we covered last week, the key identification assumption of RDDs is that there is no sorting on the running variable. That is, the running variable must be continuous around the threshold. We can do this using density checks. This means we’re looking at the density of the running variable. Let’s again use the rddensity package to do so.\nExercise 14: Examine the density of the running variable using the rddensity command. Interpret your findings. See an example of the syntax below: \nDoing this is pretty straightforward. You only have to insert the running variable as an argument and, if different from 0, the cut-off. Note that you can optionally specify bandwidths, as above, using the h argument. Otherwise, an optimal bandwidth will be specified automatically. You can then save this as an object and run the summary command to see the output.\n\nrdd &lt;- rddensity(data$running_variable, c=0)\nsummary(rdd)\n\n\n\n\n\nReveal Answer\n\n\nrdd &lt;- rddensity(enter$performance, c=0)\nsummary(rdd)\n\nThis test reveals a p-value of 0.3239. The null hypothesis of this test is that observations near the cut-off are allocated with a 0.5 probability. Given that we failed to reject the null hypothesis, we can claim there is no evidence of sorting around the cut-off.\n\n\n\nLet’s now plot this density test like we did last week. Let’s again use the rdplotdensity function. Remember that this function uses the output from the rddensity command. The syntax is simple:\n\nrdplotdensity([rdd_object] ,[running_var])\n\n\nAnd these the arguments that you can use.\n\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nplotRange\nIndicates starting and end point of plot\n\n\nplotN\nNumber of grid points used on either side of the cut-off\n\n\nCIuniform\nTrue or False. If true, CI are displayed as continuous bands\n\n\n\n\n\nExercise 15: Use the rplotdensity to conduct a visual density check. Set theplotRange function equal to c(-2,2). Set the plotN argument equal to 25. Finally set the CIuniform equal to TRUE.\n\n\n\n\nReveal Answer\n\n\nplot_rdd &lt;- rdplotdensity(rdd, enter$performance, plotRange = c(-2, 2), plotN = 25,  CIuniform = TRUE)\n\n\n\nAgain, we find visual evidence for continuity around the cut-off. Now we can be confident about sorting not being a problem in this data.\nYou can also conduct the same test using the DCdensity function from the rdd package. Unfortunately, to use this command, you need to drop the observations with missing values. You can use drop_na(running_variable) function to do this."
  },
  {
    "objectID": "frdd.html#balance",
    "href": "frdd.html#balance",
    "title": "8  Fuzzy Regression Discontinuity",
    "section": "11.3 Balance",
    "text": "11.3 Balance\nLet’s finally conduct a balance test to examine whether near the cut-off treated units are similar to control units in terms of observable characteristics. Ideally, we should get an RD estimate, \\(\\tau_{RD}\\), which is equal to zero.\nExercise 16: Let’s check for balance around the cut-off. Use the rdrobust() function. Set the cut-off argument c equal to 0, and the bwselect argument equal to “mserd”. In this case, the outcome variables are the covariates that we want to check for balance. Check if there is balance for the covariate oldparty, which is the ‘age’ of the party in terms of previously contested elections. Remember to use the summary() command.\n\n\n\n\nReveal Answer\n\n\nsummary(rdrobust(enter$oldparty, enter$performance, c=0, bwselect=\"mserd\"))\n\nWe observe that there is balance for the covariates as it should be."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "To be added"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]